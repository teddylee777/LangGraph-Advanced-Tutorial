{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델(LLM)\n",
    "\n",
    "LLM(대규모 언어 모델)은 사람처럼 텍스트를 해석하고 생성할 수 있는 강력한 AI 도구입니다. 각 작업에 대한 전문적인 훈련 없이도 콘텐츠 작성, 언어 번역, 요약 및 질문 응답에 사용할 수 있습니다.\n",
    "\n",
    "텍스트 생성 외에도 많은 모델이 다음을 지원합니다\n",
    "\n",
    "- 도구 호출 - 외부 도구(데이터베이스 쿼리 또는 API 호출 등)를 호출하고 결과를 응답에 사용\n",
    "- 구조화된 출력 - 모델의 응답이 정의된 형식을 따르도록 제한\n",
    "- 멀티모달리티 - 이미지, 오디오, 비디오 등 텍스트가 아닌 데이터 처리 및 반환\n",
    "- 추론 - 결론에 도달하기 위한 다단계 추론 수행\n",
    "\n",
    "모델은 에이전트의 추론 엔진입니다. 에이전트의 의사 결정 프로세스를 주도하여 어떤 도구를 호출할지, 결과를 해석하는 방법, 최종 답변을 제공할 시기를 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "# 추적을 위한 프로젝트 이름 설정\n",
    "logging.langsmith(\"LangChain-Advanced-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 (Model)\n",
    "\n",
    "에이전트의 추론 엔진인 LLM 은 간단하게 `provider:model` 형식의 문자열로 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 모델 식별자 문자열을 사용한 간단한 방법\n",
    "model = init_chat_model(\"openai:gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 모델의 세부 설정을 위해서 다음과 같이 다양한 옵션을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 인스턴스를 직접 초기화하여 더 세밀한 제어\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,  # 응답의 무작위성 제어\n",
    "    max_tokens=1000,  # 최대 생성 토큰 수\n",
    "    timeout=30,  # 요청 타임아웃(초)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic 모델 사용 예시 (환경 변수가 `.env` 파일에 설정되어야 합니다.)\n",
    "\n",
    "```\n",
    "ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"anthropic:claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 밖에 지원되는 모델 제공자\n",
    "\n",
    "| 제공자 | 패키지 |\n",
    "|:---|:---|\n",
    "| `openai` | [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai) |\n",
    "| `anthropic` | [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic) |\n",
    "| `azure_openai` | [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai) |\n",
    "| `azure_ai` | [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft) |\n",
    "| `google_vertexai` | [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google) |\n",
    "| `google_genai` | [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google) |\n",
    "| `bedrock` | [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws) |\n",
    "| `bedrock_converse` | [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws) |\n",
    "| `cohere` | [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere) |\n",
    "| `fireworks` | [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks) |\n",
    "| `together` | [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together) |\n",
    "| `mistralai` | [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai) |\n",
    "| `huggingface` | [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface) |\n",
    "| `groq` | [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq) |\n",
    "| `ollama` | [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama) |\n",
    "| `google_anthropic_vertex` | [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google) |\n",
    "| `deepseek` | [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek) |\n",
    "| `ibm` | [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/deepseek) |\n",
    "| `nvidia` | [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia) |\n",
    "| `xai` | [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai) |\n",
    "| `perplexity` | [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOpenAI 클래스\n",
    "\n",
    "OpenAI의 chat model API를 사용하기 위한 인터페이스입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주요 초기화 파라미터**\n",
    "\n",
    "| 파라미터 | 타입 | 설명 |\n",
    "|:---|:---|:---|\n",
    "| `model` | `str` | 사용할 OpenAI 모델 이름 |\n",
    "| `temperature` | `float` | 샘플링 온도 |\n",
    "| `max_tokens` | `int \\| None` | 생성할 최대 토큰 수 |\n",
    "| `logprobs` | `bool \\| None` | logprobs 반환 여부 |\n",
    "| `stream_options` | `dict` | 스트리밍 출력 설정 (예: `{\"include_usage\": True}`) |\n",
    "| `use_responses_api` | `bool \\| None` | Responses API 사용 여부 |\n",
    "| `timeout` | `float \\| Tuple[float, float] \\| Any \\| None` | 요청 타임아웃 |\n",
    "| `max_retries` | `int \\| None` | 최대 재시도 횟수 |\n",
    "| `api_key` | `str \\| None` | OpenAI API 키 (미지정시 `OPENAI_API_KEY` 환경변수 사용) |\n",
    "| `base_url` | `str \\| None` | API 요청 base URL (proxy 또는 emulator 사용시) |\n",
    "| `organization` | `str \\| None` | OpenAI organization ID (미지정시 `OPENAI_ORG_ID` 환경변수 사용) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메시지 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"대한민국의 수도는 어디야?\"),\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍(Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 간단한 방식으로 `stream_response` 함수를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "stream_response(model.stream(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비동기 처리(Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기 호출\n",
    "response = model.ainvoke(messages)\n",
    "# 비동기 호출 대기\n",
    "await response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기 스트리밍\n",
    "async for chunk in model.astream(messages):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "\n",
    "독립적인 요청 모음을 일괄 처리하면 처리가 병렬로 수행될 수 있으므로 다량의 데이터를 처리해야하는 상황에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 요청을 일괄 처리\n",
    "responses = model.batch(\n",
    "    [\n",
    "        \"Why do parrots have colorful feathers?\",\n",
    "        \"How do airplanes fly?\",\n",
    "        \"What is quantum computing?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 중 작업이 완료되는대로 결과를 출력하는 `batch_as_completed` 메서드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완료 시 일괄 응답 산출\n",
    "for response in model.batch_as_acompleted(\n",
    "    [\"대한민국의 수도는 어디야?\", \"대한민국\", \"What is quantum computing?\"]\n",
    "):\n",
    "    print(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling\n",
    "\n",
    "`bind_tools` 도구를 binding 하거나, pydantic 모델을 사용하여 답변을 정형화 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "model_with_tools = model.bind_tools([GetWeather])\n",
    "response = model_with_tools.invoke(\"서울의 날씨는 어때?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 호출\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "정형화된 답변 출력을 위해서는 `with_structured_output` 메서드를 사용을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"답변 형식\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"Name of the person\")\n",
    "    email: str = Field(description=\"Email address of the person\")\n",
    "    phone: str | None = Field(description=\"Phone number of the person\")\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(ResponseFormat)\n",
    "result = structured_model.invoke(\n",
    "    \"다음의 정보로부터 답변을 출력하세요: 이름: 홍길동, 이메일: hong@example.com, 전화번호: 010-1234-5678\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 사용량\n",
    "\n",
    "많은 모델 제공자가 호출 응답의 일부로 토큰 사용량 정보를 반환합니다. 사용 가능한 경우 이 정보는 해당 모델이 생성한 AIMessage 객체에 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4.1-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-4.1-nano\")\n",
    "\n",
    "# 콜백 핸들러를 사용하여 토큰 사용량 추적\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "# config에 콜백 핸들러를 추가하여 토큰 사용량 추적\n",
    "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "\n",
    "callback.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "    model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "\n",
    "    print(\"토큰 사용량은 callback.usage_metadata에 누적됩니다.\")\n",
    "    print(callback.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호출시 config 전달\n",
    "\n",
    "모델을 호출할 때 `config` 매개변수를 통해 추가 구성을 전달할 수 있습니다. 이를 통해 실행 동작, 콜백 및 메타데이터 추적을 런타임에 제어할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구성을 사용한 호출\n",
    "response = model.invoke(\n",
    "    \"안녕! 반가워.\",\n",
    "    config={\n",
    "        \"run_name\": \"greetings\",  # 이 실행의 커스텀 이름\n",
    "        \"tags\": [\"hi\", \"hello\"],  # 분류를 위한 태그\n",
    "        \"metadata\": {\"user_id\": \"teddy\"},  # 커스텀 메타데이터\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace 링크: https://smith.langchain.com/public/d610868a-266b-46d2-929d-5a09677c56ce/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론\n",
    "\n",
    "최신 모델은 결론에 도달하기 위해 다단계 추론을 수행할 수 있습니다. 기본 모델에서 지원되는 경우 이 추론 프로세스를 표시하여 모델이 최종 답변에 도달한 방법을 더 잘 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 출력 스트리밍\n",
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    reasoning_steps = [r for r in chunk.content_blocks if r.get(\"type\") == \"reasoning\"]\n",
    "    if reasoning_steps:\n",
    "        print(reasoning_steps)\n",
    "    else:\n",
    "        print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 멀티모달 LLM\n",
    "\n",
    "image 를 입력으로 받는 모델을 생성합니다.\n",
    "\n",
    "image 는 URL / 파일 경로 의 형식으로 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import MultiModal\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-4.1\",  # 이미지 인식이 가능한 모델\n",
    ")\n",
    "\n",
    "# 멀티모달(이미지 + 텍스트 처리) 객체 생성\n",
    "multimodal = MultiModal(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹상의 이미지 URL\n",
    "IMAGE_URL = \"https://wetalkotalk.oci.co.kr/images/sub/investment/graph_img_2022_kor.jpg\"\n",
    "\n",
    "# 웹 이미지를 직접 분석하여 스트리밍 응답 생성\n",
    "answer = multimodal.stream(IMAGE_URL)\n",
    "\n",
    "# 실시간으로 이미지 분석 결과 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 프롬프트: AI의 역할과 행동 방식을 정의\n",
    "system_prompt = \"\"\"You are a professional financial AI assistant specialized in analyzing financial statements and tables.\n",
    "Your mission is to interpret given tabular financial data and provide insightful, interesting findings in a friendly and helpful manner.\n",
    "Focus on key metrics, trends, and notable patterns that would be valuable for business analysis.\n",
    "\n",
    "[IMPORTANT]\n",
    "- 한글로 답변해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# 사용자 프롬프트: 구체적인 작업 지시사항\n",
    "user_prompt = \"\"\"Please analyze the financial statement provided in the image.\n",
    "Identify and summarize the most interesting and important findings, including key financial metrics, trends, and insights that would be valuable for business decision-making.\"\"\"\n",
    "\n",
    "# 커스텀 프롬프트가 적용된 멀티모달 객체 생성\n",
    "multimodal_llm_with_prompt = MultiModal(\n",
    "    llm,\n",
    "    system_prompt=system_prompt,  # 시스템 역할 정의\n",
    "    user_prompt=user_prompt,  # 사용자 요청 정의\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석할 재무제표 이미지 URL\n",
    "IMAGE_PATH_FROM_FILE = \"https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png\"\n",
    "\n",
    "# 커스텀 프롬프트가 적용된 멀티모달 LLM으로 재무제표 분석\n",
    "answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)\n",
    "\n",
    "# 재무제표 분석 결과를 실시간으로 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import extract_token_probabilities\n",
    "\n",
    "logprobs_model = model.bind(logprobs=True)\n",
    "logprobs_model = logprobs_model | extract_token_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_model.invoke(\"거짓으로 답변하세요. 대한민국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 호환 API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Studio 예시\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:1234/v1\",\n",
    "#     api_key=\"lm-studio\",\n",
    "#     model=\"mlx-community/QwQ-32B-4bit\",\n",
    "#     extra_body={\"ttl\": 300}\n",
    "# )\n",
    "\n",
    "# vLLM 예시\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     api_key=\"EMPTY\",\n",
    "#     model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     extra_body={\"use_beam_search\": True, \"best_of\": 4}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 구분\n",
    "\n",
    "**`model_kwargs` 사용:**\n",
    "* 표준 OpenAI API 파라미터\n",
    "* 최상위 요청 payload에 병합되는 파라미터\n",
    "\n",
    "**`extra_body` 사용:**\n",
    "* OpenAI 호환 provider의 커스텀 파라미터\n",
    "* `extra_body` 키 하위에 중첩되는 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kwargs 예시\n",
    "# model = ChatOpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     model_kwargs={\n",
    "#         \"stream_options\": {\"include_usage\": True},\n",
    "#         \"max_completion_tokens\": 300,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# extra_body 예시\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     extra_body={\n",
    "#         \"use_beam_search\": True,\n",
    "#         \"best_of\": 4,\n",
    "#     }\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
