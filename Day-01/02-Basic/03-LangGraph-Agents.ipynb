{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에이전트(Agent)\n",
    "\n",
    "에이전트는 언어 모델과 도구를 결합하여 작업에 대해 추론하고, 사용할 도구를 결정하며, 솔루션을 향해 반복적으로 작업할 수 있는 시스템을 만듭니다.\n",
    "\n",
    "`create_agent`는 프로덕션 수준의 에이전트 구현을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "# 추적을 위한 프로젝트 이름 설정\n",
    "logging.langsmith(\"LangChain-Advanced-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 (Model)\n",
    "\n",
    "에이전트의 추론 엔진인 LLM 은 간단하게 `provider:model` 형식의 문자열로 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# 모델 식별자 문자열을 사용한 간단한 방법\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 모델의 세부 설정을 위해서 다음과 같이 다양한 옵션을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 인스턴스를 직접 초기화하여 더 세밀한 제어\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,  # 응답의 무작위성 제어\n",
    "    max_tokens=1000,  # 최대 생성 토큰 수\n",
    "    timeout=30,  # 요청 타임아웃(초)\n",
    ")\n",
    "\n",
    "agent = create_agent(model, tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동적 모델\n",
    "\n",
    "동적 모델은 런타임에 현재 상태와 컨텍스트를 기반으로 선택됩니다. 이를 통해 정교한 라우팅 로직과 비용 최적화가 가능합니다.\n",
    "\n",
    "![](assets/wrap_model_call.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelRequest`는 agent의 모델 호출 정보를 담는 dataclass로, middleware에서 요청을 검사하고 수정할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "# 기본 모델과 고급 모델 정의\n",
    "basic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"대화 복잡도에 따라 모델 선택\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    # 긴 대화에는 고급 모델 사용\n",
    "    if message_count > 10:\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # 기본 모델\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정에 활용할 **주요 속성**은 다음과 같습니다.\n",
    "\n",
    "* `model`: 사용할 `BaseChatModel` 인스턴스\n",
    "* `system_prompt`: 시스템 프롬프트 (optional)\n",
    "* `messages`: 대화 메시지 리스트 (시스템 프롬프트 제외)\n",
    "* `tool_choice`: tool 선택 설정\n",
    "* `tools`: 사용 가능한 tool 리스트\n",
    "* `response_format`: 응답 형식 지정\n",
    "* `state`: 현재 agent 상태 (`AgentState`)\n",
    "* `runtime`: agent runtime 정보\n",
    "* `model_settings`: 추가 모델 설정 (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"대화 복잡도에 따라 모델 선택\"\"\"\n",
    "    message_count = len(request.state[\"messages\"][-1].content)\n",
    "    print(f\"글자수: {message_count}\")\n",
    "\n",
    "    # 긴 대화에는 고급 모델 사용\n",
    "    if message_count > 10:\n",
    "        # 여러 속성 동시 변경\n",
    "        new_request = request.override(\n",
    "            model=advanced_model,\n",
    "            system_prompt=\"emoji 를 사용해서 답변해줘\",\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        return handler(new_request)\n",
    "    else:\n",
    "        new_request = request.override(\n",
    "            system_prompt=\"한 문장으로 간결하게 답변해줘. emoji 는 사용하지 말아줘.\",\n",
    "            tool_choice=\"auto\",\n",
    "            model=basic_model,\n",
    "        )\n",
    "        return handler(new_request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # 기본 모델\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "글자수 10자 미만일 때의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"머신러닝 동작원리\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "글자수 10자 이상일 때의 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해 주세요.\")\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시스템 프롬프트\n",
    "\n",
    "`system_prompt` 매개변수를 사용하여 에이전트의 기본 동작을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국의 수도는 어디야?\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동적 시스템 프롬프트(Dynamic Prompting)\n",
    "\n",
    "런타임 컨텍스트나 에이전트 상태를 기반으로 시스템 프롬프트를 수정해야 하는 고급 사용 사례의 경우 `dynamic_prompt` 미들웨어를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    prompt_type: str\n",
    "    length: int\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"사용자 역할에 따라 시스템 프롬프트 생성\"\"\"\n",
    "    # 답변 형식 설정\n",
    "    answer_type = (\n",
    "        request.runtime.context.get(\"prompt_type\", \"default\")\n",
    "        if request.runtime.context\n",
    "        else \"default\"\n",
    "    )\n",
    "    # 답변 길이 설정\n",
    "    answer_length = (\n",
    "        request.runtime.context.get(\"length\", 20) if request.runtime.context else 20\n",
    "    )\n",
    "    base_prompt = \"You are a helpful assistant. Answer in Korean.\\n\"\n",
    "\n",
    "    # 답변 형식에 따라 시스템 프롬프트 생성(동적 프롬프팅)\n",
    "    if answer_type == \"default\":\n",
    "        return f\"{base_prompt} [답변 형식] 간결하게 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    elif answer_type == \"sns\":\n",
    "        return f\"{base_prompt} [답변 형식] SNS 형식으로 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    elif answer_type == \"article\":\n",
    "        return f\"{base_prompt} [답변 형식] 뉴스 기사 형식으로 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    else:\n",
    "        return f\"{base_prompt} [답변 형식] 간결하게 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "\n",
    "\n",
    "# 컨텍스트 스키마와 user_role_prompt 미들웨어를 사용하여 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨텍스트에 따라 시스템 프롬프트가 동적으로 설정됩니다\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    "    context=Context(prompt_type=\"article\", length=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    "    context=Context(prompt_type=\"sns\", length=50),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구조화된 답변 출력(Response Format)\n",
    "\n",
    "특정 형식으로 에이전트의 출력을 반환하고 싶을 때가 있습니다. LangChain은 `response_format` 매개변수를 통해 구조화된 출력 전략을 제공합니다.\n",
    "\n",
    "## Response Format 설정\n",
    "\n",
    "`response_format` 파라미터는 구조화된 응답을 위한 선택적 설정입니다.\n",
    "\n",
    "**지원 타입**\n",
    "\n",
    "다음 세 가지 타입 중 하나를 사용할 수 있습니다\n",
    "\n",
    "* `ToolStrategy`: Tool 기반 구조화 전략\n",
    "* `ProviderStrategy`: Provider 기반 구조화 전략\n",
    "* Pydantic model class: Pydantic 모델 클래스\n",
    "\n",
    "\n",
    "**참고사항**\n",
    "\n",
    "* 모델의 구조화된 출력 지원 여부에 따라 적절한 strategy가 선택됩니다\n",
    "* 구조화된 응답은 대화 컨텍스트 내에서 자동으로 처리됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pydatic model 기반 처리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"The name of the person\")\n",
    "    email: str = Field(description=\"The email of the person\")\n",
    "    phone: str = Field(description=\"The phone number of the person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(model=\"openai:gpt-4.1-mini\", tools=[], response_format=ContactInfo)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정형화된 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToolStrategy 예시\n",
    "\n",
    "`ToolStrategy`는 도구 호출을 사용하여 구조화된 출력을 생성합니다. 도구 호출(Tool Calling)을 지원하는 모든 모델에서 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "# 응답 스키마 정의\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\", tools=[], response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProviderStrategy 사용 예시\n",
    "\n",
    "`ProviderStrategy`는 모델 제공자의 네이티브 구조화된 출력 생성을 사용합니다. \n",
    "\n",
    "더 안정적이지만 네이티브 구조화된 출력을 지원하는 제공자(예: OpenAI)에서만 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1\", response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미들웨어를 통한 중간 상태 제어\n",
    "\n",
    "`before_model` 및 `after_model` 미들웨어는 모델 호출 전후에 중간 상태를 제어할 수 있는 훅입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import (\n",
    "    before_model,\n",
    "    after_model,\n",
    ")\n",
    "from langchain.agents.middleware import (\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    "    dynamic_prompt,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AIMessage, AnyMessage\n",
    "from langchain_teddynote.messages import invoke_graph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "# 노드 스타일: 모델 호출 전 로깅\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(\n",
    "        f\"\\033[95m\\n\\n모델 호출 전 메시지 {len(state['messages'])}개가 있습니다\\033[0m\"\n",
    "    )\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "\n",
    "    query_rewrite = (\n",
    "        PromptTemplate.from_template(\n",
    "            \"Rewrite the following query to be more understandable. Do not change the original meaning. Make it one sentence: {query}\"\n",
    "        )\n",
    "        | llm\n",
    "    )\n",
    "    rewritten_query = query_rewrite.invoke({\"query\": last_message})\n",
    "\n",
    "    return {\"messages\": [rewritten_query.content]}\n",
    "\n",
    "\n",
    "@after_model\n",
    "def log_after_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "\n",
    "    print(\n",
    "        f\"\\033[95m\\n\\n모델 호출 후 메시지 {len(state['messages'])}개가 있습니다\\033[0m\"\n",
    "    )\n",
    "    for i, message in enumerate(state[\"messages\"]):\n",
    "        print(f\"[{i}] {message.content}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    middleware=[\n",
    "        log_before_model,\n",
    "        log_after_model,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국 수도\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 기반 미들웨어 사용\n",
    "\n",
    "데코레이터 대신 클래스 기반 미들웨어를 사용할 수 있습니다.\n",
    "\n",
    "오버라이드 해야 하는 메서드는 `before_model` 및 `after_model` 메서드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "\n",
    "# 커스텀 상태 스키마 정의\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = []\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # 모델 호출 전 커스텀 로직\n",
    "        pass\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], middleware=[CustomMiddleware()])\n",
    "\n",
    "# 에이전트는 이제 메시지 외에 추가 상태를 추적할 수 있습니다\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 오류시 재시도 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"오류 발생으로 {attempt + 1}/3 번째 재시도합니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-minis\",  # 일부러 모델 호출 실패하도록 설정(모델명 오류)\n",
    "    middleware=[retry_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국의 수도는?\")]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
