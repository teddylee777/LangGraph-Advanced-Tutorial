{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LangChain 미들웨어\n",
                "\n",
                "미들웨어는 에이전트 실행의 모든 단계를 제어하고 커스터마이징하는 방법을 제공합니다.\n",
                "\n",
                "핵심 에이전트 루프는 모델을 호출하고, 모델이 실행할 도구를 선택하도록 한 다음, 더 이상 도구를 호출하지 않으면 종료하는 것을 포함합니다.\n",
                "\n",
                "![](./assets/langgraph-middleware.avif)\n",
                "\n",
                "미들웨어는 각 단계 전후에 후크를 노출합니다.\n",
                "\n",
                "- 에이전트 시작 전/후\n",
                "- 모델 호출 전/후\n",
                "- 도구 실행 전/후"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 사전 준비\n",
                "\n",
                "환경 변수를 설정합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "from langchain_teddynote import logging\n",
                "\n",
                "# 환경 변수 로드\n",
                "load_dotenv(override=True)\n",
                "# 추적을 위한 프로젝트 이름 설정\n",
                "logging.langsmith(\"LangChain-Advanced-Tutorial\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 미들웨어가 할 수 있는 것\n",
                "\n",
                "미들웨어는 다음과 같은 다양한 작업을 수행할 수 있습니다.\n",
                "\n",
                "- **모니터링** - 로깅, 분석 및 디버깅으로 에이전트 동작 추적\n",
                "- **수정** - 프롬프트, 도구 선택 및 출력 형식 변환\n",
                "- **제어** - 재시도, 폴백 및 조기 종료 로직 추가\n",
                "- **강제** - 속도 제한, 가드레일 및 PII 감지 적용"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 기본 예제\n",
                "\n",
                "미들웨어를 추가하려면 `create_agent`에 전달합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import create_agent\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain.tools import tool\n",
                "from langchain_teddynote.messages import stream_graph\n",
                "from langchain_core.runnables import RunnableConfig\n",
                "\n",
                "\n",
                "# 간단한 도구 정의\n",
                "@tool\n",
                "def get_weather(city: str) -> str:\n",
                "    \"\"\"Get the weather for a given city.\"\"\"\n",
                "    return f\"It's sunny in {city}!\"\n",
                "\n",
                "\n",
                "# 모델 및 에이전트 생성\n",
                "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[],  # 여기에 미들웨어를 추가합니다\n",
                ")\n",
                "\n",
                "# 에이전트 실행\n",
                "stream_graph(\n",
                "    agent,\n",
                "    inputs={\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Seoul?\"}]},\n",
                "    config=RunnableConfig(),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 내장 미들웨어\n",
                "\n",
                "LangChain은 일반적인 사용 사례를 위한 사전 구축된 미들웨어를 제공합니다.\n",
                "\n",
                "### 요약 (Summarization)\n",
                "\n",
                "토큰 제한에 접근할 때 대화 기록을 자동으로 요약합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 컨텍스트 창을 초과하는 장기 실행 대화\n",
                "- 광범위한 기록이 있는 다중 턴 대화\n",
                "- 전체 대화 컨텍스트 보존이 중요한 애플리케이션"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import create_agent\n",
                "from langchain.agents.middleware import SummarizationMiddleware\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[\n",
                "        SummarizationMiddleware(\n",
                "            model=\"openai:gpt-4.1-mini\",\n",
                "            max_tokens_before_summary=4000,  # 4000 토큰에서 요약 트리거\n",
                "            messages_to_keep=20,  # 요약 후 최근 20개 메시지 유지\n",
                "        ),\n",
                "    ],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 모델 호출 제한 (ModelCallLimitMiddleware)\n",
                "\n",
                "무한 루프나 과도한 비용을 방지하기 위해 모델 호출 수를 제한합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 에이전트가 너무 많은 API 호출을 하는 것을 방지\n",
                "- 프로덕션 배포에 대한 비용 제어 시행\n",
                "- 특정 호출 예산 내에서 에이전트 동작 테스트"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[\n",
                "        ModelCallLimitMiddleware(\n",
                "            thread_limit=3,  # 스레드당 최대 10회 호출 (실행 전반)\n",
                "            run_limit=2,  # 실행당 최대 5회 호출 (단일 호출)\n",
                "            exit_behavior=\"end\",  # 또는 \"error\"로 예외 발생\n",
                "        ),\n",
                "    ],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 도구 호출 제한 (Tool Call Limit)\n",
                "\n",
                "특정 도구 또는 모든 도구에 대한 호출 수를 제한합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 비용이 많이 드는 외부 API에 대한 과도한 호출 방지\n",
                "- 웹 검색 또는 데이터베이스 쿼리 제한\n",
                "- 특정 도구 사용에 대한 속도 제한 시행"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
                "\n",
                "# 모든 도구 호출 제한\n",
                "global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n",
                "\n",
                "# 특정 도구 제한\n",
                "weather_limiter = ToolCallLimitMiddleware(\n",
                "    tool_name=\"get_weather\",\n",
                "    thread_limit=5,\n",
                "    run_limit=3,\n",
                ")\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[global_limiter, weather_limiter],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 모델 폴백 (Model Fallback)\n",
                "\n",
                "기본 모델이 실패할 때 대체 모델로 자동 폴백합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 모델 중단을 처리하는 복원력 있는 에이전트 구축\n",
                "- 더 저렴한 모델로 폴백하여 비용 최적화\n",
                "- OpenAI, Anthropic 등에 걸친 제공자 중복성"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import ModelFallbackMiddleware\n",
                "\n",
                "agent = create_agent(\n",
                "    model=\"openai:gpt-4.1\",  # 기본 모델\n",
                "    tools=[get_weather],\n",
                "    middleware=[\n",
                "        ModelFallbackMiddleware(\n",
                "            \"openai:gpt-4.1-mini\",  # 오류 시 먼저 시도\n",
                "            \"anthropic:claude-4-5-haiku\",  # 그 다음 이것\n",
                "        ),\n",
                "    ],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PII 감지 (PII Detection)\n",
                "\n",
                "대화에서 개인 식별 정보를 감지하고 처리합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 규정 준수 요구 사항이 있는 의료 및 금융 애플리케이션\n",
                "- 로그를 정화해야 하는 고객 서비스 에이전트\n",
                "- 민감한 사용자 데이터를 처리하는 모든 애플리케이션"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import PIIMiddleware\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[\n",
                "        # 사용자 입력에서 이메일 수정\n",
                "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
                "        # 신용카드 마스킹 (마지막 4자리 표시)\n",
                "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
                "        # 정규식을 사용한 커스텀 PII 유형\n",
                "        PIIMiddleware(\n",
                "            \"api_key\",\n",
                "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
                "            strategy=\"mask\",  # 감지 시 오류 발생\n",
                "        ),\n",
                "    ],\n",
                ")\n",
                "\n",
                "# PII 감지 테스트\n",
                "result = agent.invoke(\n",
                "    {\n",
                "        \"messages\": [\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": \"My credit card number is 1234-5678-9012-3456, and my API key is sk-12345678901234567890123456789012, My email is teddy@example.com. Can you help me?\",\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "print(result[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 도구 재시도 (Tool Retry)\n",
                "\n",
                "구성 가능한 지수 백오프로 실패한 도구 호출을 자동으로 재시도합니다.\n",
                "\n",
                "**적합한 경우:**\n",
                "- 외부 API 호출의 일시적인 실패 처리\n",
                "- 네트워크 종속 도구의 안정성 향상\n",
                "- 일시적인 오류를 우아하게 처리하는 복원력 있는 에이전트 구축"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import ToolRetryMiddleware\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[\n",
                "        ToolRetryMiddleware(\n",
                "            max_retries=3,  # 최대 3회 재시도\n",
                "            backoff_factor=2.0,  # 지수 백오프 승수\n",
                "            initial_delay=1.0,  # 1초 지연으로 시작\n",
                "            max_delay=60.0,  # 지연을 60초로 제한\n",
                "            jitter=True,  # 무작위 지터 추가\n",
                "        ),\n",
                "    ],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 커스텀 미들웨어(추천)\n",
                "\n",
                "에이전트 실행 흐름의 특정 지점에서 실행되는 후크를 구현하여 커스텀 미들웨어를 구축할 수 있습니다.\n",
                "\n",
                "미들웨어를 만드는 두 가지 방법이 있습니다:\n",
                "\n",
                "1. **데코레이터 기반** - 단일 후크 미들웨어에 빠르고 간단\n",
                "2. **클래스 기반** - 여러 후크가 있는 복잡한 미들웨어에 더 강력"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 데코레이터 기반 미들웨어\n",
                "\n",
                "단일 후크만 필요한 간단한 미들웨어의 경우 데코레이터가 기능을 추가하는 가장 빠른 방법을 제공합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import (\n",
                "    before_agent,\n",
                "    before_model,\n",
                "    after_model,\n",
                "    after_agent,\n",
                "    wrap_model_call,\n",
                "    wrap_tool_call,\n",
                ")\n",
                "from langchain.agents.middleware import (\n",
                "    AgentState,\n",
                "    ModelRequest,\n",
                "    ModelResponse,\n",
                "    dynamic_prompt,\n",
                ")\n",
                "from langchain.messages import AIMessage\n",
                "from langchain_teddynote.messages import invoke_graph\n",
                "from langgraph.runtime import Runtime\n",
                "from typing import Any, Callable\n",
                "\n",
                "\n",
                "# 노드 스타일: 모델 호출 전 로깅\n",
                "@before_model\n",
                "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
                "    print(f\"모델을 호출하기 전에 메시지 {len(state['messages'])}개가 있습니다\")\n",
                "    return None\n",
                "\n",
                "\n",
                "# 노드 스타일: 모델 호출 후 검증\n",
                "@after_model\n",
                "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
                "    last_message = state[\"messages\"][-1]\n",
                "    if \"BLOCKED\" in last_message.content:\n",
                "        return {\n",
                "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
                "        }\n",
                "    return None\n",
                "\n",
                "\n",
                "@before_agent\n",
                "def log_before_agent(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
                "    print(f\"에이전트를 시작하기 전에 메시지 {len(state['messages'])}개가 있습니다\")\n",
                "    return None\n",
                "\n",
                "\n",
                "@after_agent\n",
                "def log_after_agent(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
                "    print(f\"에이전트가 종료되었습니다. 총 메시지 수: {len(state['messages'])}개\")\n",
                "\n",
                "    return None\n",
                "\n",
                "\n",
                "# wrap_model_call 재시도 로직\n",
                "@wrap_model_call\n",
                "def retry_model(\n",
                "    request: ModelRequest,\n",
                "    handler: Callable[[ModelRequest], ModelResponse],\n",
                ") -> ModelResponse:\n",
                "    for attempt in range(3):\n",
                "        try:\n",
                "            return handler(request)\n",
                "        except Exception as e:\n",
                "            if attempt == 2:\n",
                "                raise\n",
                "            print(f\"오류 발생으로 {attempt + 1}/3 번째 재시도합니다: {e}\")\n",
                "\n",
                "\n",
                "# 동적 프롬프트\n",
                "@dynamic_prompt\n",
                "def personalized_prompt(request: ModelRequest) -> str:\n",
                "    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n",
                "    return f\"You are a helpful assistant for user {user_id}. Greeting with user's name. Be concise and friendly.\"\n",
                "\n",
                "\n",
                "# 에이전트에서 데코레이터 사용\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    middleware=[\n",
                "        log_before_model,\n",
                "        validate_output,\n",
                "        retry_model,\n",
                "        personalized_prompt,\n",
                "        log_before_agent,\n",
                "        log_after_agent,\n",
                "    ],\n",
                "    tools=[get_weather],\n",
                ")\n",
                "\n",
                "invoke_graph(\n",
                "    agent,\n",
                "    inputs={\"messages\": [{\"role\": \"user\", \"content\": \"서울 날씨 알려줘\"}]},\n",
                "    context={\"user_id\": \"teddy\"},\n",
                "    config=RunnableConfig(),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 클래스 기반 미들웨어\n",
                "\n",
                "복잡한 미들웨어의 경우 클래스 기반 접근 방식을 사용하여 여러 후크를 구현할 수 있습니다.\n",
                "\n",
                "#### 노드 스타일 후크\n",
                "\n",
                "실행 흐름의 특정 지점에서 실행됩니다:\n",
                "- `before_agent` - 에이전트 시작 전 (호출당 한 번)\n",
                "- `before_model` - 각 모델 호출 전\n",
                "- `after_model` - 각 모델 응답 후\n",
                "- `after_agent` - 에이전트 완료 후 (호출당 최대 한 번)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
                "from langgraph.runtime import Runtime\n",
                "from typing import Any\n",
                "\n",
                "\n",
                "# 로깅 미들웨어\n",
                "class LoggingMiddleware(AgentMiddleware):\n",
                "    def before_model(\n",
                "        self, state: AgentState, runtime: Runtime\n",
                "    ) -> dict[str, Any] | None:\n",
                "        print(f\"About to call model with {len(state['messages'])} messages\")\n",
                "        return None\n",
                "\n",
                "    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
                "        print(f\"Model returned: {state['messages'][-1].content[:50]}...\")\n",
                "        return None\n",
                "\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[LoggingMiddleware()],\n",
                ")\n",
                "\n",
                "result = agent.invoke(\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]}\n",
                ")\n",
                "print(\"\\nFinal:\", result[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 대화 길이 제한 예제"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
                "from langchain.messages import AIMessage\n",
                "from langgraph.runtime import Runtime\n",
                "from typing import Any\n",
                "\n",
                "\n",
                "class MessageLimitMiddleware(AgentMiddleware):\n",
                "    def __init__(self, max_messages: int = 50):\n",
                "        super().__init__()\n",
                "        self.max_messages = max_messages\n",
                "\n",
                "    def before_model(\n",
                "        self, state: AgentState, runtime: Runtime\n",
                "    ) -> dict[str, Any] | None:\n",
                "        if len(state[\"messages\"]) >= self.max_messages:\n",
                "            return {\n",
                "                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n",
                "            }\n",
                "        return None\n",
                "\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[MessageLimitMiddleware(max_messages=10)],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 랩 스타일 후크\n",
                "\n",
                "실행을 가로채고 핸들러가 호출되는 시기를 제어합니다:\n",
                "- `wrap_model_call` - 각 모델 호출 주변\n",
                "- `wrap_tool_call` - 각 도구 호출 주변\n",
                "\n",
                "핸들러를 0번(단락), 1번(정상 흐름) 또는 여러 번(재시도 로직) 호출할지 결정합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
                "from typing import Callable\n",
                "\n",
                "\n",
                "class RetryMiddleware(AgentMiddleware):\n",
                "    def __init__(self, max_retries: int = 3):\n",
                "        super().__init__()\n",
                "        self.max_retries = max_retries\n",
                "\n",
                "    def wrap_model_call(\n",
                "        self,\n",
                "        request: ModelRequest,\n",
                "        handler: Callable[[ModelRequest], ModelResponse],\n",
                "    ) -> ModelResponse:\n",
                "        for attempt in range(self.max_retries):\n",
                "            try:\n",
                "                return handler(request)\n",
                "            except Exception as e:\n",
                "                if attempt == self.max_retries - 1:\n",
                "                    raise\n",
                "                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n",
                "\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[RetryMiddleware(max_retries=3)],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 동적 모델 선택 예제"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
                "from langchain.chat_models import init_chat_model\n",
                "from typing import Callable\n",
                "\n",
                "\n",
                "class DynamicModelMiddleware(AgentMiddleware):\n",
                "    def wrap_model_call(\n",
                "        self,\n",
                "        request: ModelRequest,\n",
                "        handler: Callable[[ModelRequest], ModelResponse],\n",
                "    ) -> ModelResponse:\n",
                "        # 대화 길이에 따라 다른 모델 사용\n",
                "        if len(request.messages) > 10:\n",
                "            request.model = init_chat_model(\"openai:gpt-4.1\")\n",
                "            print(\"Using gpt-4.1 for long conversation\")\n",
                "        else:\n",
                "            request.model = init_chat_model(\"openai:gpt-4.1-mini\")\n",
                "            print(\"Using gpt-4.1-mini for short conversation\")\n",
                "\n",
                "        return handler(request)\n",
                "\n",
                "\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[DynamicModelMiddleware()],\n",
                ")\n",
                "\n",
                "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]})\n",
                "print(result[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 실행 순서\n",
                "\n",
                "여러 미들웨어를 사용할 때 실행 순서를 이해하는 것이 중요합니다.\n",
                "\n",
                "**주요 규칙:**\n",
                "- `before_*` 후크: 첫 번째부터 마지막까지\n",
                "- `after_*` 후크: 마지막부터 첫 번째까지 (역순)\n",
                "- `wrap_*` 후크: 중첩됨 (첫 번째 미들웨어가 다른 모든 것을 래핑)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import AgentMiddleware\n",
                "\n",
                "\n",
                "class Middleware1(AgentMiddleware):\n",
                "    def before_model(self, state, runtime):\n",
                "        print(\"1: before_model\")\n",
                "        return None\n",
                "\n",
                "    def after_model(self, state, runtime):\n",
                "        print(\"1: after_model\")\n",
                "        return None\n",
                "\n",
                "\n",
                "class Middleware2(AgentMiddleware):\n",
                "    def before_model(self, state, runtime):\n",
                "        print(\"2: before_model\")\n",
                "        return None\n",
                "\n",
                "    def after_model(self, state, runtime):\n",
                "        print(\"2: after_model\")\n",
                "        return None\n",
                "\n",
                "\n",
                "# 실행 순서 확인\n",
                "agent = create_agent(\n",
                "    model=model,\n",
                "    tools=[get_weather],\n",
                "    middleware=[Middleware1(), Middleware2()],\n",
                ")\n",
                "\n",
                "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]})\n",
                "\n",
                "# 출력:\n",
                "# 1: before_model\n",
                "# 2: before_model\n",
                "# (모델 호출)\n",
                "# 2: after_model\n",
                "# 1: after_model"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
