{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph QuickStart\n",
    "\n",
    "LangGraph는 상태 기반 멀티 액터 애플리케이션을 구축하기 위한 프레임워크입니다.\n",
    "\n",
    "## 구현 기능\n",
    "\n",
    "- 상태 관리 기반 챗봇\n",
    "- 외부 도구 연동 (Tavily Search)\n",
    "- 메모리 및 체크포인트\n",
    "- Human-in-the-Loop\n",
    "- 상태 커스터마이징\n",
    "- 상태 이력 관리\n",
    "\n",
    "## 요구사항\n",
    "\n",
    "- Python 3.11 이상\n",
    "- OpenAI API Key\n",
    "- Tavily Search API Key\n",
    "- LangSmith API Key (선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "# 추적을 위한 프로젝트 이름 설정\n",
    "logging.langsmith(\"LangChain-Advanced-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 기본 챗봇 구축\n",
    "\n",
    "메시지 기반 챗봇을 StateGraph로 구성합니다.\n",
    "\n",
    "### 구성 요소\n",
    "\n",
    "- StateGraph: 전체 흐름 정의\n",
    "- State: 메시지 저장 구조\n",
    "- Node: 처리 함수 (LLM 호출)\n",
    "- Edge: 실행 경로 연결\n",
    "- Compile/Invoke: 실행 준비 및 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State 정의: 챗봇의 상태를 나타내는 타입\n",
    "class State(TypedDict):\n",
    "    \"\"\"챗봇의 상태를 정의하는 타입\n",
    "\n",
    "    messages: 대화 메시지 리스트\n",
    "    - add_messages 함수를 통해 새 메시지가 추가됨 (덮어쓰기가 아닌 추가)\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# StateGraph 생성\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "print(\"StateGraph 생성 완료!\")\n",
    "print(\"State는 messages 키를 가지며, add_messages 리듀서를 사용합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 설정\n",
    "\n",
    "GPT-4.1 모델을 사용합니다. temperature=0으로 일관된 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 모델 식별자 문자열을 사용한 간단한 방법\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 챗봇 노드 추가\n",
    "\n",
    "대화 메시지를 LLM에 전달하고 응답을 상태에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"챗봇 노드 함수\n",
    "\n",
    "    현재 상태의 메시지를 받아 LLM에 전달하고,\n",
    "    응답을 새 메시지로 추가하여 반환합니다.\n",
    "    \"\"\"\n",
    "    # LLM을 호출하여 응답 생성\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # 응답을 메시지 리스트에 추가하여 반환\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 그래프에 노드 추가\n",
    "# 첫 번째 인자: 노드의 고유 이름\n",
    "# 두 번째 인자: 노드가 사용될 때 호출될 함수\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엣지 설정\n",
    "\n",
    "실행 경로: START → chatbot → END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진입점: 그래프 실행이 시작되는 지점\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# 종료점: 그래프 실행이 끝나는 지점\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "print(\"진입점과 종료점 설정 완료!\")\n",
    "print(\"실행 흐름: START → chatbot → END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 컴파일\n",
    "\n",
    "그래프를 실행 가능한 형태로 컴파일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 컴파일\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"그래프 컴파일 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Config 설정(recursion_limit: 재귀 깊이 제한, thread_id: 스레드 아이디)\n",
    "config = RunnableConfig(recursion_limit=20, thread_id=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"안녕하세요! LangGraph에 대해 알려주세요.\")]\n",
    "}\n",
    "\n",
    "# 그래프 스트리밍\n",
    "stream_graph(graph, inputs=inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 도구(Tools) 추가\n",
    "\n",
    "외부 검색 도구를 통합하여 실시간 정보를 조회합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "- Tool Binding: LLM에 도구 연결\n",
    "- Tool Node: 외부 API 호출 노드\n",
    "- Conditional Edges: 도구 사용 여부 자동 분기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공유\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"add two numbers\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Tavily 검색 도구 설정\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool, add]\n",
    "\n",
    "# 도구 테스트\n",
    "result = tool.invoke(\"LangGraph란 무엇인가요?\")\n",
    "print(f\"검색 결과 수: {len(result['results'])}개\")\n",
    "print(f\"첫 번째 결과 제목: {result['results'][0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도구 사용 그래프 구성\n",
    "\n",
    "기본 흐름에 도구 호출 경로를 추가합니다: chatbot ⇄ tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret1 = llm_with_tools.invoke(\"LangGraph 가 뭐야?\")\n",
    "ret2 = llm_with_tools.invoke(\"LangGraph 가 뭐야? 검색해서 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "display_message_tree(ret1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_message_tree(ret2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State 정의 (동일)\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# 새로운 그래프 빌더 생성\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# LLM에 도구 바인딩 - LLM이 도구를 사용할 수 있도록 설정\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"도구를 사용할 수 있는 챗봇 노드\"\"\"\n",
    "    # 도구가 바인딩된 LLM 호출\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 노드 추가\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# ToolNode 추가 - 도구를 실행하는 노드\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건부 라우팅\n",
    "\n",
    "`tools_condition`이 마지막 AI 메시지의 `tool_calls` 존재 여부를 확인해 경로를 분기합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools_condition 동작\n",
    "\n",
    "`tool_calls` 존재 시 \"tools\"로, 없으면 \"\\_\\_end\\_\\_\"로 분기합니다.\n",
    "\n",
    "```python\n",
    "def tools_condition(state) -> Literal[\"tools\", \"__end__\"]:\n",
    "    ai_message = state[-1] if isinstance(state, list) else state[\"messages\"][-1]\n",
    "    return \"tools\" if getattr(ai_message, \"tool_calls\", []) else \"__end__\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건부 엣지 추가\n",
    "# tools_condition은 메시지에 tool_calls가 있으면 \"tools\"로,\n",
    "# 없으면 END로 라우팅합니다\n",
    "builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,  # 사전 정의된 조건 함수 사용\n",
    ")\n",
    "# Literal[\"tools\", \"__end__\"]\n",
    "\n",
    "# 도구 실행 후 다시 챗봇으로 돌아가기\n",
    "builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# 시작점 설정\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_with_tools = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 시각화\n",
    "visualize_graph(graph_with_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도구 사용 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "stream_graph(\n",
    "    graph_with_tools,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"2025년 LangGraph 사용 사례 알려주세요.\")]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 메모리 추가\n",
    "\n",
    "세션 간 사용자 정보를 유지하는 영구 상태 관리를 추가합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "- Checkpointer: 대화 상태 저장/복원\n",
    "- Thread ID: 세션 식별자\n",
    "- Persistent State: 누적 이력 기반 컨텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import os\n",
    "\n",
    "\n",
    "# Pydantic 모델 정의\n",
    "class MemoryItem(BaseModel):\n",
    "    \"\"\"개별 메모리 아이템\"\"\"\n",
    "\n",
    "    key: str = Field(description=\"메모리 키 (예: user_name, preference, fact)\")\n",
    "    value: str = Field(description=\"메모리 값\")\n",
    "    category: str = Field(\n",
    "        description=\"카테고리 (personal_info, preference, interest, relationship, fact, etc.)\"\n",
    "    )\n",
    "    importance: int = Field(description=\"중요도 (1-5, 5가 가장 중요)\", ge=1, le=5)\n",
    "    confidence: float = Field(description=\"추출 신뢰도 (0.0-1.0)\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class ExtractedMemories(BaseModel):\n",
    "    \"\"\"추출된 메모리 컬렉션\"\"\"\n",
    "\n",
    "    memories: List[MemoryItem] = Field(description=\"추출된 메모리 아이템 리스트\")\n",
    "    summary: str = Field(description=\"대화 내용 요약\")\n",
    "    timestamp: str = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(), description=\"추출 시간\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 기본 시스템 프롬프트\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an expert memory extraction assistant. Your task is to extract important information from user conversations and convert them into structured key-value pairs for long-term memory storage.\n",
    "\n",
    "Extract ALL relevant information from the conversation, including:\n",
    "- Personal information (name, age, location, occupation, etc.)\n",
    "- Preferences and interests\n",
    "- Relationships and social connections\n",
    "- Important facts or events mentioned\n",
    "- Opinions and beliefs\n",
    "- Goals and aspirations\n",
    "- Any other notable information\n",
    "\n",
    "For each piece of information:\n",
    "1. Create a concise, searchable key\n",
    "2. Store the complete value\n",
    "3. Categorize appropriately\n",
    "4. Assess importance (1-5 scale)\n",
    "5. Evaluate extraction confidence (0.0-1.0)\"\"\"\n",
    "\n",
    "\n",
    "def create_memory_extractor(\n",
    "    model_name: Optional[str] = \"openai:gpt-4.1-mini\",\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> any:\n",
    "    \"\"\"\n",
    "    메모리 추출기를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        model: 사용할 언어 모델. None일 경우 기본 ChatOpenAI 모델 사용\n",
    "        system_prompt: 시스템 프롬프트. None일 경우 기본 프롬프트 사용\n",
    "\n",
    "    Returns:\n",
    "        메모리 추출 체인\n",
    "    \"\"\"\n",
    "    # Output Parser 생성\n",
    "    memory_parser = PydanticOutputParser(pydantic_object=ExtractedMemories)\n",
    "\n",
    "    # 시스템 프롬프트 설정\n",
    "    if system_prompt is None:\n",
    "        system_prompt = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    # 전체 프롬프트 템플릿 구성\n",
    "    template = f\"\"\"{system_prompt}\n",
    "\n",
    "User Input: {{input}}\n",
    "\n",
    "{{format_instructions}}\n",
    "\n",
    "Remember to:\n",
    "- Extract multiple memory items if the conversation contains various pieces of information\n",
    "- Use clear, consistent key naming conventions\n",
    "- Preserve context in values when necessary\n",
    "- Be comprehensive but avoid redundancy\n",
    "\"\"\"\n",
    "\n",
    "    # 프롬프트 생성\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template,\n",
    "        partial_variables={\n",
    "            \"format_instructions\": memory_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 모델 설정\n",
    "    model = init_chat_model(model_name)\n",
    "\n",
    "    # 메모리 추출 체인 생성\n",
    "    memory_extractor = prompt | model | memory_parser\n",
    "\n",
    "    return memory_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_teddynote.memory import create_memory_extractor\n",
    "import uuid\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-4.1\")\n",
    "memory_extractor = create_memory_extractor(model=\"openai:gpt-4.1\")\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    store: BaseStore,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Call the LLM model and manage user memory.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state containing messages.\n",
    "        config (RunnableConfig): The runnable configuration.\n",
    "        store (BaseStore): The memory store.\n",
    "    \"\"\"\n",
    "    # 마지막 메시지에서 user_id 추출\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    print(namespace)\n",
    "\n",
    "    # 유저의 메모리 검색\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([f\"{memory.key}: {memory.value}\" for memory in memories])\n",
    "    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "    # 사용자가 기억 요청 시 메모리 저장\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        result = memory_extractor.invoke({\"input\": str(state[\"messages\"][-1].content)})\n",
    "        for memory in result.memories:\n",
    "            print(memory)\n",
    "            print(\"-\" * 100)\n",
    "            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})\n",
    "\n",
    "    # LLM 호출\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# 그래프 빌드\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# 메모리 체크포인터 생성\n",
    "# 실제 프로덕션에서는 PostgresSaver 사용 권장\n",
    "memory_saver = InMemorySaver()\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_with_memory = builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    "    store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "\n",
    "def run_graph(\n",
    "    msg,\n",
    "    thread_id=\"default\",\n",
    "    user_id=\"default\",\n",
    "):\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id + user_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "    }\n",
    "    print(f\"\\n[유저] {msg}\")\n",
    "    stream_graph(\n",
    "        graph_with_memory,\n",
    "        inputs={\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
    "        config=config,\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"안녕? 내 이름은 테디야\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"내 이름이 뭐라고?\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"내 이름이 뭐라고?\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장기 기억 저장\n",
    "\n",
    "메시지에 `remember` 키워드 포함 시 장기 저장소에 정보를 기록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"내 이름이 테디야 remember\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread 간 지속성\n",
    "\n",
    "User ID 기반 장기 기억은 Thread가 달라도 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\"내 이름이 뭐라고 했더라?\", \"1004\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지, thread_id, user_id 전달\n",
    "run_graph(\n",
    "    \"내 직업은 AI Engineer 야. 내 취미는 Netflix 보기 야. remember\", \"4\", \"someone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 스레드에서 실행\n",
    "run_graph(\"내 이름, 직업, 취미 알려줘\", \"100\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 user_id 로 실행한 경우\n",
    "run_graph(\"내 이름, 직업, 취미 알려줘\", \"100\", \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State 확인\n",
    "\n",
    "저장된 상태를 조회하여 메시지 이력과 체크포인트 정보를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 Config 설정\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"100\" + \"someone\",\n",
    "        \"user_id\": \"someone\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# 현재 상태 가져오기\n",
    "snapshot = graph_with_memory.get_state(config)\n",
    "\n",
    "print(\"현재 상태 정보:\")\n",
    "print(f\"- 메시지 수: {len(snapshot.values['messages'])}개\")\n",
    "print(f\"- 체크포인트 ID: {snapshot.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "# 최근 메시지 몇 개 표시\n",
    "print(\"\\n[최근 메시지]\")\n",
    "for msg in snapshot.values[\"messages\"]:\n",
    "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Human-in-the-Loop\n",
    "\n",
    "고위험 작업에 대해 인간 승인을 요청하는 흐름을 도입합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "- interrupt: 실행 일시정지 및 승인 대기\n",
    "- Command: 승인/거부 후 재개 명령\n",
    "- Human Approval: 승인 워크플로우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from an expert(human).\"\"\"\n",
    "    # interrupt를 호출하여 실행 일시 중지\n",
    "    # 사람의 응답을 기다림\n",
    "    human_response = interrupt({\"query\": query})\n",
    "\n",
    "    # 사람의 응답 반환\n",
    "    return human_response[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITL 그래프 구성\n",
    "\n",
    "`human_assistance` 도구를 통해 interrupt로 중단 후 인간 답변을 대기합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 리스트 업데이트\n",
    "tools_with_human = [human_assistance]\n",
    "\n",
    "# 새로운 그래프 구성\n",
    "graph_builder_hitl = StateGraph(State)\n",
    "\n",
    "# LLM에 도구 바인딩\n",
    "llm_with_human_tools = llm.bind_tools(tools_with_human)\n",
    "\n",
    "\n",
    "def chatbot_with_human(state: State):\n",
    "    \"\"\"Human Interuption 요청할 수 있는 챗봇\"\"\"\n",
    "    message = llm_with_human_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    # interrupt 중 병렬 도구 호출 방지\n",
    "    # (재개 시 도구 호출이 반복되는 것을 방지)\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert (\n",
    "            len(message.tool_calls) <= 1\n",
    "        ), \"병렬 도구 호출은 interrupt와 함께 사용할 수 없습니다\"\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# 노드 추가\n",
    "graph_builder_hitl.add_node(\"chatbot_with_human\", chatbot_with_human)\n",
    "\n",
    "# ToolNode 추가\n",
    "tool_node_hitl = ToolNode(tools=tools_with_human)\n",
    "graph_builder_hitl.add_node(\"tools\", tool_node_hitl)\n",
    "\n",
    "# 엣지 추가\n",
    "graph_builder_hitl.add_conditional_edges(\"chatbot_with_human\", tools_condition)\n",
    "graph_builder_hitl.add_edge(\"tools\", \"chatbot_with_human\")\n",
    "graph_builder_hitl.add_edge(START, \"chatbot_with_human\")\n",
    "\n",
    "# 메모리와 함께 컴파일\n",
    "memory_hitl = InMemorySaver()\n",
    "graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITL 테스트\n",
    "\n",
    "사람에게 조언을 요청하는 질문으로 interrupt와 재개 흐름을 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import random_uuid\n",
    "\n",
    "# 인간 지원을 요청하는 메시지\n",
    "user_input = \"LangGraph 가 뭐야? 사람한테 듣고 싶어.\"\n",
    "config_hitl = {\"configurable\": {\"thread_id\": random_uuid()}}\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "stream_graph(\n",
    "    graph_hitl,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=config_hitl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 확인 - 어느 노드에서 중단되었는지 확인\n",
    "snapshot = graph_hitl.get_state(config_hitl)\n",
    "print(f\"\\n현재 상태:\")\n",
    "print(f\"  다음 실행할 노드: {snapshot.next}\")\n",
    "print(f\"  체크포인트 ID: {snapshot.config['configurable']['checkpoint_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인간의 응답으로 실행 재개\n",
    "human_response = \"\"\"## 전문가의 조언:\n",
    "- YouTube 테디노트: https://www.youtube.com/c/teddynote\n",
    "- 고급 개발자 강의 [패스트캠퍼스 RAG 비법노트](https://fastcampus.co.kr/data_online_teddy)\n",
    "\"\"\"\n",
    "\n",
    "# Command 객체로 재개\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(f\"\\n사람의 응답: {human_response}\\n\")\n",
    "\n",
    "# 재개\n",
    "stream_graph(graph_hitl, inputs=human_command, config=config_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 상태 커스터마이징\n",
    "\n",
    "메시지 외 업무 데이터를 다루는 커스텀 상태와 도구 기반 상태 업데이트를 도입합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "- Custom State Fields: 메시지 외 추가 필드\n",
    "- State Updates from Tools: 도구 결과로 상태 갱신\n",
    "- Manual State Updates: 수동 상태 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "\n",
    "\n",
    "# 확장된 State 정의\n",
    "class CustomState(TypedDict):\n",
    "    \"\"\"커스텀 필드가 추가된 상태\"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "    human_feedback: str  # 사람의 피드백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 업데이트 도구\n",
    "\n",
    "도구 실행 결과를 `Command(update=...)`로 상태에 반영합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_review(\n",
    "    human_feedback, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request human review for information.\"\"\"\n",
    "    # 인간에게 검토 요청\n",
    "    human_response = interrupt(\n",
    "        {\"question\": \"이 정보가 맞나요?\", \"human_feedback\": human_feedback}\n",
    "    )\n",
    "\n",
    "    feedback = human_response.get(\"human_feedback\", \"\")\n",
    "\n",
    "    if feedback.strip() == \"\":\n",
    "        # 사용자가 AI 의 답변에 동의하는 경우\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [ToolMessage(human_response, tool_call_id=tool_call_id)]\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # 사용자가 AI 의 답변에 동의하지 않는 경우\n",
    "        corrected_information = f\"# 사용자에 의해 수정된 피드백: {feedback}\"\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [\n",
    "                    ToolMessage(corrected_information, tool_call_id=tool_call_id)\n",
    "                ]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 상태 그래프\n",
    "\n",
    "`CustomState`로 그래프를 구성하고 도구를 통한 상태 업데이트 루프를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 리스트\n",
    "tools_custom = [human_review]\n",
    "\n",
    "# 새로운 그래프 구성\n",
    "custom_graph_builder = StateGraph(CustomState)  # CustomState 사용\n",
    "\n",
    "# LLM에 도구 바인딩\n",
    "llm_with_custom_tools = llm.bind_tools(tools_custom)\n",
    "\n",
    "\n",
    "def chatbot_custom(state: CustomState):\n",
    "    \"\"\"커스텀 상태를 사용하는 챗봇\"\"\"\n",
    "    message = llm_with_custom_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert len(message.tool_calls) <= 1\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# 노드와 엣지 추가\n",
    "custom_graph_builder.add_node(\"chatbot\", chatbot_custom)\n",
    "tool_node_custom = ToolNode(tools=tools_custom)\n",
    "custom_graph_builder.add_node(\"tools\", tool_node_custom)\n",
    "\n",
    "custom_graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "custom_graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "custom_graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# 컴파일\n",
    "memory_custom = InMemorySaver()\n",
    "custom_graph = custom_graph_builder.compile(checkpointer=memory_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 시각화\n",
    "visualize_graph(custom_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 상태 테스트\n",
    "\n",
    "`human_review` 도구 호출 시 interrupt로 중단되고, 재개 시 상태가 갱신되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph의 출시일을 조사하고 검토 요청\n",
    "user_input = (\n",
    "    \"2024년 노벨 문학상 수상자가 누구인지 조사해주세요. \"\n",
    "    \"답을 찾으면 `human_review` 도구를 사용해서 검토를 요청하세요.\"\n",
    ")\n",
    "\n",
    "custom_config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "# 실행 (interrupt에서 중단될 것임)\n",
    "stream_graph(\n",
    "    custom_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=custom_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "# 최신 메시지 가져오기\n",
    "last_message = custom_graph.get_state(custom_config).values[\"messages\"][-1]\n",
    "\n",
    "# 최신 메시지 tree 구조로 표시\n",
    "display_message_tree(last_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 가 작성한 내용\n",
    "print(last_message.tool_calls[0][\"args\"][\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인간의 검토 응답으로 재개\n",
    "human_command = Command(\n",
    "    resume={\"human_feedback\": \"2024년 노벨 문학상 수상자는 대한민국의 한강 작가입니다.\"}\n",
    ")\n",
    "\n",
    "stream_graph(custom_graph, inputs=human_command, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 상태 이력 관리\n",
    "\n",
    "체크포인트 기반으로 상태를 저장/복원하여 롤백/재실행합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "- State History: 상태 변경 이력 관리\n",
    "- Checkpoint ID: 특정 시점 식별자\n",
    "- Rollback: 지정 시점으로 복원\n",
    "- Resume: 복원 상태에서 재실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 체크포인트 기반 그래프 구성\n",
    "\n",
    "상태 이력 확인과 롤백/재실행을 위한 그래프를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 관리 테스트를 위한 체크포인트 기반 그래프\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 도구와 LLM 설정\n",
    "tools = [TavilySearch(max_results=2)]\n",
    "llm_with_tools_tt = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot_tt(state: State):\n",
    "    \"\"\"상태 관리 테스트용 챗봇\"\"\"\n",
    "    return {\"messages\": [llm_with_tools_tt.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# 그래프 구성\n",
    "graph_builder.add_node(\"chatbot\", chatbot_tt)\n",
    "tool_node_tt = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node_tt)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# 메모리와 함께 컴파일\n",
    "memory_tt = InMemorySaver()\n",
    "time_travel_graph = graph_builder.compile(checkpointer=memory_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "visualize_graph(time_travel_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 체크포인트 시퀀스 생성\n",
    "\n",
    "여러 번 대화를 실행하여 상태 이력을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_travel_config = RunnableConfig(configurable={\"thread_id\": \"time-travel-1\"})\n",
    "\n",
    "# 첫 번째 대화\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"테디노트에 대해서 조사 좀 해주세요.\")]},\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 번째 대화\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"테디노트 온라인 강의 주소를 조사 해주세요.\")]\n",
    "    },\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 이력 탐색\n",
    "\n",
    "`get_state_history`로 이력을 조회하고 롤백할 체크포인트를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 상태 히스토리 확인\n",
    "print(\"상태 히스토리 (최신순):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# to_replay 변수 초기화\n",
    "to_replay = None\n",
    "\n",
    "for i, state in enumerate(time_travel_graph.get_state_history(time_travel_config)):\n",
    "    print(f\"\\n[체크포인트 {i}]\")\n",
    "    print(f\"  다음 노드: {state.next}\")\n",
    "    print(f\"  체크포인트 ID: {state.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "    if len(state.values[\"messages\"]) == 6 and to_replay is None:\n",
    "        print(\"  이 상태로 되돌아갈 예정\")\n",
    "        display_message_tree(state.values[\"messages\"][-1])\n",
    "        to_replay = state\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 체크포인트로 롤백\n",
    "\n",
    "선택한 체크포인트로 상태를 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 수정\n",
    "\n",
    "복원된 상태에서 도구 호출 파라미터를 수정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import update_tool_call\n",
    "\n",
    "# 사용 예시:\n",
    "updated_message = update_tool_call(\n",
    "    to_replay.values[\"messages\"][-1],\n",
    "    tool_name=\"tavily_search\",\n",
    "    tool_args={\"query\": \"테디노트 온라인 강의 site:naver.com\", \"search_depth\": \"basic\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경하기 전의 message\n",
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경한 이후의 메시지 트리\n",
    "display_message_tree(updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경된 메시지를 update_state 로 업데이트\n",
    "updated_state = time_travel_graph.update_state(\n",
    "    values={\"messages\": [updated_message]}, config=to_replay.config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수정 상태 재실행\n",
    "\n",
    "업데이트된 상태로 재실행하여 결과를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업데이트된 메시지를 스트리밍 합니다.\n",
    "stream_graph(time_travel_graph, inputs=None, config=updated_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
