{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f947a944",
   "metadata": {},
   "source": [
    "# Kiwi BM25 토크나이징\n",
    "\n",
    "이 섹션에서는 Kiwi 형태소 분석기의 여러 기능을 활용하여 BM25 검색을 최적화합니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "1. **사용자 사전**: 도메인 특화 용어를 Kiwi에 추가하여 토크나이징 정확도 향상\n",
    "2. **품사 필터링**: 명사, 고유명사 등 의미있는 품사만 추출\n",
    "3. **불용어 제거**: 조사, 접속사 등 의미없는 토큰 제거\n",
    "4. **정규화**: 토큰 정규화 옵션으로 검색 재현율 향상\n",
    "5. **BM25 점수 보정**: Softmax 정규화 및 길이 패널티 적용\n",
    "\n",
    "## 참고 자료\n",
    "\n",
    "- [Kiwi GitHub](https://github.com/bab2min/kiwipiepy)\n",
    "- [rank-bm25](https://github.com/dorianbrown/rank_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6603b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "입력 텍스트: 근로기준법 제50조에 따라 연차유급휴가를 부여합니다.\n",
      "\n",
      "기본 토크나이징 결과:\n",
      "  형태소: 근로기준법           | 품사: NNP   | 시작:   0 | 길이:  5\n",
      "  형태소: 제               | 품사: XPN   | 시작:   6 | 길이:  1\n",
      "  형태소: 50              | 품사: SN    | 시작:   7 | 길이:  2\n",
      "  형태소: 조               | 품사: NNB   | 시작:   9 | 길이:  1\n",
      "  형태소: 에               | 품사: JKB   | 시작:  10 | 길이:  1\n",
      "  형태소: 따르              | 품사: VV    | 시작:  12 | 길이:  2\n",
      "  형태소: 어               | 품사: EC    | 시작:  13 | 길이:  1\n",
      "  형태소: 연차              | 품사: NNG   | 시작:  15 | 길이:  2\n",
      "  형태소: 유급              | 품사: NNG   | 시작:  17 | 길이:  2\n",
      "  형태소: 휴가              | 품사: NNG   | 시작:  19 | 길이:  2\n",
      "  형태소: 를               | 품사: JKO   | 시작:  21 | 길이:  1\n",
      "  형태소: 부여              | 품사: NNG   | 시작:  23 | 길이:  2\n",
      "  형태소: 하               | 품사: XSV   | 시작:  25 | 길이:  1\n",
      "  형태소: ᆸ니다             | 품사: EF    | 시작:  25 | 길이:  3\n",
      "  형태소: .               | 품사: SF    | 시작:  28 | 길이:  1\n",
      "\n",
      "================================================================================\n",
      "사용자 사전 추가 (도메인 특화 용어)\n",
      "================================================================================\n",
      "추가: 근로기준법 (NNP)\n",
      "추가: 연차유급휴가 (NNG)\n",
      "추가: 퇴직금 (NNG)\n",
      "추가: 야간근로 (NNG)\n",
      "추가: RAG (SL)\n",
      "추가: LangChain (SL)\n",
      "\n",
      "사용자 사전 적용 후:\n",
      "  형태소: 근로기준법           | 품사: NNP  \n",
      "  형태소: 제               | 품사: XPN  \n",
      "  형태소: 50              | 품사: SN   \n",
      "  형태소: 조               | 품사: NNB  \n",
      "  형태소: 에               | 품사: JKB  \n",
      "  형태소: 따르              | 품사: VV   \n",
      "  형태소: 어               | 품사: EC   \n",
      "  형태소: 연차유급휴가          | 품사: NNG  \n",
      "  형태소: 를               | 품사: JKO  \n",
      "  형태소: 부여              | 품사: NNG  \n",
      "  형태소: 하               | 품사: XSV  \n",
      "  형태소: ᆸ니다             | 품사: EF   \n",
      "  형태소: .               | 품사: SF   \n",
      "\n",
      "차이점:\n",
      "  기본: ['근로기준법', '제', '50', '조', '에', '따르', '어', '연차', '유급', '휴가', '를', '부여', '하', 'ᆸ니다', '.']\n",
      "  강화: ['근로기준법', '제', '50', '조', '에', '따르', '어', '연차유급휴가', '를', '부여', '하', 'ᆸ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Kiwi 초기화 및 사용자 사전\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 기본 토크나이징 테스트\n",
    "test_text = \"근로기준법 제50조에 따라 연차유급휴가를 부여합니다.\"\n",
    "basic_tokens = kiwi.tokenize(test_text)\n",
    "\n",
    "print(f\"\\n입력 텍스트: {test_text}\\n\")\n",
    "print(\"기본 토크나이징 결과:\")\n",
    "for token in basic_tokens:\n",
    "    print(\n",
    "        f\"  형태소: {token.form:15s} | 품사: {token.tag:5s} | 시작: {token.start:3d} | 길이: {token.len:2d}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"사용자 사전 추가 (도메인 특화 용어)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 법률/HR 도메인 용어 추가\n",
    "user_words = [\n",
    "    (\"근로기준법\", \"NNP\", 10.0),  # 고유명사\n",
    "    (\"연차유급휴가\", \"NNG\", 10.0),  # 일반명사\n",
    "    (\"퇴직금\", \"NNG\", 10.0),\n",
    "    (\"야간근로\", \"NNG\", 10.0),\n",
    "    (\"RAG\", \"SL\", 10.0),  # 외국어\n",
    "    (\"LangChain\", \"SL\", 10.0),\n",
    "]\n",
    "\n",
    "for word, tag, score in user_words:\n",
    "    kiwi.add_user_word(word, tag, score)\n",
    "    print(f\"추가: {word} ({tag})\")\n",
    "\n",
    "print(\"\\n사용자 사전 적용 후:\")\n",
    "enhanced_tokens = kiwi.tokenize(test_text)\n",
    "for token in enhanced_tokens:\n",
    "    print(f\"  형태소: {token.form:15s} | 품사: {token.tag:5s}\")\n",
    "\n",
    "print(\"\\n차이점:\")\n",
    "print(f\"  기본: {[t.form for t in basic_tokens]}\")\n",
    "print(f\"  강화: {[t.form for t in enhanced_tokens]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3676d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "입력: 근로기준법 제50조에 따라 1주 40시간을 초과할 수 없습니다.\n",
      "  기본 split(): ['근로기준법', '제50조에', '따라', '1주', '40시간을', '초과할', '수', '없습니다.']\n",
      "  Kiwi 기본: ['근로기준법', '제', '50', '조', '에', '따르', '어', '1', '주', '40', '시간', '을', '초과', '하', 'ᆯ', '수', '없', '습니다', '.']\n",
      "  Kiwi 고급: ['근로기준법', '50', '따르', '40', '시간', '초과']\n",
      "\n",
      "입력: 야간근로 수당은 통상임금의 50% 이상을 가산하여 지급합니다.\n",
      "  기본 split(): ['야간근로', '수당은', '통상임금의', '50%', '이상을', '가산하여', '지급합니다.']\n",
      "  Kiwi 기본: ['야간근로', '수당', '은', '통상', '임금', '의', '50', '%', '이상', '을', '가산', '하', '어', '지급', '하', 'ᆸ니다', '.']\n",
      "  Kiwi 고급: ['야간근로', '수당', '통상', '임금', '50', '이상', '가산', '지급']\n",
      "\n",
      "입력: RAG는 Retrieval-Augmented Generation의 약자입니다.\n",
      "  기본 split(): ['RAG는', 'Retrieval-Augmented', 'Generation의', '약자입니다.']\n",
      "  Kiwi 기본: ['RAG', '는', 'Retrieval', '-', 'Augmented', 'Generation', '의', '약자', '이', 'ᆸ니다', '.']\n",
      "  Kiwi 고급: ['rag', 'retrieval', 'augmented', 'generation', '약자']\n",
      "\n",
      "품사 필터링으로 검색 품질을 높일 수 있습니다:\n",
      "\n",
      "- 포함할 품사:\n",
      "  - NNG, NNP: 명사 (핵심 개념)\n",
      "  - VV, VA: 동사, 형용사 (행위/상태)\n",
      "  - SN: 숫자 (날짜, 금액 등)\n",
      "  - SL: 외국어 (기술 용어)\n",
      "\n",
      "- 제외할 품사:\n",
      "  - JKS, JKO: 조사 (이/가, 을/를)\n",
      "  - EC, EF: 어미 (연결어미, 종결어미)\n",
      "  - SF, SP: 구두점\n",
      "\n",
      "예시:\n",
      "  입력: \"근로기준법을 준수해야 합니다\"\n",
      "  전체: [\"근로기준법\", \"을\", \"준수\", \"하\", \"어야\", \"합니다\"]\n",
      "  필터: [\"근로기준법\", \"준수\"]  ← 의미 있는 토큰만\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. 품사 필터링 및 불용어 제거\n",
    "# ============================================================================\n",
    "# 한국어 품사 태그 참고: https://github.com/bab2min/kiwipiepy#품사-태그\n",
    "# NNG: 일반명사, NNP: 고유명사, VV: 동사, VA: 형용사, MM: 관형사, MAG: 일반부사\n",
    "\n",
    "# 의미있는 품사 정의\n",
    "MEANINGFUL_POS = {\n",
    "    \"NNG\",  # 일반명사\n",
    "    \"NNP\",  # 고유명사\n",
    "    \"NNB\",  # 의존명사\n",
    "    \"VV\",  # 동사\n",
    "    \"VA\",  # 형용사\n",
    "    \"MAG\",  # 일반부사\n",
    "    \"SN\",  # 숫자\n",
    "    \"SL\",  # 외국어\n",
    "}\n",
    "\n",
    "# 불용어 리스트\n",
    "STOPWORDS = {\n",
    "    \"것\",\n",
    "    \"수\",\n",
    "    \"등\",\n",
    "    \"및\",\n",
    "    \"내\",\n",
    "    \"또는\",\n",
    "    \"이\",\n",
    "    \"그\",\n",
    "    \"저\",\n",
    "    \"이다\",\n",
    "    \"되다\",\n",
    "    \"하다\",\n",
    "    \"있다\",\n",
    "    \"없다\",\n",
    "    \"같다\",\n",
    "}\n",
    "\n",
    "\n",
    "def advanced_kiwi_tokenize(\n",
    "    text: str,\n",
    "    *,\n",
    "    pos_filter: set[str] = MEANINGFUL_POS,\n",
    "    stopwords: set[str] = STOPWORDS,\n",
    "    normalize: bool = True,\n",
    "    min_length: int = 2,\n",
    "    keep_numbers: bool = True,\n",
    "    keep_english: bool = True,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Kiwi 토크나이징: 품사 필터링, 불용어 제거, 정규화 등을 적용합니다.\n",
    "\n",
    "    Args:\n",
    "        text: 토크나이징할 텍스트\n",
    "        pos_filter: 허용할 품사 태그 집합\n",
    "        stopwords: 제거할 불용어 집합\n",
    "        normalize: 토큰 정규화 여부 (소문자 변환 등)\n",
    "        min_length: 최소 토큰 길이\n",
    "        keep_numbers: 숫자 유지 여부\n",
    "        keep_english: 영문 유지 여부\n",
    "\n",
    "    Returns:\n",
    "        필터링 및 정규화된 토큰 리스트\n",
    "    \"\"\"\n",
    "    tokens = kiwi.tokenize(text)\n",
    "\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # 1. 품사 필터링\n",
    "        if token.tag not in pos_filter:\n",
    "            continue\n",
    "\n",
    "        # 2. 불용어 제거\n",
    "        if token.form in stopwords:\n",
    "            continue\n",
    "\n",
    "        # 3. 길이 필터\n",
    "        if len(token.form) < min_length:\n",
    "            continue\n",
    "\n",
    "        # 4. 숫자 필터\n",
    "        if not keep_numbers and token.tag == \"SN\":\n",
    "            continue\n",
    "\n",
    "        # 5. 영문 필터\n",
    "        if not keep_english and token.tag == \"SL\":\n",
    "            continue\n",
    "\n",
    "        # 6. 정규화\n",
    "        form = token.form\n",
    "        if normalize:\n",
    "            # 소문자 변환 (영문)\n",
    "            if re.match(r\"^[A-Za-z]+$\", form):\n",
    "                form = form.lower()\n",
    "\n",
    "        filtered_tokens.append(form)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# 테스트\n",
    "test_texts = [\n",
    "    \"근로기준법 제50조에 따라 1주 40시간을 초과할 수 없습니다.\",\n",
    "    \"야간근로 수당은 통상임금의 50% 이상을 가산하여 지급합니다.\",\n",
    "    \"RAG는 Retrieval-Augmented Generation의 약자입니다.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\n입력: {text}\")\n",
    "\n",
    "    # 기본 split()\n",
    "    basic = text.split()\n",
    "    print(f\"  기본 split(): {basic}\")\n",
    "\n",
    "    # 기본 Kiwi (필터 없음)\n",
    "    kiwi_basic = [t.form for t in kiwi.tokenize(text)]\n",
    "    print(f\"  Kiwi 기본: {kiwi_basic}\")\n",
    "\n",
    "    # 고급 Kiwi (품사 필터 + 불용어)\n",
    "    kiwi_advanced = advanced_kiwi_tokenize(text)\n",
    "    print(f\"  Kiwi 고급: {kiwi_advanced}\")\n",
    "\n",
    "print(\"\"\"\n",
    "품사 필터링으로 검색 품질을 높일 수 있습니다:\n",
    "\n",
    "- 포함할 품사:\n",
    "  - NNG, NNP: 명사 (핵심 개념)\n",
    "  - VV, VA: 동사, 형용사 (행위/상태)\n",
    "  - SN: 숫자 (날짜, 금액 등)\n",
    "  - SL: 외국어 (기술 용어)\n",
    "\n",
    "- 제외할 품사:\n",
    "  - JKS, JKO: 조사 (이/가, 을/를)\n",
    "  - EC, EF: 어미 (연결어미, 종결어미)\n",
    "  - SF, SP: 구두점\n",
    "\n",
    "예시:\n",
    "  입력: \"근로기준법을 준수해야 합니다\"\n",
    "  전체: [\"근로기준법\", \"을\", \"준수\", \"하\", \"어야\", \"합니다\"]\n",
    "  필터: [\"근로기준법\", \"준수\"]  ← 의미 있는 토큰만\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d98f56",
   "metadata": {},
   "source": [
    "## Kiwi 형태소 분석기 활용 가이드\n",
    "\n",
    "### 왜 Kiwi 형태소 분석기를 써야 하는가?\n",
    "\n",
    "**Kiwi 형태소 분석기**는 한국어 검색에서 **필수**입니다:\n",
    "1. **정확한 토큰화**: \"먹었습니다\" → [\"먹\", \"었\", \"습니다\"] (형태소 분리)\n",
    "2. **불용어 제거**: 검색에 불필요한 조사, 어미 제거\n",
    "3. **품사 필터링**: 명사, 동사만 추출하여 BM25 정확도 ↑\n",
    "4. **사용자 사전**: 도메인 특화 용어 (RAG, LLM, 벡터DB 등) 추가\n",
    "5. 현재까지 관리되고 있는 유일한 오픈소스 형태소 분석기 입니다.\n",
    "\n",
    "**Sparse 검색 (BM25) vs Dense 검색 (임베딩)**:\n",
    "| 특성 | Dense (임베딩) | Sparse (BM25 + Kiwi) |\n",
    "|------|----------------|----------------------|\n",
    "| 의미 유사도 |  우수 |  없음 |\n",
    "| 정확한 키워드 |  약함 |  우수 |\n",
    "| 고유명사 |  약함 |  우수 |\n",
    "| 숫자/날짜 |  약함 |  우수 |\n",
    "| 한국어 | LM 모델 의존 |  Kiwi 필수 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d52af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Kiwi 고급 기능을 모두 활용하는 BM25 Retriever.\n",
    "\n",
    "이 모듈은 Kiwi v0.6의 모든 기능을 지원하도록 구현했습니다:\n",
    "- CoNg, SkipBigram 등 한국어에 특화된 신경망 기반 언어 모델 활용 가능\n",
    "- 세종 품사 태그 기반 필터링\n",
    "- 오타 교정 (basic, continual, lengthening)\n",
    "- 불용어 필터링\n",
    "- 사용자 사전 관리\n",
    "- 문장 분리 통합\n",
    "- 다양한 정규화 옵션\n",
    "- NgramExtractor 기반 다어절 보강\n",
    "\n",
    "References:\n",
    "    - Kiwi GitHub: https://github.com/bab2min/kiwipiepy\n",
    "\"\"\"\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal\n",
    "\n",
    "import numpy as np\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from pydantic import ConfigDict, Field\n",
    "\n",
    "try:\n",
    "    from kiwipiepy import Kiwi, NgramExtractor\n",
    "    from kiwipiepy.utils import Stopwords\n",
    "except ImportError:\n",
    "    print(\"Could not import kiwipiepy, please install with `pip install kiwipiepy`.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except ImportError:\n",
    "    print(\"Could not import rank_bm25, please install with `pip install rank-bm25`.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 세종 품사 태그 상수 및 편의 함수\n",
    "# ============================================================================\n",
    "\n",
    "# 세종 품사 태그 체계\n",
    "# 대분류별 품사 태그 정의\n",
    "SEJONG_POS_TAGS = {\n",
    "    # 체언 (Noun)\n",
    "    \"NOUN\": {\n",
    "        \"NNG\": \"일반 명사\",\n",
    "        \"NNP\": \"고유 명사\",\n",
    "        \"NNB\": \"의존 명사\",\n",
    "        \"NR\": \"수사\",\n",
    "        \"NP\": \"대명사\",\n",
    "    },\n",
    "    # 용언 (Verb/Adjective)\n",
    "    \"PREDICATE\": {\n",
    "        \"VV\": \"동사\",\n",
    "        \"VA\": \"형용사\",\n",
    "        \"VX\": \"보조 용언\",\n",
    "        \"VCP\": \"긍정 지시사(이다)\",\n",
    "        \"VCN\": \"부정 지시사(아니다)\",\n",
    "    },\n",
    "    # 관형사\n",
    "    \"DETERMINER\": {\n",
    "        \"MM\": \"관형사\",\n",
    "    },\n",
    "    # 부사\n",
    "    \"ADVERB\": {\n",
    "        \"MAG\": \"일반 부사\",\n",
    "        \"MAJ\": \"접속 부사\",\n",
    "    },\n",
    "    # 감탄사\n",
    "    \"INTERJECTION\": {\n",
    "        \"IC\": \"감탄사\",\n",
    "    },\n",
    "    # 조사\n",
    "    \"PARTICLE\": {\n",
    "        \"JKS\": \"주격 조사\",\n",
    "        \"JKC\": \"보격 조사\",\n",
    "        \"JKG\": \"관형격 조사\",\n",
    "        \"JKO\": \"목적격 조사\",\n",
    "        \"JKB\": \"부사격 조사\",\n",
    "        \"JKV\": \"호격 조사\",\n",
    "        \"JKQ\": \"인용격 조사\",\n",
    "        \"JX\": \"보조사\",\n",
    "        \"JC\": \"접속 조사\",\n",
    "    },\n",
    "    # 어미\n",
    "    \"ENDING\": {\n",
    "        \"EP\": \"선어말 어미\",\n",
    "        \"EF\": \"종결 어미\",\n",
    "        \"EC\": \"연결 어미\",\n",
    "        \"ETN\": \"명사형 전성 어미\",\n",
    "        \"ETM\": \"관형형 전성 어미\",\n",
    "    },\n",
    "    # 접두사\n",
    "    \"PREFIX\": {\n",
    "        \"XPN\": \"체언 접두사\",\n",
    "    },\n",
    "    # 접미사\n",
    "    \"SUFFIX\": {\n",
    "        \"XSN\": \"명사 파생 접미사\",\n",
    "        \"XSV\": \"동사 파생 접미사\",\n",
    "        \"XSA\": \"형용사 파생 접미사\",\n",
    "        \"XSM\": \"부사 파생 접미사\",  # Kiwi 독자적 태그\n",
    "    },\n",
    "    # 어근\n",
    "    \"ROOT\": {\n",
    "        \"XR\": \"어근\",\n",
    "    },\n",
    "    # 부호, 외국어, 특수문자\n",
    "    \"SYMBOL\": {\n",
    "        \"SF\": \"종결 부호(. ! ?)\",\n",
    "        \"SP\": \"구분 부호(, / : ;)\",\n",
    "        \"SS\": \"인용 부호 및 괄호\",\n",
    "        \"SSO\": \"SS 중 여는 부호\",  # Kiwi 독자적 태그\n",
    "        \"SSC\": \"SS 중 닫는 부호\",  # Kiwi 독자적 태그\n",
    "        \"SE\": \"줄임표(…)\",\n",
    "        \"SO\": \"붙임표(- ~)\",\n",
    "        \"SW\": \"기타 특수 문자\",\n",
    "        \"SL\": \"알파벳(A-Z a-z)\",\n",
    "        \"SH\": \"한자\",\n",
    "        \"SN\": \"숫자(0-9)\",\n",
    "        \"SB\": \"순서 있는 글머리\",  # Kiwi 독자적 태그\n",
    "    },\n",
    "    # 분석 불능\n",
    "    \"UNKNOWN\": {\n",
    "        \"UN\": \"분석 불능\",  # Kiwi 독자적 태그\n",
    "    },\n",
    "    # 웹 관련\n",
    "    \"WEB\": {\n",
    "        \"W_URL\": \"URL 주소\",  # Kiwi 독자적 태그\n",
    "        \"W_EMAIL\": \"이메일 주소\",  # Kiwi 독자적 태그\n",
    "        \"W_HASHTAG\": \"해시태그(#abcd)\",  # Kiwi 독자적 태그\n",
    "        \"W_MENTION\": \"멘션(@abcd)\",  # Kiwi 독자적 태그\n",
    "        \"W_SERIAL\": \"일련번호(전화번호, 통장번호, IP주소 등)\",  # Kiwi 독자적 태그\n",
    "        \"W_EMOJI\": \"이모지\",  # Kiwi 독자적 태그\n",
    "    },\n",
    "    # 기타\n",
    "    \"OTHER\": {\n",
    "        \"Z_CODA\": \"덧붙은 받침\",  # Kiwi 독자적 태그\n",
    "        \"Z_SIOT\": \"사이시옷\",  # Kiwi 독자적 태그\n",
    "        \"USER0\": \"사용자 정의 태그 0\",  # Kiwi 독자적 태그\n",
    "        \"USER1\": \"사용자 정의 태그 1\",  # Kiwi 독자적 태그\n",
    "        \"USER2\": \"사용자 정의 태그 2\",  # Kiwi 독자적 태그\n",
    "        \"USER3\": \"사용자 정의 태그 3\",  # Kiwi 독자적 태그\n",
    "        \"USER4\": \"사용자 정의 태그 4\",  # Kiwi 독자적 태그\n",
    "    },\n",
    "}\n",
    "\n",
    "# 모든 품사 태그 집합\n",
    "ALL_POS_TAGS: set[str] = set()\n",
    "for category in SEJONG_POS_TAGS.values():\n",
    "    ALL_POS_TAGS.update(category.keys())\n",
    "\n",
    "# 품사 태그 설명 딕셔너리 (태그 -> 설명)\n",
    "POS_TAG_DESCRIPTIONS: dict[str, str] = {}\n",
    "for category in SEJONG_POS_TAGS.values():\n",
    "    POS_TAG_DESCRIPTIONS.update(category)\n",
    "\n",
    "\n",
    "def get_pos_tags_by_category(category: str) -> set[str]:\n",
    "    \"\"\"대분류에 속한 모든 품사 태그를 반환한다.\n",
    "\n",
    "    Args:\n",
    "        category: 대분류 이름 ('NOUN', 'PREDICATE', 'PARTICLE' 등)\n",
    "\n",
    "    Returns:\n",
    "        해당 대분류에 속한 품사 태그 집합\n",
    "\n",
    "    Examples:\n",
    "        >>> get_pos_tags_by_category(\"NOUN\")\n",
    "        {'NNG', 'NNP', 'NNB', 'NR', 'NP'}\n",
    "        >>> get_pos_tags_by_category(\"PREDICATE\")\n",
    "        {'VV', 'VA', 'VX', 'VCP', 'VCN'}\n",
    "    \"\"\"\n",
    "    return set(SEJONG_POS_TAGS.get(category, {}).keys())\n",
    "\n",
    "\n",
    "def get_pos_description(tag: str) -> str:\n",
    "    \"\"\"품사 태그의 설명을 반환한다.\n",
    "\n",
    "    Args:\n",
    "        tag: 품사 태그 (예: 'NNG', 'VV')\n",
    "\n",
    "    Returns:\n",
    "        품사 태그 설명\n",
    "\n",
    "    Examples:\n",
    "        >>> get_pos_description(\"NNG\")\n",
    "        '일반 명사'\n",
    "        >>> get_pos_description(\"VV\")\n",
    "        '동사'\n",
    "    \"\"\"\n",
    "    return POS_TAG_DESCRIPTIONS.get(tag, f\"알 수 없는 품사 태그: {tag}\")\n",
    "\n",
    "\n",
    "def get_default_important_pos() -> set[str]:\n",
    "    \"\"\"BM25 검색에 기본적으로 사용할 품사 집합을 반환한다.\n",
    "\n",
    "    명사, 동사, 형용사, 영어, 한자, 숫자 등 의미 있는 단어만 포함한다.\n",
    "\n",
    "    Returns:\n",
    "        기본 품사 태그 집합\n",
    "\n",
    "    Examples:\n",
    "        >>> pos_set = get_default_important_pos()\n",
    "        >>> \"NNG\" in pos_set\n",
    "        True\n",
    "        >>> \"JKS\" in pos_set  # 조사는 제외\n",
    "        False\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"NNG\",  # 일반 명사\n",
    "        \"NNP\",  # 고유 명사\n",
    "        \"NNB\",  # 의존 명사\n",
    "        \"VV\",  # 동사\n",
    "        \"VA\",  # 형용사\n",
    "        \"SL\",  # 알파벳\n",
    "        \"SH\",  # 한자\n",
    "        \"SN\",  # 숫자\n",
    "    }\n",
    "\n",
    "\n",
    "def get_noun_only_pos() -> set[str]:\n",
    "    \"\"\"명사만 포함하는 품사 집합을 반환한다.\n",
    "\n",
    "    Returns:\n",
    "        명사 품사 태그 집합\n",
    "\n",
    "    Examples:\n",
    "        >>> pos_set = get_noun_only_pos()\n",
    "        >>> pos_set == {\"NNG\", \"NNP\", \"NNB\", \"NR\", \"NP\"}\n",
    "        True\n",
    "    \"\"\"\n",
    "    return get_pos_tags_by_category(\"NOUN\")\n",
    "\n",
    "\n",
    "def get_content_word_pos() -> set[str]:\n",
    "    \"\"\"내용어(명사, 동사, 형용사)만 포함하는 품사 집합을 반환한다.\n",
    "\n",
    "    Returns:\n",
    "        내용어 품사 태그 집합\n",
    "\n",
    "    Examples:\n",
    "        >>> pos_set = get_content_word_pos()\n",
    "        >>> \"NNG\" in pos_set and \"VV\" in pos_set and \"VA\" in pos_set\n",
    "        True\n",
    "    \"\"\"\n",
    "    return get_pos_tags_by_category(\"NOUN\") | get_pos_tags_by_category(\"PREDICATE\")\n",
    "\n",
    "\n",
    "def get_all_content_pos() -> set[str]:\n",
    "    \"\"\"모든 내용어(명사, 동사, 형용사, 부사, 관형사)를 포함하는 품사 집합을 반환한다.\n",
    "\n",
    "    Returns:\n",
    "        모든 내용어 품사 태그 집합\n",
    "    \"\"\"\n",
    "    return (\n",
    "        get_pos_tags_by_category(\"NOUN\")\n",
    "        | get_pos_tags_by_category(\"PREDICATE\")\n",
    "        | get_pos_tags_by_category(\"ADVERB\")\n",
    "        | get_pos_tags_by_category(\"DETERMINER\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Kiwi 싱글톤 캐시 (모델별)\n",
    "_KIWI_INSTANCES: dict[str, Kiwi] = {}\n",
    "\n",
    "\n",
    "def get_kiwi_instance(\n",
    "    model_type: str = \"knlm\",\n",
    "    typos: str | None = None,\n",
    "    model_path: str | None = None,\n",
    "    load_default_dict: bool = True,\n",
    "    load_typo_dict: bool = True,\n",
    "    load_multi_dict: bool = True,\n",
    "    integrate_allomorph: bool = True,\n",
    "    typo_cost_threshold: float = 2.5,\n",
    "    num_workers: int = -1,\n",
    ") -> Kiwi:\n",
    "    \"\"\"Kiwi 인스턴스를 캐시하여 반환한다.\n",
    "\n",
    "    동일한 설정의 Kiwi 인스턴스는 재사용됩니다.\n",
    "\n",
    "    Args:\n",
    "        model_type: 언어 모델 타입 ('knlm', 'sbg', 'cong', 'cong-global', 'none', 'largest')\n",
    "        typos: 오타 교정 모드 ('basic', 'continual', 'basic_with_continual',\n",
    "            'lengthening', 'basic_with_continual_and_lengthening')\n",
    "        model_path: 모델 경로 (CoNg 사용 시 필요)\n",
    "        load_default_dict: 기본 사전 로드 여부 (위키백과 표제어)\n",
    "        load_typo_dict: 내장 오타 사전 로드 여부\n",
    "        load_multi_dict: 다어절 사전 로드 여부 (WikiData 고유명사)\n",
    "        integrate_allomorph: 음운론적 이형태 통합 여부 (아/어, 았/었 등)\n",
    "        typo_cost_threshold: 오타 교정 최대 비용 (기본값: 2.5)\n",
    "        num_workers: 워커 수 (-1=모든 코어, 0=단일 스레드)\n",
    "\n",
    "    Returns:\n",
    "        Kiwi 인스턴스\n",
    "\n",
    "    Examples:\n",
    "        >>> # 기본 설정\n",
    "        >>> kiwi = get_kiwi_instance()\n",
    "        >>> # CoNg 모델 사용\n",
    "        >>> kiwi = get_kiwi_instance(model_type=\"cong\", model_path=\"./cong-base\")\n",
    "        >>> # 오타 교정 활성화\n",
    "        >>> kiwi = get_kiwi_instance(typos=\"basic_with_continual_and_lengthening\")\n",
    "    \"\"\"\n",
    "    cache_key = (\n",
    "        f\"{model_type}_{typos}_{model_path}_{load_default_dict}_\"\n",
    "        f\"{load_typo_dict}_{load_multi_dict}_{integrate_allomorph}_\"\n",
    "        f\"{typo_cost_threshold}_{num_workers}\"\n",
    "    )\n",
    "\n",
    "    if cache_key not in _KIWI_INSTANCES:\n",
    "        _KIWI_INSTANCES[cache_key] = Kiwi(\n",
    "            model_type=model_type,\n",
    "            typos=typos,\n",
    "            model_path=model_path,\n",
    "            load_default_dict=load_default_dict,\n",
    "            load_typo_dict=load_typo_dict,\n",
    "            load_multi_dict=load_multi_dict,\n",
    "            integrate_allomorph=integrate_allomorph,\n",
    "            typo_cost_threshold=typo_cost_threshold,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    return _KIWI_INSTANCES[cache_key]\n",
    "\n",
    "\n",
    "def advanced_kiwi_tokenizer(\n",
    "    text: str,\n",
    "    kiwi: Kiwi,\n",
    "    important_pos: set[str] | None = None,\n",
    "    stopwords: Stopwords | None = None,\n",
    "    normalize_coda: bool = False,\n",
    "    z_coda: bool = True,\n",
    "    compatible_jamo: bool = False,\n",
    "    saisiot: bool | None = None,\n",
    "    min_token_len: int = 1,\n",
    "    blocklist: set[tuple[str, str]] | None = None,\n",
    ") -> list[str]:\n",
    "    \"\"\"Kiwi의 고급 기능을 모두 활용하는 토크나이저.\n",
    "\n",
    "    세종 품사 태그를 기반으로 의미 있는 단어만 추출하여 BM25 검색 성능을 향상시킵니다.\n",
    "    기본적으로 명사, 동사, 형용사, 영어, 한자, 숫자만 포함합니다.\n",
    "\n",
    "    Args:\n",
    "        text: 토큰화할 텍스트\n",
    "        kiwi: Kiwi 인스턴스\n",
    "        important_pos: 사용할 품사 집합 (None이면 기본값 사용)\n",
    "        - 기본값: get_default_important_pos() 반환값\n",
    "        - 편의 함수 사용 예:\n",
    "            * get_noun_only_pos(): 명사만\n",
    "            * get_content_word_pos(): 명사, 동사, 형용사\n",
    "            * get_all_content_pos(): 모든 내용어\n",
    "        - 세종 품사 태그 참고:\n",
    "            * NNG: 일반 명사, NNP: 고유 명사, NNB: 의존 명사\n",
    "            * VV: 동사, VA: 형용사\n",
    "            * SL: 알파벳, SH: 한자, SN: 숫자\n",
    "            * 전체 태그는 SEJONG_POS_TAGS 상수 참고\n",
    "        stopwords: 불용어 객체 (kiwipiepy.utils.Stopwords)\n",
    "        normalize_coda: 덧붙은 받침 정규화 (ㅋㅋㅋ 처리)\n",
    "            True일 경우 \"안 먹었엌ㅋㅋ\" -> \"안 먹었어 ㅋㅋㅋ\"로 정규화\n",
    "        z_coda: 덧붙은 받침을 Z_CODA 태그로 분리\n",
    "            True일 경우 덧붙은 받침을 별도 토큰으로 분리\n",
    "        compatible_jamo: 받침을 호환용 자모로 출력\n",
    "            True일 경우 받침을 자모 단위로 분해\n",
    "        saisiot: 사이시옷 처리\n",
    "            - True: 사이시옷을 분리 (예: \"나뭇잎\" -> \"나무/시옷/잎\")\n",
    "            - False: 사이시옷을 통합\n",
    "            - None: 기본값 사용\n",
    "        min_token_len: 최소 토큰 길이 (기본값: 1)\n",
    "        blocklist: 제외할 (형태, 품사) 쌍 집합\n",
    "            예: {(\"것\", \"NNB\"), (\"수\", \"NNB\")}  # 의존 명사 제외\n",
    "\n",
    "    Returns:\n",
    "        토큰 리스트 (품사 필터링 및 불용어 제거 후)\n",
    "\n",
    "    Examples:\n",
    "        >>> from kiwipiepy import Kiwi\n",
    "        >>> from kiwipiepy.utils import Stopwords\n",
    "        >>> kiwi = Kiwi()\n",
    "        >>>\n",
    "        >>> # 기본 토큰화 (명사, 동사, 형용사, 영어, 한자, 숫자)\n",
    "        >>> advanced_kiwi_tokenizer(\"한국어를 분석합니다\", kiwi)\n",
    "        ['한국어', '분석']\n",
    "        >>>\n",
    "        >>> # 명사만 추출\n",
    "        >>> from day2.retrieval.kiwi_bm25_advanced import get_noun_only_pos\n",
    "        >>> advanced_kiwi_tokenizer(\"한국어를 분석합니다\", kiwi, important_pos=get_noun_only_pos())\n",
    "        ['한국어']\n",
    "        >>>\n",
    "        >>> # 불용어 제거\n",
    "        >>> stopwords = Stopwords()\n",
    "        >>> advanced_kiwi_tokenizer(\"이것은 테스트입니다\", kiwi, stopwords=stopwords)\n",
    "        ['테스트']\n",
    "        >>>\n",
    "        >>> # 오타 정규화\n",
    "        >>> advanced_kiwi_tokenizer(\"안 먹었엌ㅋㅋ\", kiwi, normalize_coda=True)\n",
    "        ['안', '먹', '었', '어', 'ㅋㅋㅋ']\n",
    "        >>>\n",
    "        >>> # 내용어만 (명사, 동사, 형용사)\n",
    "        >>> from day2.retrieval.kiwi_bm25_advanced import get_content_word_pos\n",
    "        >>> advanced_kiwi_tokenizer(\"빠르게 달린다\", kiwi, important_pos=get_content_word_pos())\n",
    "        ['빠르', '달리']\n",
    "    \"\"\"\n",
    "    if important_pos is None:\n",
    "        # 기본: 명사, 동사, 형용사, 영어, 한자, 숫자\n",
    "        important_pos = get_default_important_pos()\n",
    "\n",
    "    # Kiwi tokenize 호출\n",
    "    tokens = kiwi.tokenize(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        normalize_coda=normalize_coda,\n",
    "        z_coda=z_coda,\n",
    "        compatible_jamo=compatible_jamo,\n",
    "        saisiot=saisiot,\n",
    "    )\n",
    "\n",
    "    # 품사 필터링 및 토큰 추출\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        # 품사 체크\n",
    "        if token.tag not in important_pos:\n",
    "            continue\n",
    "\n",
    "        # Blocklist 체크\n",
    "        if blocklist and (token.form, token.tag) in blocklist:\n",
    "            continue\n",
    "\n",
    "        # 최소 길이 필터\n",
    "        if len(token.form) >= min_token_len:\n",
    "            result.append(token.form)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class KiwiBM25Retriever(BaseRetriever):\n",
    "    \"\"\"Kiwi 기능을 모두 활용하는 BM25 Retriever.\n",
    "\n",
    "    LangChain BaseRetriever를 직접 상속하며, Kiwi v0.6x의 모든 기능을 지원합니다.\n",
    "\n",
    "    Attributes:\n",
    "        vectorizer: BM25Okapi 인스턴스\n",
    "        docs: 문서 리스트\n",
    "        k: 반환할 문서 수\n",
    "\n",
    "        # Kiwi 설정\n",
    "        model_type: 언어 모델 ('knlm', 'sbg', 'cong')\n",
    "        typos: 오타 교정 모드\n",
    "        model_path: 모델 경로\n",
    "        load_default_dict: 기본 사전 로드 여부\n",
    "        num_workers: 워커 수\n",
    "\n",
    "        # 토큰화 설정\n",
    "        important_pos: 사용할 품사 집합\n",
    "        stopwords: 불용어 객체\n",
    "        normalize_coda: 덧붙은 받침 정규화\n",
    "        z_coda: 덧붙은 받침 분리\n",
    "        compatible_jamo: 호환용 자모 사용\n",
    "        saisiot: 사이시옷 처리\n",
    "        min_token_len: 최소 토큰 길이\n",
    "        space_tolerance: 공백 허용도\n",
    "\n",
    "        # BM25 파라미터\n",
    "        bm25_k1: BM25 k1 파라미터\n",
    "        bm25_b: BM25 b 파라미터\n",
    "\n",
    "    Examples:\n",
    "        >>> # 기본 사용\n",
    "        >>> retriever = KiwiBM25Retriever.from_documents(docs)\n",
    "        >>> results = retriever.invoke(\"검색 쿼리\")\n",
    "\n",
    "        >>> # CoNg 모델 + 오타 교정\n",
    "        >>> retriever = KiwiBM25Retriever.from_documents(docs, model_type=\"cong\", typos=\"basic_with_continual\")\n",
    "\n",
    "        >>> # 명사만 + 불용어 제거\n",
    "        >>> stopwords = Stopwords()\n",
    "        >>> retriever = KiwiBM25Retriever.from_documents(\n",
    "        ...     docs, important_pos={\"NNG\", \"NNP\"}, stopwords=stopwords\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    vectorizer: Any = Field(default=None, repr=False)\n",
    "    \"\"\"BM25 vectorizer\"\"\"\n",
    "\n",
    "    docs: list[Document] = Field(default_factory=list, repr=False)\n",
    "    \"\"\"문서 리스트\"\"\"\n",
    "\n",
    "    k: int = 5\n",
    "    \"\"\"반환할 문서 수\"\"\"\n",
    "\n",
    "    # Kiwi 설정\n",
    "    model_type: Literal[\"knlm\", \"sbg\", \"cong\"] = \"knlm\"\n",
    "    \"\"\"언어 모델 타입\n",
    "    - 'knlm': 기본 언어 모델 (권장)\n",
    "    - 'sbg': SkipBigram 모델\n",
    "    - 'cong': CoNg 모델 (고정밀도, model_path 필요)\n",
    "    \"\"\"\n",
    "\n",
    "    typos: (\n",
    "        Literal[\n",
    "            \"basic\",\n",
    "            \"continual\",\n",
    "            \"basic_with_continual\",\n",
    "            \"lengthening\",\n",
    "            \"basic_with_continual_and_lengthening\",\n",
    "        ]\n",
    "        | None\n",
    "    ) = \"basic_with_continual_and_lengthening\"\n",
    "    \"\"\"오타 교정 모드\n",
    "    - 'basic': 기본 오타 교정\n",
    "    - 'continual': 연철 오타 교정\n",
    "    - 'basic_with_continual': 기본 + 연철\n",
    "    - 'lengthening': 장음화 오타 교정\n",
    "    - 'basic_with_continual_and_lengthening': 모든 오타 교정 (권장)\n",
    "    \"\"\"\n",
    "\n",
    "    model_path: str | None = None\n",
    "    \"\"\"모델 경로 (CoNg 사용 시 필요)\"\"\"\n",
    "\n",
    "    load_default_dict: bool = True\n",
    "    \"\"\"기본 사전 로드 (위키백과 표제어)\"\"\"\n",
    "\n",
    "    load_typo_dict: bool = True\n",
    "    \"\"\"내장 오타 사전 로드 여부\"\"\"\n",
    "\n",
    "    load_multi_dict: bool = True\n",
    "    \"\"\"다어절 사전 로드 여부 (WikiData 고유명사)\"\"\"\n",
    "\n",
    "    integrate_allomorph: bool = True\n",
    "    \"\"\"음운론적 이형태 통합 여부 (아/어, 았/었 등)\"\"\"\n",
    "\n",
    "    typo_cost_threshold: float = 2.5\n",
    "    \"\"\"오타 교정 최대 비용\"\"\"\n",
    "\n",
    "    num_workers: int = -1\n",
    "    \"\"\"워커 수 (-1=모든 코어, 0=단일 스레드)\"\"\"\n",
    "\n",
    "    # 토큰화 설정\n",
    "    important_pos: set[str] | None = None\n",
    "    \"\"\"사용할 품사 집합\"\"\"\n",
    "\n",
    "    stopwords: Stopwords | None = None\n",
    "    \"\"\"불용어 객체 (Stopwords)\"\"\"\n",
    "\n",
    "    normalize_coda: bool = False\n",
    "    \"\"\"덧붙은 받침 정규화 (ㅋㅋㅋ 처리)\"\"\"\n",
    "\n",
    "    z_coda: bool = True\n",
    "    \"\"\"덧붙은 받침을 Z_CODA로 분리\"\"\"\n",
    "\n",
    "    compatible_jamo: bool = False\n",
    "    \"\"\"받침을 호환용 자모로 출력\"\"\"\n",
    "\n",
    "    saisiot: bool | None = None\n",
    "    \"\"\"사이시옷 처리 (True: 분리, False: 통합)\"\"\"\n",
    "\n",
    "    min_token_len: int = 1\n",
    "    \"\"\"최소 토큰 길이\"\"\"\n",
    "\n",
    "    space_tolerance: int = 0\n",
    "    \"\"\"공백 허용도 (형태소 내 공백 허용 개수)\"\"\"\n",
    "\n",
    "    blocklist: set[tuple[str, str]] | None = None\n",
    "    \"\"\"제외할 (형태, 품사) 쌍 집합\"\"\"\n",
    "\n",
    "    # NgramExtractor 기반 다어절 보강 설정\n",
    "    enable_ngram_enrichment: bool = False\n",
    "    \"\"\"NgramExtractor를 사용한 다어절 보강 활성화 여부\"\"\"\n",
    "\n",
    "    ngram_min_cnt: int = 10\n",
    "    \"\"\"Ngram 추출 최소 출현 빈도\"\"\"\n",
    "\n",
    "    ngram_max_length: int = 5\n",
    "    \"\"\"Ngram 최대 길이\"\"\"\n",
    "\n",
    "    ngram_min_score: float = 1e-3\n",
    "    \"\"\"Ngram 최소 점수\"\"\"\n",
    "\n",
    "    ngram_auto_add: bool = True\n",
    "    \"\"\"추출된 Ngram을 자동으로 사용자 사전에 추가할지 여부\"\"\"\n",
    "\n",
    "    # BM25 파라미터\n",
    "    bm25_k1: float = 1.5\n",
    "    \"\"\"BM25 k1 파라미터\"\"\"\n",
    "\n",
    "    bm25_b: float = 0.75\n",
    "    \"\"\"BM25 b 파라미터\"\"\"\n",
    "\n",
    "    # 내부 상태\n",
    "    _kiwi: Kiwi | None = None\n",
    "    \"\"\"Kiwi 인스턴스 (내부용)\"\"\"\n",
    "\n",
    "    def _get_kiwi(self) -> Kiwi:\n",
    "        \"\"\"Kiwi 인스턴스를 가져온다 (lazy initialization).\"\"\"\n",
    "        if self._kiwi is None:\n",
    "            self._kiwi = get_kiwi_instance(\n",
    "                model_type=self.model_type,\n",
    "                typos=self.typos,\n",
    "                model_path=self.model_path,\n",
    "                load_default_dict=self.load_default_dict,\n",
    "                load_typo_dict=self.load_typo_dict,\n",
    "                load_multi_dict=self.load_multi_dict,\n",
    "                integrate_allomorph=self.integrate_allomorph,\n",
    "                typo_cost_threshold=self.typo_cost_threshold,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "            # space_tolerance 설정\n",
    "            if self.space_tolerance > 0:\n",
    "                self._kiwi.space_tolerance = self.space_tolerance\n",
    "\n",
    "        return self._kiwi\n",
    "\n",
    "    def add_user_word(\n",
    "        self,\n",
    "        word: str,\n",
    "        tag: str = \"NNP\",\n",
    "        score: float = 0.0,\n",
    "        orig_word: str | None = None,\n",
    "    ) -> bool:\n",
    "        \"\"\"사용자 사전에 단어를 추가한다.\n",
    "\n",
    "        Args:\n",
    "            word: 추가할 단어\n",
    "            tag: 품사 태그\n",
    "            score: 점수\n",
    "            orig_word: 원본 단어 (이형태인 경우)\n",
    "\n",
    "        Returns:\n",
    "            추가 성공 여부\n",
    "\n",
    "        Examples:\n",
    "            >>> retriever.add_user_word(\"김갑갑\", \"NNP\")\n",
    "            True\n",
    "            >>> retriever.add_user_word(\"팅기\", \"VV\")  # 동사 활용형 자동 등재\n",
    "            True\n",
    "        \"\"\"\n",
    "        kiwi = self._get_kiwi()\n",
    "        return kiwi.add_user_word(word, tag, score, orig_word)\n",
    "\n",
    "    def add_pre_analyzed_word(\n",
    "        self,\n",
    "        form: str,\n",
    "        analyzed: list[str | tuple],\n",
    "        score: float = 0.0,\n",
    "    ) -> bool:\n",
    "        \"\"\"기분석 형태를 추가한다.\n",
    "\n",
    "        Args:\n",
    "            form: 기분석 형태\n",
    "            analyzed: 형태소 분석 결과\n",
    "            score: 점수\n",
    "\n",
    "        Returns:\n",
    "            추가 성공 여부\n",
    "\n",
    "        Examples:\n",
    "            >>> retriever.add_pre_analyzed_word(\n",
    "            ...     \"사겼다\", [(\"사귀\", \"VV\", 0, 2), (\"었\", \"EP\", 1, 2), (\"다\", \"EF\", 2, 3)], -3.0\n",
    "            ... )\n",
    "            True\n",
    "        \"\"\"\n",
    "        kiwi = self._get_kiwi()\n",
    "        return kiwi.add_pre_analyzed_word(form, analyzed, score)\n",
    "\n",
    "    def add_re_rule(\n",
    "        self,\n",
    "        tag: str,\n",
    "        pattern: str,\n",
    "        repl: str,\n",
    "        score: float = -3.0,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"정규표현식 규칙으로 이형태를 일괄 추가한다.\n",
    "\n",
    "        Args:\n",
    "            tag: 품사 태그\n",
    "            pattern: 정규표현식 패턴\n",
    "            repl: 치환 문자열\n",
    "            score: 점수\n",
    "\n",
    "        Returns:\n",
    "            추가된 형태소 리스트\n",
    "\n",
    "        Examples:\n",
    "            >>> # '요' -> '영' 종결어미 변형 일괄 추가\n",
    "            >>> retriever.add_re_rule(\"EF\", r\"요$\", r\"영\", -3.0)\n",
    "            ['어영', '에영', '지영', ...]\n",
    "        \"\"\"\n",
    "        kiwi = self._get_kiwi()\n",
    "        return kiwi.add_re_rule(tag, pattern, repl, score)\n",
    "\n",
    "    def load_user_dictionary(self, user_dict_path: str) -> int:\n",
    "        \"\"\"사용자 사전 파일을 로드한다.\n",
    "\n",
    "        Args:\n",
    "            user_dict_path: 사전 파일 경로\n",
    "\n",
    "        Returns:\n",
    "            추가된 형태소 개수\n",
    "        \"\"\"\n",
    "        kiwi = self._get_kiwi()\n",
    "        return kiwi.load_user_dictionary(user_dict_path)\n",
    "\n",
    "    def _enrich_with_ngrams(self, texts: list[str]) -> None:\n",
    "        \"\"\"NgramExtractor를 사용하여 다어절 단어를 추출하고 사전에 추가한다.\n",
    "\n",
    "        Args:\n",
    "            texts: 분석할 텍스트 리스트\n",
    "        \"\"\"\n",
    "        if not self.enable_ngram_enrichment:\n",
    "            return\n",
    "\n",
    "        kiwi = self._get_kiwi()\n",
    "        extractor = NgramExtractor(kiwi, gather_lm_score=True)\n",
    "\n",
    "        # 텍스트 추가\n",
    "        extractor.add(texts)\n",
    "\n",
    "        # Ngram 추출\n",
    "        candidates = extractor.extract(\n",
    "            max_candidates=-1,\n",
    "            min_cnt=self.ngram_min_cnt,\n",
    "            max_length=self.ngram_max_length,\n",
    "            min_score=self.ngram_min_score,\n",
    "            num_workers=self.num_workers if self.num_workers > 0 else 1,\n",
    "        )\n",
    "\n",
    "        # 자동 추가 옵션이 활성화된 경우 사용자 사전에 추가\n",
    "        if self.ngram_auto_add:\n",
    "            added_count = 0\n",
    "            for candidate in candidates:\n",
    "                # candidate.text는 문자열 형태의 Ngram\n",
    "                # candidate.tokens는 (형태, 품사) 튜플 리스트\n",
    "                if candidate.text and len(candidate.text.strip()) > 0:\n",
    "                    # 공백이 포함된 다어절 단어는 NNP(고유명사)로 추가\n",
    "                    # 또는 길이가 2 이상인 단어도 추가\n",
    "                    text = candidate.text.strip()\n",
    "                    if \" \" in text or len(text) > 2:\n",
    "                        if kiwi.add_user_word(text, \"NNP\", score=0.0):\n",
    "                            added_count += 1\n",
    "\n",
    "            if added_count > 0:\n",
    "                print(f\"NgramExtractor로 {added_count}개 다어절 단어 추가됨\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(\n",
    "        cls,\n",
    "        texts: list[str],\n",
    "        metadatas: list[dict] | None = None,\n",
    "        *,\n",
    "        auto_save: bool = False,\n",
    "        save_path: str | Path | None = None,\n",
    "        save_user_dict: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"KiwiBM25Retriever\":\n",
    "        \"\"\"텍스트 리스트로부터 Retriever 생성.\n",
    "\n",
    "        Args:\n",
    "            texts: 텍스트 리스트\n",
    "            metadatas: 메타데이터 리스트\n",
    "            auto_save: 생성 직후 인덱스를 자동 저장\n",
    "            save_path: 자동 저장 경로 (미지정 시 'models/kiwi_bm25')\n",
    "            save_user_dict: 사용자 사전 템플릿도 함께 저장\n",
    "            **kwargs: 추가 파라미터\n",
    "                - enable_ngram_enrichment: NgramExtractor 기반 다어절 보강 활성화\n",
    "                - ngram_min_cnt: Ngram 최소 출현 빈도 (기본값: 10)\n",
    "                - ngram_max_length: Ngram 최대 길이 (기본값: 5)\n",
    "                - ngram_min_score: Ngram 최소 점수 (기본값: 1e-3)\n",
    "                - ngram_auto_add: 추출된 Ngram 자동 추가 여부 (기본값: True)\n",
    "\n",
    "        Returns:\n",
    "            KiwiBM25Retriever 인스턴스\n",
    "\n",
    "        Examples:\n",
    "            >>> # 기본 사용\n",
    "            >>> retriever = KiwiBM25Retriever.from_texts(texts)\n",
    "            >>>\n",
    "            >>> # Ngram 보강 활성화\n",
    "            >>> retriever = KiwiBM25Retriever.from_texts(texts, enable_ngram_enrichment=True, ngram_min_cnt=5)\n",
    "        \"\"\"\n",
    "        # Retriever 인스턴스 생성 (Kiwi는 lazy initialization)\n",
    "        instance = cls(**kwargs)\n",
    "        kiwi = instance._get_kiwi()\n",
    "\n",
    "        # Ngram 보강 수행 (사전에 단어 추가)\n",
    "        instance._enrich_with_ngrams(texts)\n",
    "\n",
    "        # 커스텀 토크나이저 생성\n",
    "        def custom_tokenizer(text: str) -> list[str]:\n",
    "            return advanced_kiwi_tokenizer(\n",
    "                text,\n",
    "                kiwi=kiwi,\n",
    "                important_pos=instance.important_pos,\n",
    "                stopwords=instance.stopwords,\n",
    "                normalize_coda=instance.normalize_coda,\n",
    "                z_coda=instance.z_coda,\n",
    "                compatible_jamo=instance.compatible_jamo,\n",
    "                saisiot=instance.saisiot,\n",
    "                min_token_len=instance.min_token_len,\n",
    "                blocklist=instance.blocklist,\n",
    "            )\n",
    "\n",
    "        # 텍스트 토큰화\n",
    "        texts_processed = [custom_tokenizer(t) for t in texts]\n",
    "\n",
    "        # BM25 인덱스 생성\n",
    "        instance.vectorizer = BM25Okapi(\n",
    "            texts_processed,\n",
    "            k1=instance.bm25_k1,\n",
    "            b=instance.bm25_b,\n",
    "        )\n",
    "\n",
    "        # 문서 생성\n",
    "        metadatas = metadatas or [{} for _ in texts]\n",
    "        instance.docs = [\n",
    "            Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas, strict=True)\n",
    "        ]\n",
    "\n",
    "        # 자동 저장\n",
    "        if auto_save:\n",
    "            target_path = Path(save_path) if save_path is not None else Path(\"models/kiwi_bm25\")\n",
    "            instance.save(target_path, save_user_dict=save_user_dict)\n",
    "\n",
    "        return instance\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        documents: Any,\n",
    "        *,\n",
    "        auto_save: bool = False,\n",
    "        save_path: str | Path | None = None,\n",
    "        save_user_dict: bool = True,\n",
    "        load_user_dict: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"KiwiBM25Retriever\":\n",
    "        \"\"\"입력으로부터 Retriever 생성 (Document 리스트, 텍스트 리스트, 또는 저장 경로).\n",
    "\n",
    "        Args:\n",
    "            documents: 아래 중 하나\n",
    "                - list[Document]: 문서 리스트\n",
    "                - list[str]: 텍스트 리스트\n",
    "                - str | Path: 저장된 인덱스 디렉토리 경로 (로드)\n",
    "            auto_save: 인덱스 생성 후 자동 저장\n",
    "            save_path: 자동 저장 경로 (미지정 시 'models/kiwi_bm25')\n",
    "            save_user_dict: 자동 저장 시 사용자 사전 템플릿도 저장\n",
    "            load_user_dict: 로드시 사용자 사전 로드 여부\n",
    "            **kwargs: from_texts의 나머지 파라미터들과 동일\n",
    "\n",
    "        Returns:\n",
    "            KiwiBM25Retriever 인스턴스\n",
    "\n",
    "        Examples:\n",
    "            >>> # 1) Document 리스트로 생성\n",
    "            >>> retriever = KiwiBM25Retriever.from_documents(docs, auto_save=True, save_path=\"models/my_ret\")\n",
    "            >>>\n",
    "            >>> # 2) 텍스트 리스트로 생성\n",
    "            >>> retriever = KiwiBM25Retriever.from_documents([\"문장1\", \"문장2\"], auto_save=True)\n",
    "            >>>\n",
    "            >>> # 3) 저장된 인덱스 로드\n",
    "            >>> retriever = KiwiBM25Retriever.from_documents(\"models/my_ret\")\n",
    "        \"\"\"\n",
    "        # 3) 경로 입력 시 로드\n",
    "        if isinstance(documents, (str, Path)):\n",
    "            return cls.load(documents, load_user_dict=load_user_dict)\n",
    "\n",
    "        # 2) 텍스트 리스트 처리\n",
    "        if isinstance(documents, list) and (len(documents) == 0 or isinstance(documents[0], str)):\n",
    "            texts = documents  # type: ignore[assignment]\n",
    "            return cls.from_texts(\n",
    "                texts=texts,  # type: ignore[arg-type]\n",
    "                metadatas=None,\n",
    "                auto_save=auto_save,\n",
    "                save_path=save_path,\n",
    "                save_user_dict=save_user_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        # 1) Document 리스트 처리\n",
    "        if isinstance(documents, list) and documents and isinstance(documents[0], Document):\n",
    "            texts = [d.page_content for d in documents]\n",
    "            metadatas = [d.metadata for d in documents]\n",
    "            return cls.from_texts(\n",
    "                texts=texts,\n",
    "                metadatas=metadatas,\n",
    "                auto_save=auto_save,\n",
    "                save_path=save_path,\n",
    "                save_user_dict=save_user_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        raise TypeError(\n",
    "            \"documents must be list[Document], list[str], or a path (str|Path) to a saved index\"\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        run_manager: CallbackManagerForRetrieverRun | None = None,\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"관련 문서 검색 (LangChain 필수 메서드).\"\"\"\n",
    "        kiwi = self._get_kiwi()\n",
    "\n",
    "        # 쿼리 토큰화\n",
    "        processed_query = advanced_kiwi_tokenizer(\n",
    "            query,\n",
    "            kiwi=kiwi,\n",
    "            important_pos=self.important_pos,\n",
    "            stopwords=self.stopwords,\n",
    "            normalize_coda=self.normalize_coda,\n",
    "            z_coda=self.z_coda,\n",
    "            compatible_jamo=self.compatible_jamo,\n",
    "            saisiot=self.saisiot,\n",
    "            min_token_len=self.min_token_len,\n",
    "            blocklist=self.blocklist,\n",
    "        )\n",
    "\n",
    "        # BM25 검색\n",
    "        return_docs = self.vectorizer.get_top_n(processed_query, self.docs, n=self.k)\n",
    "        return return_docs\n",
    "\n",
    "    def search_with_score(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int | None = None,\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"점수와 함께 검색 (메타데이터에 score 추가).\n",
    "\n",
    "        Args:\n",
    "            query: 검색 쿼리\n",
    "            top_k: 반환할 문서 수 (None이면 self.k 사용)\n",
    "\n",
    "        Returns:\n",
    "            점수가 메타데이터에 포함된 문서 리스트\n",
    "        \"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.k\n",
    "\n",
    "        kiwi = self._get_kiwi()\n",
    "\n",
    "        # 쿼리 토큰화\n",
    "        processed_query = advanced_kiwi_tokenizer(\n",
    "            query,\n",
    "            kiwi=kiwi,\n",
    "            important_pos=self.important_pos,\n",
    "            stopwords=self.stopwords,\n",
    "            normalize_coda=self.normalize_coda,\n",
    "            z_coda=self.z_coda,\n",
    "            compatible_jamo=self.compatible_jamo,\n",
    "            saisiot=self.saisiot,\n",
    "            min_token_len=self.min_token_len,\n",
    "            blocklist=self.blocklist,\n",
    "        )\n",
    "\n",
    "        # BM25 점수 계산\n",
    "        scores = self.vectorizer.get_scores(processed_query)\n",
    "\n",
    "        # Softmax 정규화\n",
    "        normalized_scores = self._softmax(scores)\n",
    "\n",
    "        # 점수 내림차순 정렬\n",
    "        score_indices = self._argsort(normalized_scores, reverse=True)[:top_k]\n",
    "\n",
    "        # 문서에 점수 추가\n",
    "        docs_with_scores = []\n",
    "        for idx in score_indices:\n",
    "            doc = self.docs[idx]\n",
    "            metadata = doc.metadata.copy()\n",
    "            metadata[\"score\"] = float(normalized_scores[idx])\n",
    "            docs_with_scores.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "        return docs_with_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax 정규화.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def _argsort(seq: list, reverse: bool = False) -> list:\n",
    "        \"\"\"시퀀스 정렬 인덱스 반환.\"\"\"\n",
    "        return sorted(range(len(seq)), key=seq.__getitem__, reverse=reverse)\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        path: str | Path,\n",
    "        save_user_dict: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Retriever 인덱스를 저장한다.\n",
    "\n",
    "        BM25 인덱스와 문서, 토큰화 결과를 저장하여\n",
    "        다음에 빠르게 로드할 수 있습니다.\n",
    "\n",
    "        Args:\n",
    "            path: 저장할 디렉토리 경로\n",
    "            save_user_dict: 사용자 사전도 함께 저장 (True 권장)\n",
    "\n",
    "        Examples:\n",
    "            >>> retriever.save(\"models/my_retriever\")\n",
    "            >>> # 다음 실행 시 빠르게 로드\n",
    "            >>> retriever = KiwiBM25Retriever.load(\"models/my_retriever\")\n",
    "\n",
    "        Note:\n",
    "            Kiwi 모델 자체는 C++ 바인딩이라 저장되지 않습니다.\n",
    "            사용자 사전은 텍스트 파일로 별도 저장됩니다.\n",
    "        \"\"\"\n",
    "        save_path = Path(path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 1. BM25 인덱스 + 문서 저장\n",
    "        index_file = save_path / \"bm25_index.pkl\"\n",
    "        with open(index_file, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"docs\": self.docs,\n",
    "                    \"vectorizer\": self.vectorizer,\n",
    "                    # 설정 저장\n",
    "                    \"k\": self.k,\n",
    "                    \"model_type\": self.model_type,\n",
    "                    \"typos\": self.typos,\n",
    "                    \"load_default_dict\": self.load_default_dict,\n",
    "                    \"load_typo_dict\": self.load_typo_dict,\n",
    "                    \"load_multi_dict\": self.load_multi_dict,\n",
    "                    \"integrate_allomorph\": self.integrate_allomorph,\n",
    "                    \"typo_cost_threshold\": self.typo_cost_threshold,\n",
    "                    \"important_pos\": self.important_pos,\n",
    "                    \"normalize_coda\": self.normalize_coda,\n",
    "                    \"z_coda\": self.z_coda,\n",
    "                    \"compatible_jamo\": self.compatible_jamo,\n",
    "                    \"saisiot\": self.saisiot,\n",
    "                    \"min_token_len\": self.min_token_len,\n",
    "                    \"space_tolerance\": self.space_tolerance,\n",
    "                    \"enable_ngram_enrichment\": self.enable_ngram_enrichment,\n",
    "                    \"ngram_min_cnt\": self.ngram_min_cnt,\n",
    "                    \"ngram_max_length\": self.ngram_max_length,\n",
    "                    \"ngram_min_score\": self.ngram_min_score,\n",
    "                    \"ngram_auto_add\": self.ngram_auto_add,\n",
    "                    \"bm25_k1\": self.bm25_k1,\n",
    "                    \"bm25_b\": self.bm25_b,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "\n",
    "        # 2. 사용자 사전 저장 (선택)\n",
    "        if save_user_dict:\n",
    "            # Kiwi는 사용자 사전 직접 저장 기능이 없으므로\n",
    "            # 메타데이터로 저장 안내만 출력\n",
    "            dict_file = save_path / \"user_dict.txt\"\n",
    "            if not dict_file.exists():\n",
    "                with open(dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"# 사용자 사전 파일\\n\")\n",
    "                    f.write(\"# 형태\\\\t품사\\\\t점수 형식으로 추가하세요\\n\")\n",
    "                    f.write(\"# 예: 김갑갑\\\\tNNP\\\\t0.0\\n\")\n",
    "\n",
    "        print(f\"Retriever 저장 완료: {save_path}\")\n",
    "        print(f\"  - BM25 인덱스: {index_file}\")\n",
    "        if save_user_dict:\n",
    "            print(f\"  - 사용자 사전: {dict_file} (수동 편집 필요)\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        path: str | Path,\n",
    "        load_user_dict: bool = True,\n",
    "    ) -> \"KiwiBM25Retriever\":\n",
    "        \"\"\"저장된 Retriever 인덱스를 로드한다.\n",
    "\n",
    "        Args:\n",
    "            path: 로드할 디렉토리 경로\n",
    "            load_user_dict: 사용자 사전도 로드\n",
    "\n",
    "        Returns:\n",
    "            로드된 KiwiBM25Retriever 인스턴스\n",
    "\n",
    "        Examples:\n",
    "            >>> retriever = KiwiBM25Retriever.load(\"models/my_retriever\")\n",
    "            >>> results = retriever.invoke(\"검색 쿼리\")\n",
    "\n",
    "        Note:\n",
    "            저장된 인덱스를 로드하면 문서 재분석 없이 바로 검색 가능합니다.\n",
    "            사용자 사전은 user_dict.txt를 수동으로 편집 후 로드됩니다.\n",
    "        \"\"\"\n",
    "        load_path = Path(path)\n",
    "        index_file = load_path / \"bm25_index.pkl\"\n",
    "\n",
    "        if not index_file.exists():\n",
    "            raise FileNotFoundError(f\"인덱스 파일을 찾을 수 없습니다: {index_file}\")\n",
    "\n",
    "        # 1. BM25 인덱스 로드\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # 2. Retriever 인스턴스 생성\n",
    "        instance = cls(\n",
    "            k=data.get(\"k\", 4),\n",
    "            model_type=data.get(\"model_type\", \"knlm\"),\n",
    "            typos=data.get(\"typos\"),\n",
    "            load_default_dict=data.get(\"load_default_dict\", True),\n",
    "            load_typo_dict=data.get(\"load_typo_dict\", True),\n",
    "            load_multi_dict=data.get(\"load_multi_dict\", True),\n",
    "            integrate_allomorph=data.get(\"integrate_allomorph\", True),\n",
    "            typo_cost_threshold=data.get(\"typo_cost_threshold\", 2.5),\n",
    "            important_pos=data.get(\"important_pos\"),\n",
    "            normalize_coda=data.get(\"normalize_coda\", False),\n",
    "            z_coda=data.get(\"z_coda\", True),\n",
    "            compatible_jamo=data.get(\"compatible_jamo\", False),\n",
    "            saisiot=data.get(\"saisiot\"),\n",
    "            min_token_len=data.get(\"min_token_len\", 1),\n",
    "            space_tolerance=data.get(\"space_tolerance\", 0),\n",
    "            enable_ngram_enrichment=data.get(\"enable_ngram_enrichment\", False),\n",
    "            ngram_min_cnt=data.get(\"ngram_min_cnt\", 10),\n",
    "            ngram_max_length=data.get(\"ngram_max_length\", 5),\n",
    "            ngram_min_score=data.get(\"ngram_min_score\", 1e-3),\n",
    "            ngram_auto_add=data.get(\"ngram_auto_add\", True),\n",
    "            bm25_k1=data.get(\"bm25_k1\", 1.5),\n",
    "            bm25_b=data.get(\"bm25_b\", 0.75),\n",
    "        )\n",
    "\n",
    "        # 3. 데이터 복원\n",
    "        instance.docs = data[\"docs\"]\n",
    "        instance.vectorizer = data[\"vectorizer\"]\n",
    "\n",
    "        # 4. 사용자 사전 로드 (선택)\n",
    "        if load_user_dict:\n",
    "            dict_file = load_path / \"user_dict.txt\"\n",
    "            if dict_file.exists():\n",
    "                try:\n",
    "                    kiwi = instance._get_kiwi()\n",
    "                    num_added = kiwi.load_user_dictionary(str(dict_file))\n",
    "                    print(f\"사용자 사전 로드: {num_added}개 단어\")\n",
    "                except Exception as e:\n",
    "                    print(f\"사용자 사전 로드 실패: {e}\")\n",
    "\n",
    "        print(f\"Retriever 로드 완료: {load_path}\")\n",
    "        print(f\" - 문서 수: {len(instance.docs)}\")\n",
    "        print(f\" - 모델: {instance.model_type}\")\n",
    "\n",
    "        return instance\n",
    "\n",
    "\n",
    "# 편의 함수\n",
    "def create_kiwi_bm25_retriever(\n",
    "    documents: list[Document],\n",
    "    model_type: Literal[\"knlm\", \"sbg\", \"cong\"] = \"knlm\",\n",
    "    **kwargs: Any,\n",
    ") -> KiwiBM25Retriever:\n",
    "    \"\"\"KiwiBM25Retriever 생성 헬퍼 함수.\n",
    "\n",
    "    Examples:\n",
    "        >>> retriever = create_kiwi_bm25_retriever(docs, model_type=\"sbg\")\n",
    "    \"\"\"\n",
    "    return KiwiBM25Retriever.from_documents(\n",
    "        documents,\n",
    "        model_type=model_type,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ed1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "사용자 사전 추가 완료: 15개 용어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<kiwipiepy.utils.Stopwords at 0x115d37250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 사용자 사전 구축 (도메인 특화)\n",
    "# ============================================================================\n",
    "kiwi_basic = KiwiBM25Retriever()\n",
    "\n",
    "# RAG/LLM 도메인 용어 추가\n",
    "domain_terms = [\n",
    "    # 기술 용어\n",
    "    (\"RAG\", \"NNP\", 1.0),  # 고유명사, 가중치 1.0\n",
    "    (\"LLM\", \"NNP\", 1.0),\n",
    "    (\"임베딩\", \"NNG\", 1.0),  # 일반명사\n",
    "    (\"벡터DB\", \"NNG\", 0.8),\n",
    "    (\"벡터데이터베이스\", \"NNG\", 0.8),\n",
    "    (\"Qdrant\", \"NNP\", 1.0),\n",
    "    (\"OpenAI\", \"NNP\", 1.0),\n",
    "    (\"GPT\", \"NNP\", 1.0),\n",
    "    (\"Claude\", \"NNP\", 1.0),\n",
    "    # 금융 용어\n",
    "    (\"주식회사\", \"NNG\", 0.5),\n",
    "    (\"유한회사\", \"NNG\", 0.5),\n",
    "    (\"분기보고서\", \"NNG\", 0.8),\n",
    "    (\"재무제표\", \"NNG\", 0.8),\n",
    "    # 법률 용어\n",
    "    (\"근로기준법\", \"NNG\", 0.8),\n",
    "    (\"민사소송법\", \"NNG\", 0.8),\n",
    "]\n",
    "\n",
    "for term, pos, weight in domain_terms:\n",
    "    kiwi_basic.add_user_word(term, pos, weight)\n",
    "\n",
    "print(f\"\\n사용자 사전 추가 완료: {len(domain_terms)}개 용어\")\n",
    "\n",
    "# Stopwords 초기화\n",
    "stopwords_advanced = Stopwords()\n",
    "\n",
    "# 한국어 불용어 (기본 + 도메인 특화)\n",
    "korean_stopwords = [\n",
    "    # 기본 불용어\n",
    "    \"이\",\n",
    "    \"그\",\n",
    "    \"저\",\n",
    "    \"것\",\n",
    "    \"수\",\n",
    "    \"등\",\n",
    "    \"들\",\n",
    "    \"및\",\n",
    "    \"한\",\n",
    "    \"하다\",\n",
    "    \"되다\",\n",
    "    \"있다\",\n",
    "    \"없다\",\n",
    "    \"같다\",\n",
    "    \"대하다\",\n",
    "    \"위하다\",\n",
    "    \"통하다\",\n",
    "    \"의\",\n",
    "    \"가\",\n",
    "    \"이\",\n",
    "    \"을\",\n",
    "    \"를\",\n",
    "    \"에\",\n",
    "    \"에서\",\n",
    "    \"로\",\n",
    "    \"으로\",\n",
    "    \"와\",\n",
    "    \"과\",\n",
    "    # 도메인 특화 (검색에 불필요한 단어)\n",
    "    \"정말\",\n",
    "    \"진짜\",\n",
    "    \"아주\",\n",
    "    \"매우\",\n",
    "    \"너무\",\n",
    "    \"굉장히\",\n",
    "    \"그냥\",\n",
    "    \"막\",\n",
    "    \"뭔가\",\n",
    "    \"약간\",\n",
    "    \"좀\",\n",
    "    \"거의\",\n",
    "    \"항상\",\n",
    "    \"보통\",\n",
    "    \"대부분\",\n",
    "    \"전체\",\n",
    "]\n",
    "\n",
    "for word in korean_stopwords:\n",
    "    stopwords_advanced.add(word)\n",
    "\n",
    "kiwi_basic.stopwords = stopwords_advanced\n",
    "kiwi_basic.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98bfd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SPLADE-ko 한국어 기반 Sparse Embedding\n",
    "\n",
    "https://huggingface.co/yjoonjang/splade-ko-v1.0\n",
    "\n",
    "**SPLADE-ko-v1.0**은 한국어에 특화된 학습된 Sparse 임베딩입니다:\n",
    "- 기반 모델: `skt/A.X-Encoder-base` (ModernBERT)\n",
    "- 차원: 50,000 (vs BM25의 vocabulary size)\n",
    "- 자동 Query Expansion: 의미적으로 관련된 토큰 자동 추가\n",
    "\n",
    "SPLADE의 핵심:\n",
    "- BM25처럼 Sparse하지만 (99%+ 값이 0)\n",
    "- BERT처럼 의미를 이해하여 관련 토큰 자동 확장\n",
    "- Inverted Index로 빠른 검색 가능\n",
    "\n",
    "아쉽게도, Sentence-Transformer 를 이용하기 때문에 쉽게 활용해보기는 아직까지는 어렵습니다. (또한 모델도 계속 발전이 계속 되고 있어서 ko 버전도 부족함이 많아 실전에는 쓰기 어렵지만 잘 알아두시면 금방 Sparse Embedding 을 쓸 날이 올겁니다.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f956ff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLADE-ko 한국어 Sparse 임베딩\n",
      "================================================================================\n",
      "\n",
      "쿼리: 'RAG 시스템을 구축하는 방법'\n",
      "\n",
      "SPLADE 모델 로드 실패: name 'splade_model' is not defined\n",
      "\n",
      "해결 방법:\n",
      "  1. 네트워크 연결 확인 (HuggingFace 다운로드)\n",
      "  2. 모델 크기 확인 (~400MB)\n",
      "  3. 메모리 확인 (최소 2GB 필요)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SPLADE-ko 한국어 Sparse 임베딩\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPLADE-ko 한국어 Sparse 임베딩\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # NOTE: Sentence Transformers\n",
    "    from sentence_transformers import SparseEncoder\n",
    "\n",
    "    # Download from the 🤗 Hub\n",
    "    splade_model = SparseEncoder(\"yjoonjang/splade-ko-v1.0\")\n",
    "\n",
    "    # 테스트 쿼리\n",
    "    test_query_splade = \"RAG 시스템을 구축하는 방법\"\n",
    "    test_doc = \"벡터 데이터베이스와 임베딩을 활용한 검색 증강 생성 시스템 개발\"\n",
    "\n",
    "    # Sparse 임베딩 생성\n",
    "    print(f\"\\n쿼리: '{test_query_splade}'\")\n",
    "    query_embedding_splade = splade_model.encode(test_query_splade)\n",
    "    doc_embedding_splade = splade_model.encode(test_doc)\n",
    "\n",
    "    print(\"\\nSPLADE 임베딩 정보:\")\n",
    "    print(f\"  차원: {len(query_embedding_splade)}\")\n",
    "    print(f\"  Non-zero 값 수 (쿼리): {np.count_nonzero(query_embedding_splade)}\")\n",
    "    print(f\"  Non-zero 값 수 (문서): {np.count_nonzero(doc_embedding_splade)}\")\n",
    "    print(\n",
    "        f\"  희소성 (Sparsity): {(1 - np.count_nonzero(query_embedding_splade) / len(query_embedding_splade)) * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # 유사도 계산 (Dot Product for Sparse)\n",
    "    similarity = np.dot(query_embedding_splade, doc_embedding_splade)\n",
    "    print(f\"\\n유사도 점수: {similarity:.4f}\")\n",
    "\n",
    "    # 상위 활성화 토큰 분석 (Query Expansion 확인)\n",
    "    print(\"\\nQuery Expansion 분석:\")\n",
    "    print(\"  상위 활성화 인덱스 (자동 확장된 토큰):\")\n",
    "    top_indices = np.argsort(query_embedding_splade)[-10:][::-1]\n",
    "    top_values = query_embedding_splade[top_indices]\n",
    "    for idx, val in zip(top_indices, top_values, strict=False):\n",
    "        if val > 0:\n",
    "            print(f\"    인덱스 {idx}: 가중치 {val:.4f}\")\n",
    "\n",
    "    # 샘플 청크로 SPLADE 검색 테스트\n",
    "    if chunks_data:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(\"SPLADE 검색 테스트 (샘플 청크 10개)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 샘플 청크 임베딩\n",
    "        sample_docs = [chunk[\"text\"] for chunk in chunks_data[:10]]\n",
    "        print(\"\\nSPLADE 임베딩 생성 중... (10개 청크)\")\n",
    "        sample_splade_embeddings = splade_model.encode(sample_docs, show_progress_bar=True)\n",
    "\n",
    "        # 쿼리 임베딩\n",
    "        query_splade_emb = splade_model.encode(test_query_splade)\n",
    "\n",
    "        # 유사도 계산 (Dot Product)\n",
    "        similarities = np.dot(sample_splade_embeddings, query_splade_emb)\n",
    "\n",
    "        # 상위 3개 결과\n",
    "        top_k = 3\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        print(f\"\\nSPLADE 검색 결과 (상위 {top_k}개):\")\n",
    "        for rank, idx in enumerate(top_indices, start=1):\n",
    "            print(f\"  {rank}. 점수: {similarities[idx]:.4f}\")\n",
    "            print(f\"     텍스트: {sample_docs[idx][:100]}...\")\n",
    "\n",
    "    SPLADE_AVAILABLE = True\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"   에러: {e}\")\n",
    "    SPLADE_AVAILABLE = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nSPLADE 모델 로드 실패: {e}\")\n",
    "    print(\"\\n해결 방법:\")\n",
    "    print(\"  1. 네트워크 연결 확인 (HuggingFace 다운로드)\")\n",
    "    print(\"  2. 모델 크기 확인 (~400MB)\")\n",
    "    print(\"  3. 메모리 확인 (최소 2GB 필요)\")\n",
    "    SPLADE_AVAILABLE = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
