{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47029267",
   "metadata": {},
   "source": [
    "# Chunking 전략 비교\n",
    "\n",
    "**Chunking Strategy: Chonkie Library 를 기반**\n",
    "   - https://docs.chonkie.ai/oss/chunkers/overview, https://docs.chonkie.ai/oss/chunkers/slumber-chunker, https://github.com/chonkie-inc/chonkie  \n",
    "   - SlumberChunker (LLM 기반 Contextual Chunking - 의미를 채워서 Context 구성) 등\n",
    "\n",
    "[Docling 에서도 HybridChunker 를 지원합니다](https://docling-project.github.io/docling/examples/hybrid_chunking/)\n",
    "\n",
    "## 사전 준비\n",
    "\n",
    "1. `pip install 'chunkie[all]'`\n",
    "2. 실습용 PDF 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d83eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# OpenAI / OpenRouter 모델 초기화 헬퍼\n",
    "# ----------------------------------------------------------------------------\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _resolve_api_context() -> tuple[str, str]:\n",
    "    \"\"\"선택된 API 키와 베이스 URL 정보를 반환합니다.\"\"\"\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY가 필요합니다.\")\n",
    "\n",
    "    base_url = os.getenv(\"OPENROUTER_API_BASE\") or \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "    return (api_key, base_url)\n",
    "\n",
    "\n",
    "def create_openrouter_llm(\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.3,\n",
    "    max_tokens: int | None = None,\n",
    "    **kwargs: object,\n",
    ") -> ChatOpenAI:\n",
    "    \"\"\"OpenAI 호환 LLM 생성 헬퍼.\n",
    "\n",
    "    Args:\n",
    "        model: 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/gpt-4o, anthropic/claude-3-sonnet, google/gemini-pro)\n",
    "        temperature: 생성 온도 (0.0-2.0)\n",
    "        max_tokens: 최대 생성 토큰 수\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: 설정된 LLM 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    openai_kwargs: dict = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_retries\": 3,\n",
    "        \"timeout\": 60,\n",
    "        **kwargs,\n",
    "    }\n",
    "    if max_tokens is not None:\n",
    "        openai_kwargs[\"max_tokens\"] = max_tokens\n",
    "    if base_url:\n",
    "        openai_kwargs[\"base_url\"] = base_url\n",
    "    return ChatOpenAI(**openai_kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model(\n",
    "    model: str = \"openai/text-embedding-3-small\",\n",
    "    **kwargs,\n",
    ") -> OpenAIEmbeddings:\n",
    "    \"\"\"OpenAI 호환 임베딩 모델 생성.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/text-embedding-3-small, openai/text-embedding-3-large)\n",
    "        **kwargs: 추가 파라미터 (encoding_format 등은 model_kwargs로 전달됨)\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: 설정된 임베딩 모델 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    # 전달받은 kwargs에서 model_kwargs로 전달할 파라미터 분리\n",
    "    # encoding_format, extra_headers 등은 model_kwargs로 전달\n",
    "    model_kwargs: dict = {}\n",
    "    embedding_kwargs: dict = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"show_progress_bar\": True,\n",
    "        \"skip_empty\": True,\n",
    "    }\n",
    "\n",
    "    # 전달받은 kwargs 처리\n",
    "    for key, value in kwargs.items():\n",
    "        # OpenRouter API 특정 파라미터는 model_kwargs로 전달\n",
    "        if key in (\"encoding_format\"):\n",
    "            model_kwargs[key] = value\n",
    "        else:\n",
    "            # 나머지는 OpenAIEmbeddings에 직접 전달\n",
    "            embedding_kwargs[key] = value\n",
    "\n",
    "    if base_url:\n",
    "        embedding_kwargs[\"base_url\"] = base_url\n",
    "\n",
    "    # model_kwargs가 있으면 전달\n",
    "    if model_kwargs:\n",
    "        embedding_kwargs[\"model_kwargs\"] = model_kwargs\n",
    "\n",
    "    return OpenAIEmbeddings(**embedding_kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model_direct(\n",
    "    model: str = \"qwen/qwen3-embedding-0.6b\",\n",
    "    encoding_format: Literal[\"float\", \"base64\"] = \"float\",\n",
    "    input_text: str | list[str] = \"\",\n",
    "    **kwargs,\n",
    ") -> list[float] | list[list[float]]:\n",
    "    \"\"\"OpenAI SDK를 직접 사용하여 임베딩 생성 (encoding_format 지원).\n",
    "\n",
    "    LangChain의 OpenAIEmbeddings가 encoding_format을 지원하지 않을 때 사용.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름\n",
    "        encoding_format: 인코딩 형식 (\"float\")\n",
    "        input_text: 임베딩할 텍스트 (문자열 또는 문자열 리스트)\n",
    "        **kwargs: 추가 파라미터\n",
    "\n",
    "    Returns:\n",
    "        임베딩 벡터 리스트 (단일 텍스트) 또는 리스트의 리스트 (여러 텍스트)\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    # input_text가 비어있으면 kwargs에서 가져오기\n",
    "    if not input_text:\n",
    "        input_text = kwargs.get(\"input\", \"\")\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text,\n",
    "        encoding_format=encoding_format,\n",
    "    )\n",
    "\n",
    "    # 단일 텍스트인 경우 첫 번째 임베딩 반환\n",
    "    if isinstance(input_text, str):\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        # 여러 텍스트인 경우 모든 임베딩 반환\n",
    "        return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def get_available_model_types() -> dict[str, list[str]]:\n",
    "    \"\"\"OpenRouter에서 사용 가능한 모델 유형을 반환합니다.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: 모델 유형별 모델 목록\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"chat\": [\n",
    "            \"openai/gpt-4.1\",\n",
    "            \"openai/gpt-4.1-mini\",\n",
    "            \"openai/gpt-5\",\n",
    "            \"openai/gpt-5-mini\",\n",
    "            \"anthropic/claude-sonnet-4.5\",\n",
    "            \"anthropic/claude-haiku-4.5\",\n",
    "            \"google/gemini-2.5-flash-preview-09-2025\",\n",
    "            \"google/gemini-pro-2.5\",\n",
    "            \"x-ai/grok-4-fast\",\n",
    "            \"moonshotai/kimi-k2-thinking\",\n",
    "            \"liquid/lfm-2.2-6b\",\n",
    "            \"z-ai/glm-4.6\",\n",
    "        ],\n",
    "        \"embedding\": [\n",
    "            \"openai/text-embedding-3-small\",\n",
    "            \"openai/text-embedding-3-large\",\n",
    "            \"google/gemini-embedding-001\",\n",
    "            \"qwen/qwen3-embedding-0.6b\",\n",
    "            \"qwen/qwen3-embedding-4b\",\n",
    "            \"qwen/qwen3-embedding-8b\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "embeddings = create_embedding_model()\n",
    "llm = create_openrouter_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b9c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "읽어온 파일 길이 확인: 38723\n",
      "\n",
      "[2/4] SemanticChunker (의미 유사도 기반)\n",
      "오류: embedding_model must be a string or a BaseEmbeddings object\n",
      "\n",
      "[3/4] LangChain RecursiveCharacterTextSplitter (계층적)\n",
      " 생성된 청크 수: 107\n",
      "   첫 청크 미리보기 (처음 200자):\n",
      "--------------------------------------------------------------------------------\n",
      "근로기준법\n",
      "\n",
      "근로기준법\n",
      "\n",
      "[시행 2025. 2. 23.] [법률 제20520호, 2024. 10. 22., 일부개정]\n",
      "\n",
      "고용노동부 (근로기준정책과 - 해고, 취업규칙, 기타) 044-202-7534\n",
      "고용노동부 (근로기준정책과 - 소년) 044-202-7535\n",
      "고용노동부 (근로기준정책과 - 임금) 044-202-7548\n",
      "고용노동부 (여성고용정책과 - 여성) \n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4/4] LangChain MarkdownTextSplitter (마크다운 구조 인식)\n",
      " 생성된 청크 수: 97\n",
      "   첫 청크 미리보기 (처음 200자):\n",
      "--------------------------------------------------------------------------------\n",
      "근로기준법\n",
      "\n",
      "근로기준법\n",
      "\n",
      "[시행 2025. 2. 23.] [법률 제20520호, 2024. 10. 22., 일부개정]\n",
      "\n",
      "고용노동부 (근로기준정책과 - 해고, 취업규칙, 기타) 044-202-7534\n",
      "고용노동부 (근로기준정책과 - 소년) 044-202-7535\n",
      "고용노동부 (근로기준정책과 - 임금) 044-202-7548\n",
      "고용노동부 (여성고용정책과 - 여성) \n",
      "...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chunking 전략 비교\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: Parsing 완료된 문서를 Load 해와야함\n",
    "from pathlib import Path\n",
    "\n",
    "chunks_dir = Path(\"./chunks\")\n",
    "suffix = \"_docling\"\n",
    "\n",
    "# NOTE: 파일 이름에 따라 파일 읽기\n",
    "for md_path in chunks_dir.glob(f\"*{suffix}.md\"):\n",
    "    with open(md_path) as f:\n",
    "        markdown_text = f.read()\n",
    "\n",
    "print(f\"읽어온 파일 길이 확인: {len(markdown_text)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Chonkie SemanticChunker\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[2/4] SemanticChunker (의미 유사도 기반)\")\n",
    "try:\n",
    "    from chonkie import SemanticChunker\n",
    "\n",
    "    embeddings = create_embedding_model(model=\"openai/text-embedding-3-small\")\n",
    "    chunker = SemanticChunker(\n",
    "        embedding_model=embeddings,\n",
    "        chunk_size=512,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "\n",
    "    chunks = chunker.chunk(markdown_text)\n",
    "\n",
    "    print(f\" 생성된 청크 수: {len(chunks)}\")\n",
    "    print(\"   첫 청크 미리보기 (처음 200자):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(chunks[0].text[:200] if hasattr(chunks[0], \"text\") else str(chunks[0])[:200])\n",
    "    print(\"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Chonkie가 설치되지 않았습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. LangChain RecursiveCharacterTextSplitter\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[3/4] LangChain RecursiveCharacterTextSplitter (계층적)\")\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 우선순위대로 분할\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(markdown_text)\n",
    "\n",
    "    print(f\" 생성된 청크 수: {len(chunks)}\")\n",
    "    print(\"   첫 청크 미리보기 (처음 200자):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(chunks[0][:200])\n",
    "    print(\"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. LangChain MarkdownTextSplitter\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[4/4] LangChain MarkdownTextSplitter (마크다운 구조 인식)\")\n",
    "try:\n",
    "    from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "    splitter = MarkdownTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(markdown_text)\n",
    "\n",
    "    print(f\" 생성된 청크 수: {len(chunks)}\")\n",
    "    print(\"   첫 청크 미리보기 (처음 200자):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(chunks[0][:200])\n",
    "    print(\"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0640611",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Chonkie 라이브러리 소개\n",
    "\n",
    "Chonkie는 최신 RAG 파이프라인을 위한 포괄적인 청킹 라이브러리로, 다양한 구성요소를 제공합니다.\n",
    "\n",
    "### Chonkie 구성요소\n",
    "\n",
    "**1. Chefs (입력 정규화)**\n",
    "- 원시 입력 데이터를 정규화하고 전처리\n",
    "- TableChef, TextChef, MarkdownChef 등\n",
    "\n",
    "**2. Fetchers (파일/텍스트 수집)**\n",
    "- 다양한 소스에서 데이터 수집\n",
    "- FileFetcher 등\n",
    "\n",
    "**3. Chunkers (분할)**\n",
    "- 다양한 전략으로 텍스트 분할\n",
    "- TokenChunker, SentenceChunker, RecursiveChunker, SemanticChunker\n",
    "- LateChunker, NeuralChunker, SlumberChunker, TableChunker\n",
    "- **Experimental**: CodeChunker (코드 전용)\n",
    "\n",
    "**4. Embeddings (임베딩)**\n",
    "- 다양한 임베딩 모델 지원\n",
    "- OpenAIEmbeddings, CohereEmbeddings, GeminiEmbeddings, JinaEmbeddings, VoyageAI\n",
    "- Model2Vec, SentenceTransformer\n",
    "- **Custom: TEI(Hugging Face Text Embeddings Inference) 연동 가능**\n",
    "\n",
    "**5. Refinery (문맥 보강)**\n",
    "- 청크에 추가 문맥 정보 부착\n",
    "- OverlapRefinery: 오버랩으로 문맥 보강\n",
    "- EmbeddingsRefinery: 임베딩 벡터 부착\n",
    "\n",
    "**6. Handshakes (벡터DB 연동)**\n",
    "- 다양한 벡터 데이터베이스 통합\n",
    "- QdrantHandshake, ChromaHandshake, WeaviateHandshake\n",
    "- TurbopufferHandshake, PineconeHandshake, PgvectorHandshake\n",
    "- MongoDBHandshake, ElasticsearchHandshake\n",
    "\n",
    "**7. Porters (입출력)**\n",
    "- 청크 데이터 저장/로드\n",
    "- JSONPorter, DatasetsPorter\n",
    "\n",
    "**8. Utils (유틸리티)**\n",
    "- Visualizer: 청크 시각화\n",
    "- Hubbie: 헬퍼 함수\n",
    "\n",
    "### 참고 문서\n",
    "\n",
    "- Chunkers 개요: [https://docs.chonkie.ai/oss/chunkers/overview](https://docs.chonkie.ai/oss/chunkers/overview)\n",
    "- OpenAI 임베딩: [https://docs.chonkie.ai/oss/embeddings/openai-embeddings](https://docs.chonkie.ai/oss/embeddings/openai-embeddings)\n",
    "- 커스텀 임베딩(TEI): [https://docs.chonkie.ai/oss/embeddings/custom-embeddings](https://docs.chonkie.ai/oss/embeddings/custom-embeddings)\n",
    "- Refinery 개요: [https://docs.chonkie.ai/oss/refinery/overview](https://docs.chonkie.ai/oss/refinery/overview)\n",
    "- Qdrant Handshake: [https://docs.chonkie.ai/oss/handshakes/qdrant-handshake](https://docs.chonkie.ai/oss/handshakes/qdrant-handshake)\n",
    "- JSONPorter: [https://docs.chonkie.ai/oss/porters/json-porter](https://docs.chonkie.ai/oss/porters/json-porter)\n",
    "- Visualizer: [https://docs.chonkie.ai/oss/utils/visualizer](https://docs.chonkie.ai/oss/utils/visualizer)\n",
    "- Hubbie: [https://docs.chonkie.ai/oss/utils/hubbie](https://docs.chonkie.ai/oss/utils/hubbie)\n",
    "- Experimental Code Chunker: [https://docs.chonkie.ai/oss/experimental/code-chunker](https://docs.chonkie.ai/oss/experimental/code-chunker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9f314ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAIEmbeddings 초기화 완료\n",
      "   모델: text-embedding-3-small\n",
      "\n",
      "[1/3] 단일 텍스트 임베딩\n",
      "   텍스트: '청크 임베딩 테스트'\n",
      "   임베딩 차원: 1536\n",
      "   임베딩 벡터 (처음 5개): [-0.03322652 -0.02554174 -0.00478171 -0.04074111 -0.02861827]\n",
      "\n",
      "[2/3] 배치 임베딩\n",
      "   입력 텍스트 개수: 3\n",
      "   출력 벡터 개수: 3\n",
      "   각 벡터 차원: 1536\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chunkie - OpenAI Embeddings\n",
    "# ============================================================================\n",
    "import tiktoken\n",
    "from chonkie.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Chunkie - OpenAIEmbeddings 초기화\n",
    "chunkie_openai_embeddins = OpenAIEmbeddings(\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    model=\"openai/text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    tokenizer=tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "    dimension=1536,\n",
    "    max_tokens=8192,\n",
    ")\n",
    "print(\" OpenAIEmbeddings 초기화 완료\")\n",
    "print(\"   모델: text-embedding-3-small\")\n",
    "\n",
    "# 1. 단일 텍스트 임베딩\n",
    "print(\"\\n[1/3] 단일 텍스트 임베딩\")\n",
    "text = \"청크 임베딩 테스트\"\n",
    "vec = chunkie_openai_embeddins.embed(text)\n",
    "print(f\"   텍스트: '{text}'\")\n",
    "print(f\"   임베딩 차원: {len(vec)}\")\n",
    "print(f\"   임베딩 벡터 (처음 5개): {vec[:5]}\")\n",
    "\n",
    "# 2. 배치 임베딩\n",
    "print(\"\\n[2/3] 배치 임베딩\")\n",
    "texts = [\"첫 번째 문장입니다.\", \"두 번째 문장입니다.\", \"세 번째 문장입니다.\"]\n",
    "vecs = chunkie_openai_embeddins.embed_batch(texts)\n",
    "print(f\"   입력 텍스트 개수: {len(texts)}\")\n",
    "print(f\"   출력 벡터 개수: {len(vecs)}\")\n",
    "print(f\"   각 벡터 차원: {len(vecs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90581e",
   "metadata": {},
   "source": [
    "## 2. TEI(Hugging Face Text Embeddings Inference) 기반 Chunkie CustomEmbeddings\n",
    "\n",
    "TEI는 Hugging Face가 제공하는 고성능 임베딩 추론 서버입니다.  \n",
    "Chonkie의 CustomEmbeddings를 통해 TEI 서버와 연동할 수 있습니다.\n",
    "  \n",
    "**TEI 서버 시작 (GPU 가 있는 Docker):**\n",
    "```bash\n",
    "docker run --rm -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:1.8 \\\n",
    "  --model-id Qwen/Qwen3-Embedding-0.6B\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- GPU 가속 지원, On-Premise 배포 가능 (데이터 보안)\n",
    "- 다양한 오픈소스 임베딩 모델 지원, REST API 기반\n",
    "- [**Docker Image 버전에 따라서 지원하는 GPU 가 다르니 꼭 Github 에서 확인해주세요.**](https://github.com/huggingface/text-embeddings-inference?tab=readme-ov-file#docker-images)\n",
    "\n",
    "### Docker Images\n",
    "\n",
    "Text Embeddings Inference ships with multiple Docker images that you can use to target a specific backend:\n",
    "\n",
    "| Architecture                        | Image                                                                   |\n",
    "|-------------------------------------|-------------------------------------------------------------------------|\n",
    "| CPU                                 | ghcr.io/huggingface/text-embeddings-inference:cpu-1.8                   |\n",
    "| Volta                               | NOT SUPPORTED                                                           |\n",
    "| Turing (T4, RTX 2000 series, ...)   | ghcr.io/huggingface/text-embeddings-inference:turing-1.8 (experimental) |\n",
    "| Ampere 80 (A100, A30)               | ghcr.io/huggingface/text-embeddings-inference:1.8                       |\n",
    "| Ampere 86 (A10, A40, ...)           | ghcr.io/huggingface/text-embeddings-inference:86-1.8                    |\n",
    "| Ada Lovelace (RTX 4000 series, ...) | ghcr.io/huggingface/text-embeddings-inference:89-1.8                    |\n",
    "| Hopper (H100)                       | ghcr.io/huggingface/text-embeddings-inference:hopper-1.8 (experimental) |\n",
    "\n",
    "**Warning**: Flash Attention is turned off by default for the Turing image as it suffers from precision issues.\n",
    "You can turn Flash Attention v1 ON by using the `USE_FLASH_ATTENTION=True` environment variable.\n",
    "\n",
    "### API documentation\n",
    "\n",
    "You can consult the OpenAPI documentation of the `text-embeddings-inference` REST API using the `/docs` route.\n",
    "The Swagger UI is also available\n",
    "at: [https://huggingface.github.io/text-embeddings-inference](https://huggingface.github.io/text-embeddings-inference).\n",
    "\n",
    "\n",
    "### **참고 문서:**\n",
    "- [TEI GitHub](https://github.com/huggingface/text-embeddings-inference)\n",
    "- [TEI Docs](https://huggingface.github.io/text-embeddings-inference/)\n",
    "- [CustomEmbeddings 문서](https://docs.chonkie.ai/oss/embeddings/custom-embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8b9c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] TEI 서버 가용성 확인\n",
      "   TEI URL: https://dew21ae8bkg6ce-8000.proxy.runpod.net\n",
      "    TEI 서버 연결 성공!\n",
      "\n",
      "[2/4] TEIEmbeddings 초기화\n",
      "   TEIEmbeddings(base_url='https://dew21ae8bkg6ce-8000.proxy.runpod.net', model='Qwen/Qwen3-Embedding-4B', version='1.8.3')\n",
      "\n",
      "[3/4] 단일 임베딩 테스트\n",
      "   임베딩 차원: 2560\n",
      "   임베딩 벡터 (처음 5개): [-0.0002205   0.01461212  0.01620024  0.0285861  -0.00088334]\n",
      "\n",
      "[4/4] 배치 임베딩 테스트\n",
      "   입력 개수: 3\n",
      "   출력 개수: 3\n",
      "   각 벡터 차원: 2560\n",
      "\n",
      "[추가] Tokenizer 기능 테스트\n",
      "============================================================\n",
      "\n",
      "[1] get_tokenizer() 테스트\n",
      "   토크나이저 타입: <class 'chonkie.tokenizer.TEITokenizer'>\n",
      "   토크나이저 == self: False\n",
      "\n",
      "[2] encode() 테스트\n",
      "   입력 텍스트: 안녕하세요, Chonkie 테스트입니다.\n",
      "   토큰 ID 개수: 13\n",
      "   토큰 ID (처음 10개): [126246, 144370, 91145, 11, 910, 263, 30322, 10764, 72509, 53189]\n",
      "\n",
      "[3] decode() 테스트\n",
      "   원본 텍스트: 안녕하세요, Chonkie 테스트입니다.\n",
      "   디코딩 텍스트: 안녕하세요, Chonkie 테스트입니다.\n",
      "   일치 여부: True\n",
      "\n",
      "[4] Tokenizer Protocol 호환성\n",
      "   hasattr(tokenizer, 'encode'): True\n",
      "   hasattr(tokenizer, 'decode'): True\n",
      "   callable(tokenizer.encode): True\n",
      "   callable(tokenizer.decode): True\n",
      "\n",
      "Tokenizer 구현 완료!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEI CustomEmbeddings 구현\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "from chonkie.embeddings import BaseEmbeddings\n",
    "\n",
    "# TEI 서버 URL 설정\n",
    "RUNPOD_TEI_SERVER = \"https://dew21ae8bkg6ce-8000.proxy.runpod.net\"\n",
    "TEI_BASE_URL = os.getenv(\"TEI_BASE_URL\", RUNPOD_TEI_SERVER)\n",
    "\n",
    "\n",
    "from chonkie.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "class TEITokenizer(Tokenizer):\n",
    "    \"\"\"TEI 서버를 위한 Tokenizer wrapper\n",
    "\n",
    "    Chonkie의 Tokenizer 클래스를 상속받아 AutoTokenizer가 \"chonkie\" backend로 인식합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # AutoTokenizer가 \"chonkie\" backend로 인식하도록 __module__ 설정\n",
    "    __module__ = \"chonkie.tokenizer\"\n",
    "\n",
    "    def __init__(self, tei_embeddings):\n",
    "        \"\"\"TEIEmbeddings 인스턴스를 받아서 래핑합니다.\"\"\"\n",
    "        # Tokenizer의 __init__을 호출하지 않음 (vocab 불필요)\n",
    "        self._embeddings = tei_embeddings\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"문자열 표현\"\"\"\n",
    "        return f\"TEITokenizer(base_url={self._embeddings.base_url})\"\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"텍스트를 토큰 ID로 인코딩\"\"\"\n",
    "        return self._embeddings.encode(text)\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"토큰 ID를 텍스트로 디코딩\"\"\"\n",
    "        return self._embeddings.decode(tokens)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[int]:\n",
    "        \"\"\"텍스트를 토큰화 (encode와 동일)\"\"\"\n",
    "        return self._embeddings.encode(text)\n",
    "\n",
    "    def encode_batch(self, texts: list[str]) -> list[list[int]]:\n",
    "        \"\"\"배치 인코딩\"\"\"\n",
    "        return self._embeddings.encode_batch(texts)\n",
    "\n",
    "    def decode_batch(self, token_sequences: list[list[int]]) -> list[str]:\n",
    "        \"\"\"배치 디코딩\"\"\"\n",
    "        return self._embeddings.decode_batch(token_sequences)\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"토큰 수 카운트\"\"\"\n",
    "        return self._embeddings.count_tokens(text)\n",
    "\n",
    "    def count_tokens_batch(self, texts: list[str]) -> list[int]:\n",
    "        \"\"\"배치 토큰 카운트\"\"\"\n",
    "        return self._embeddings.count_tokens_batch(texts)\n",
    "\n",
    "\n",
    "class TEIEmbeddings(BaseEmbeddings):\n",
    "    \"\"\"TEI(Text Embeddings Inference) 서버를 위한 Chonkie CustomEmbeddings 구현\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self._cached_dim = None\n",
    "        self._cached_info = None\n",
    "        self._client = httpx.Client(timeout=60.0, verify=False)\n",
    "        self._tokenizer = None  # lazy init\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"리소스 정리\"\"\"\n",
    "        if hasattr(self, \"_client\"):\n",
    "            self._client.close()\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"임베딩 차원 반환 (최초 임베딩 시 캐싱)\"\"\"\n",
    "        if self._cached_dim is None:\n",
    "            # 더미 텍스트로 차원 확인\n",
    "            self._cached_dim = len(self.embed(\"dimension check\"))\n",
    "        return self._cached_dim\n",
    "\n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        \"\"\"단일 텍스트 임베딩\"\"\"\n",
    "        resp = self._client.post(f\"{self.base_url}/embed\", json={\"inputs\": text}, timeout=30.0)\n",
    "        resp.raise_for_status()\n",
    "        return np.array(resp.json()[0])  # TEI는 배열로 반환\n",
    "\n",
    "    def embed_batch(self, texts: list[str]) -> list[np.ndarray]:\n",
    "        \"\"\"배치 텍스트 임베딩\"\"\"\n",
    "        resp = self._client.post(f\"{self.base_url}/embed\", json={\"inputs\": texts}, timeout=60.0)\n",
    "        resp.raise_for_status()\n",
    "        return [np.array(vec) for vec in resp.json()]\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"토큰 수 측정 (TEI /tokenize 엔드포인트 사용)\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            resp = self._client.post(\n",
    "                f\"{self.base_url}/tokenize\", json={\"inputs\": text}, timeout=30.0\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            # 응답 형식: List[List[TokenizeResponse]]\n",
    "            # TokenizeResponse: {\"id\": int, \"text\": str, \"start\": int, \"stop\": int}\n",
    "            result = resp.json()\n",
    "            if result and len(result) > 0:\n",
    "                token_count = len(result[0])\n",
    "                # 명시적으로 int로 변환 (numpy int 등을 방지)\n",
    "                return int(token_count)\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: count_tokens failed for text (len={len(text)}): {e}\")\n",
    "            # Fallback: encode를 사용\n",
    "            return len(self.encode(text))\n",
    "\n",
    "    def count_tokens_batch(self, texts: list[str]) -> list[int]:\n",
    "        \"\"\"배치 토큰 수 측정 (각 텍스트를 순차적으로 /tokenize에 요청)\"\"\"\n",
    "        return [self.count_tokens(text) for text in texts]\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        \"\"\"토크나이저 객체 반환 (TEITokenizer wrapper 반환)\"\"\"\n",
    "        if self._tokenizer is None:\n",
    "            self._tokenizer = TEITokenizer(self)\n",
    "        return self._tokenizer\n",
    "\n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:\n",
    "        \"\"\"텍스트를 토큰 ID로 인코딩 (TEI /tokenize 엔드포인트 사용)\n",
    "\n",
    "        Args:\n",
    "            text: 인코딩할 텍스트\n",
    "            add_special_tokens: 특수 토큰 추가 여부 (TEI가 자동 처리)\n",
    "\n",
    "        Returns:\n",
    "            토큰 ID 리스트\n",
    "        \"\"\"\n",
    "        resp = self._client.post(f\"{self.base_url}/tokenize\", json={\"inputs\": text}, timeout=30.0)\n",
    "        resp.raise_for_status()\n",
    "        # TEI 응답: List[List[TokenizeResponse]]\n",
    "        # TokenizeResponse: {\"id\": int, \"text\": str, \"start\": int, \"stop\": int}\n",
    "        token_data = resp.json()[0]\n",
    "        token_ids = [token[\"id\"] for token in token_data]\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize(self, text: str) -> list[int]:\n",
    "        \"\"\"텍스트를 토큰화 (encode와 동일)\n",
    "\n",
    "        Args:\n",
    "            text: 토큰화할 텍스트\n",
    "\n",
    "        Returns:\n",
    "            토큰 ID 리스트\n",
    "        \"\"\"\n",
    "        return self.encode(text)\n",
    "\n",
    "    def encode_batch(self, texts: list[str]) -> list[list[int]]:\n",
    "        \"\"\"배치 텍스트를 토큰 ID로 인코딩\n",
    "\n",
    "        Args:\n",
    "            texts: 인코딩할 텍스트 리스트\n",
    "\n",
    "        Returns:\n",
    "            토큰 ID 리스트의 리스트\n",
    "        \"\"\"\n",
    "        return [self.encode(text) for text in texts]\n",
    "\n",
    "    def decode_batch(self, token_sequences: list[list[int]]) -> list[str]:\n",
    "        \"\"\"배치 토큰 ID를 텍스트로 디코딩\n",
    "\n",
    "        Args:\n",
    "            token_sequences: 디코딩할 토큰 ID 리스트의 리스트\n",
    "\n",
    "        Returns:\n",
    "            디코딩된 텍스트 리스트\n",
    "        \"\"\"\n",
    "        return [self.decode(tokens) for tokens in token_sequences]\n",
    "\n",
    "    def decode(self, token_ids: list[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"토큰 ID를 텍스트로 디코딩\n",
    "\n",
    "        Args:\n",
    "            token_ids: 디코딩할 토큰 ID 리스트\n",
    "            skip_special_tokens: 특수 토큰 제거 여부\n",
    "\n",
    "        Returns:\n",
    "            디코딩된 텍스트\n",
    "        \"\"\"\n",
    "        # Lazy loading: 첫 호출 시 로컬 토크나이저 로드\n",
    "        if not hasattr(self, \"_local_tokenizer\"):\n",
    "            try:\n",
    "                from tokenizers import Tokenizer\n",
    "\n",
    "                info = self.get_info()\n",
    "                model_id = info.get(\"model_id\", \"Qwen/Qwen3-Embedding-4B\")\n",
    "                self._local_tokenizer = Tokenizer.from_pretrained(model_id)\n",
    "            except Exception as e:\n",
    "                print(f\"로컬 토크나이저 로드 실패: {e}. 'pip install tokenizers'로 설치 필요\")\n",
    "                raise\n",
    "\n",
    "        return self._local_tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    @classmethod\n",
    "    def is_available(cls) -> bool:\n",
    "        \"\"\"TEI 서버 가용성 확인\"\"\"\n",
    "        try:\n",
    "            with httpx.Client(timeout=5.0, verify=False) as client:\n",
    "                r = client.get(f\"{TEI_BASE_URL}/health\")\n",
    "                return r.status_code == 200\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def get_info(self) -> dict:\n",
    "        \"\"\"TEI 서버의 모델 정보 조회 (/info 엔드포인트 사용)\"\"\"\n",
    "        if self._cached_info is None:\n",
    "            try:\n",
    "                resp = self._client.get(f\"{self.base_url}/info\", timeout=10.0)\n",
    "                resp.raise_for_status()\n",
    "                self._cached_info = resp.json()\n",
    "            except Exception:\n",
    "                self._cached_info = {}\n",
    "        return self._cached_info\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"모델 정보를 포함한 문자열 표현\"\"\"\n",
    "        try:\n",
    "            info = self.get_info()\n",
    "            model_id = info.get(\"model_id\", \"unknown\")\n",
    "            version = info.get(\"version\", \"unknown\")\n",
    "            return f\"TEIEmbeddings(base_url='{self.base_url}', model='{model_id}', version='{version}')\"\n",
    "        except Exception:\n",
    "            return f\"TEIEmbeddings(base_url='{self.base_url}')\"\n",
    "\n",
    "\n",
    "# TEI 서버 가용성 확인\n",
    "print(\"\\n[1/4] TEI 서버 가용성 확인\")\n",
    "print(f\"   TEI URL: {TEI_BASE_URL}\")\n",
    "if TEIEmbeddings.is_available():\n",
    "    print(\"    TEI 서버 연결 성공!\")\n",
    "\n",
    "# TEIEmbeddings 인스턴스 생성\n",
    "print(\"\\n[2/4] TEIEmbeddings 초기화\")\n",
    "chunkie_tei_embeddings = TEIEmbeddings(base_url=TEI_BASE_URL)\n",
    "print(f\"   {chunkie_tei_embeddings}\")\n",
    "\n",
    "# 단일 임베딩 테스트\n",
    "print(\"\\n[3/4] 단일 임베딩 테스트\")\n",
    "vec = chunkie_tei_embeddings.embed(\"Huggingface TEI 연동 테스트\")\n",
    "print(f\"   임베딩 차원: {len(vec)}\")\n",
    "print(f\"   임베딩 벡터 (처음 5개): {vec[:5]}\")\n",
    "\n",
    "# 배치 임베딩 테스트\n",
    "print(\"\\n[4/4] 배치 임베딩 테스트\")\n",
    "texts = [\"첫 번째 문장\", \"두 번째 문장\", \"세 번째 문장\"]\n",
    "vecs = chunkie_tei_embeddings.embed_batch(texts)\n",
    "print(f\"   입력 개수: {len(texts)}\")\n",
    "print(f\"   출력 개수: {len(vecs)}\")\n",
    "print(f\"   각 벡터 차원: {len(vecs[0])}\")\n",
    "\n",
    "# Tokenizer 기능 테스트\n",
    "print(\"\\n[추가] Tokenizer 기능 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. get_tokenizer() 테스트\n",
    "print(\"\\n[1] get_tokenizer() 테스트\")\n",
    "tokenizer = chunkie_tei_embeddings.get_tokenizer()\n",
    "print(f\"   토크나이저 타입: {type(tokenizer)}\")\n",
    "print(f\"   토크나이저 == self: {tokenizer is chunkie_tei_embeddings}\")\n",
    "\n",
    "# 2. encode() 테스트\n",
    "print(\"\\n[2] encode() 테스트\")\n",
    "test_text = \"안녕하세요, Chonkie 테스트입니다.\"\n",
    "token_ids = chunkie_tei_embeddings.encode(test_text)\n",
    "print(f\"   입력 텍스트: {test_text}\")\n",
    "print(f\"   토큰 ID 개수: {len(token_ids)}\")\n",
    "print(f\"   토큰 ID (처음 10개): {token_ids[:10]}\")\n",
    "\n",
    "# 3. decode() 테스트\n",
    "print(\"\\n[3] decode() 테스트\")\n",
    "decoded_text = chunkie_tei_embeddings.decode(token_ids)\n",
    "print(f\"   원본 텍스트: {test_text}\")\n",
    "print(f\"   디코딩 텍스트: {decoded_text}\")\n",
    "print(f\"   일치 여부: {test_text.strip() == decoded_text.strip()}\")\n",
    "\n",
    "# 4. Tokenizer Protocol 호환성 확인\n",
    "print(\"\\n[4] Tokenizer Protocol 호환성\")\n",
    "print(f\"   hasattr(tokenizer, 'encode'): {hasattr(tokenizer, 'encode')}\")\n",
    "print(f\"   hasattr(tokenizer, 'decode'): {hasattr(tokenizer, 'decode')}\")\n",
    "print(f\"   callable(tokenizer.encode): {callable(tokenizer.encode)}\")\n",
    "print(f\"   callable(tokenizer.decode): {callable(tokenizer.decode)}\")\n",
    "\n",
    "print(\"\\nTokenizer 구현 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEIEmbeddings와 Chonkie Chunker 통합 테스트\n",
      "================================================================================\n",
      "\n",
      "[DEBUG] TEITokenizer와 AutoTokenizer 확인\n",
      "   TEITokenizer 타입: <class 'chonkie.tokenizer.TEITokenizer'>\n",
      "   TEITokenizer 클래스명: TEITokenizer\n",
      "   'chonkie' in type string: True\n",
      "\n",
      "   AutoTokenizer Backend: chonkie\n",
      "   Wrapped tokenizer type: <class 'chonkie.tokenizer.TEITokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:56:30.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized TokenChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:30.212\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.token\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m128\u001b[0m - \u001b[34m\u001b[1mChunking text of length 392 with chunk_size=100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   count_tokens('테스트'): 3\n",
      "   타입: <class 'int'>\n",
      "   isinstance(count, int): True\n",
      "   count >= 0: True\n",
      "   ✅ count_tokens 성공!\n",
      "\n",
      "\n",
      "[1] TokenChunker + TEIEmbeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:56:30.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.token\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mCreated 3 chunks from 237 tokens\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:30.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized SemanticChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:30.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m531\u001b[0m - \u001b[34m\u001b[1mStarting semantic chunking for text of length 392\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 3\n",
      "   첫 번째 청크 길이: 170 문자\n",
      "   첫 번째 청크 미리보기: \n",
      "인공지능 기술의 발전은 우리 삶의 많은 부분을 변화시키고 있습니다.\n",
      "특히 자연어 처리 분야에서는 놀라운 성과를 보이고 있습니다.\n",
      "\n",
      "텍스트 임베딩은 자연어 처리의 핵심 기술 중 하...\n",
      "   ✅ TokenChunker 통합 성공!\n",
      "\n",
      "[2] SemanticChunker + TEIEmbeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:56:34.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m535\u001b[0m - \u001b[34m\u001b[1mPrepared 11 sentences for semantic analysis\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:36.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m573\u001b[0m - \u001b[1mCreated 3 semantic chunks from 11 sentences\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:36.029\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized SentenceChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:36.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m304\u001b[0m - \u001b[34m\u001b[1mChunking text of length 392\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 3\n",
      "   첫 번째 청크 길이: 222 문자\n",
      "   첫 번째 청크 미리보기: \n",
      "인공지능 기술의 발전은 우리 삶의 많은 부분을 변화시키고 있습니다.\n",
      "특히 자연어 처리 분야에서는 놀라운 성과를 보이고 있습니다.\n",
      "\n",
      "텍스트 임베딩은 자연어 처리의 핵심 기술 중 하...\n",
      "   ✅ SemanticChunker 통합 성공!\n",
      "\n",
      "[3] SentenceChunker + TEIEmbeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:56:39.625\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m312\u001b[0m - \u001b[34m\u001b[1mPrepared 11 sentences for chunking\u001b[0m\n",
      "\u001b[32m2025-11-10 17:56:40.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mCreated 2 chunks from text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 2\n",
      "   첫 번째 청크 길이: 222 문자\n",
      "   첫 번째 청크 미리보기: \n",
      "인공지능 기술의 발전은 우리 삶의 많은 부분을 변화시키고 있습니다.\n",
      "특히 자연어 처리 분야에서는 놀라운 성과를 보이고 있습니다.\n",
      "\n",
      "텍스트 임베딩은 자연어 처리의 핵심 기술 중 하...\n",
      "   ✅ SentenceChunker 통합 성공!\n",
      "\n",
      "================================================================================\n",
      "✅ TEIEmbeddings와 Chonkie Chunker 통합 테스트 완료!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================================\n",
    "# TEIEmbeddings와 Chonkie Chunker 통합 테스트\n",
    "# ===========================================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEIEmbeddings와 Chonkie Chunker 통합 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 디버깅: TEITokenizer와 AutoTokenizer 확인\n",
    "print(\"\\n[DEBUG] TEITokenizer와 AutoTokenizer 확인\")\n",
    "from chonkie.tokenizer import AutoTokenizer\n",
    "\n",
    "# get_tokenizer()로 TEITokenizer 가져오기\n",
    "tei_tokenizer = chunkie_tei_embeddings.get_tokenizer()\n",
    "print(f\"   TEITokenizer 타입: {type(tei_tokenizer)}\")\n",
    "print(f\"   TEITokenizer 클래스명: {tei_tokenizer.__class__.__name__}\")\n",
    "print(f\"   'chonkie' in type string: {'chonkie' in str(type(tei_tokenizer)).lower()}\")\n",
    "\n",
    "# AutoTokenizer로 래핑\n",
    "test_auto_tokenizer = AutoTokenizer(tei_tokenizer)\n",
    "print(f\"\\n   AutoTokenizer Backend: {test_auto_tokenizer._backend}\")\n",
    "print(f\"   Wrapped tokenizer type: {type(test_auto_tokenizer.tokenizer)}\")\n",
    "\n",
    "# 간단한 토큰 카운트 테스트\n",
    "test_text = \"테스트\"\n",
    "try:\n",
    "    count = test_auto_tokenizer.count_tokens(test_text)\n",
    "    print(f\"\\n   count_tokens('{test_text}'): {count}\")\n",
    "    print(f\"   타입: {type(count)}\")\n",
    "    print(f\"   isinstance(count, int): {isinstance(count, int)}\")\n",
    "    print(f\"   count >= 0: {count >= 0}\")\n",
    "    print(\"    count_tokens 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"     count_tokens 오류: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()\n",
    "\n",
    "print()\n",
    "\n",
    "# 샘플 텍스트 (한국어)\n",
    "sample_text = \"\"\"\n",
    "인공지능 기술의 발전은 우리 삶의 많은 부분을 변화시키고 있습니다.\n",
    "특히 자연어 처리 분야에서는 놀라운 성과를 보이고 있습니다.\n",
    "\n",
    "텍스트 임베딩은 자연어 처리의 핵심 기술 중 하나입니다.\n",
    "문장이나 문서를 고차원 벡터 공간에 매핑하여 의미를 수치화합니다.\n",
    "이를 통해 컴퓨터가 텍스트의 의미를 이해하고 처리할 수 있게 됩니다.\n",
    "\n",
    "청킹(Chunking)은 긴 텍스트를 작은 단위로 나누는 과정입니다.\n",
    "검색 증강 생성(RAG) 시스템에서 매우 중요한 전처리 단계입니다.\n",
    "적절한 청킹 전략은 검색 품질과 생성 결과에 큰 영향을 미칩니다.\n",
    "\n",
    "의미 기반 청킹은 텍스트의 의미적 일관성을 고려합니다.\n",
    "단순히 길이만 고려하는 것이 아니라 내용의 연관성을 판단합니다.\n",
    "이를 위해 임베딩 모델이 필수적으로 사용됩니다.\n",
    "\"\"\"\n",
    "\n",
    "# 1. TokenChunker와 TEIEmbeddings\n",
    "print(\"\\n[1] TokenChunker + TEIEmbeddings\")\n",
    "try:\n",
    "    from chonkie.chunker import TokenChunker\n",
    "\n",
    "    token_chunker = TokenChunker(\n",
    "        tokenizer=chunkie_tei_embeddings.get_tokenizer(), chunk_size=100, chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    token_chunks = token_chunker.chunk(sample_text)\n",
    "    print(f\"   생성된 청크 수: {len(token_chunks)}\")\n",
    "    print(\n",
    "        f\"   첫 번째 청크 길이: {len(token_chunks[0].text) if hasattr(token_chunks[0], 'text') else len(str(token_chunks[0]))} 문자\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   첫 번째 청크 미리보기: {(token_chunks[0].text if hasattr(token_chunks[0], 'text') else str(token_chunks[0]))[:100]}...\"\n",
    "    )\n",
    "    print(\"    TokenChunker 통합 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"     오류: {e}\")\n",
    "\n",
    "# 2. SemanticChunker와 TEIEmbeddings\n",
    "print(\"\\n[2] SemanticChunker + TEIEmbeddings\")\n",
    "try:\n",
    "    from chonkie.chunker import SemanticChunker\n",
    "\n",
    "    semantic_chunker = SemanticChunker(\n",
    "        embedding_model=chunkie_tei_embeddings, chunk_size=200, threshold=0.5\n",
    "    )\n",
    "\n",
    "    semantic_chunks = semantic_chunker.chunk(sample_text)\n",
    "    print(f\"   생성된 청크 수: {len(semantic_chunks)}\")\n",
    "    print(\n",
    "        f\"   첫 번째 청크 길이: {len(semantic_chunks[0].text) if hasattr(semantic_chunks[0], 'text') else len(str(semantic_chunks[0]))} 문자\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   첫 번째 청크 미리보기: {(semantic_chunks[0].text if hasattr(semantic_chunks[0], 'text') else str(semantic_chunks[0]))[:100]}...\"\n",
    "    )\n",
    "    print(\"    SemanticChunker 통합 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"     오류: {e}\")\n",
    "\n",
    "# 3. SentenceChunker와 TEIEmbeddings\n",
    "print(\"\\n[3] SentenceChunker + TEIEmbeddings\")\n",
    "try:\n",
    "    from chonkie.chunker import SentenceChunker\n",
    "\n",
    "    sentence_chunker = SentenceChunker(\n",
    "        tokenizer=chunkie_tei_embeddings.get_tokenizer(), chunk_size=150, chunk_overlap=10\n",
    "    )\n",
    "\n",
    "    sentence_chunks = sentence_chunker.chunk(sample_text)\n",
    "    print(f\"   생성된 청크 수: {len(sentence_chunks)}\")\n",
    "    print(\n",
    "        f\"   첫 번째 청크 길이: {len(sentence_chunks[0].text) if hasattr(sentence_chunks[0], 'text') else len(str(sentence_chunks[0]))} 문자\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   첫 번째 청크 미리보기: {(sentence_chunks[0].text if hasattr(sentence_chunks[0], 'text') else str(sentence_chunks[0]))[:100]}...\"\n",
    "    )\n",
    "    print(\"    SentenceChunker 통합 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"     오류: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" TEIEmbeddings와 Chonkie Chunker 통합 테스트 완료!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6bbdb",
   "metadata": {},
   "source": [
    "## 3. Chunkie 에서 제공하는 Chunkers\n",
    "\n",
    "다양한 청킹 전략을 비교합니다.\n",
    "\n",
    "**참고 문서:**\n",
    "- [Chunkers 개요](https://docs.chonkie.ai/oss/chunkers/overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a9efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:42:06.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized TokenChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.token\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m128\u001b[0m - \u001b[34m\u001b[1mChunking text of length 411 with chunk_size=64\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.token\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mCreated 8 chunks from 411 tokens\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized SentenceChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m304\u001b[0m - \u001b[34m\u001b[1mChunking text of length 411\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.658\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m312\u001b[0m - \u001b[34m\u001b[1mPrepared 12 sentences for chunking\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.sentence\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mCreated 4 chunks from text\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized RecursiveChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.660\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.recursive\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mStarting recursive chunking for text of length 411\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.recursive\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mCreated 4 chunks using recursive chunking\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.660\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized SemanticChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m531\u001b[0m - \u001b[34m\u001b[1mStarting semantic chunking for text of length 411\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:06.664\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m535\u001b[0m - \u001b[34m\u001b[1mPrepared 11 sentences for semantic analysis\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "샘플 텍스트 길이: 411 문자\n",
      "\n",
      "[1/4] TokenChunker (고정 토큰)\n",
      "   생성된 청크 수: 8\n",
      "   첫 청크: Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여...\n",
      "\n",
      "[2/4] SentenceChunker (문장 단위)\n",
      "   생성된 청크 수: 4\n",
      "   첫 청크: Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다...\n",
      "\n",
      "[3/4] RecursiveChunker (재귀적 분할)\n",
      "   생성된 청크 수: 4\n",
      "   첫 청크: Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다...\n",
      "\n",
      "[4/4] SemanticChunker (의미 유사도 기반)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 17:42:08.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.semantic\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m573\u001b[0m - \u001b[1mCreated 3 semantic chunks from 11 sentences\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:08.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized SlumberChunker\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:08.105\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.slumber\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m249\u001b[0m - \u001b[34m\u001b[1mStarting slumber chunking for text of length 411\u001b[0m\n",
      "\u001b[32m2025-11-10 17:42:08.105\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.slumber\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m251\u001b[0m - \u001b[34m\u001b[1mCreated 11 initial splits for LLM-based semantic boundary detection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 3\n",
      "   첫 청크: Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "...\n",
      "\n",
      "[5/5] SlumberChunker (LLM 기반 Contextual Chunking)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 11/11 splits processed [00:03<00:00,  2.99split/s] 🌱\u001b[32m2025-11-10 17:42:11.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.slumber\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1mCreated 2 chunks using LLM-guided semantic splitting\u001b[0m\n",
      "🦛 choooooooooooooooooooonk 100% • 11/11 splits processed [00:03<00:00,  3.26split/s] 🌱"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 2\n",
      "   첫 청크: Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다...\n",
      "청크 수 비교:\n",
      "  TokenChunker:     8개\n",
      "  SentenceChunker:  4개\n",
      "  RecursiveChunker: 4개\n",
      "  SemanticChunker:  3개\n",
      "  SlumberChunker:    2개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================================\n",
    "# Chunkers - TokenChunker, SentenceChunker, RecursiveChunker, SemanticChunker, SlumberChunker\n",
    "# ===========================================================================================\n",
    "import os\n",
    "\n",
    "from chonkie.chunker import (\n",
    "    RecursiveChunker,\n",
    "    SemanticChunker,\n",
    "    SentenceChunker,\n",
    "    SlumberChunker,\n",
    "    TokenChunker,\n",
    ")\n",
    "from chonkie.genie import OpenAIGenie\n",
    "\n",
    "# 샘플 텍스트 준비\n",
    "sample_text = \"\"\"Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
    "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
    "\n",
    "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
    "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
    "\n",
    "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
    "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
    "\n",
    "세 번째로 RecursiveChunker는 계층적으로 분할합니다.\n",
    "문단, 문장, 단어 순서로 구조를 보존합니다.\n",
    "\n",
    "네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\n",
    "가장 정교하지만 임베딩 모델이 필요합니다.\n",
    "\n",
    "마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.\"\"\"\n",
    "\n",
    "print(f\"\\n샘플 텍스트 길이: {len(sample_text)} 문자\\n\")\n",
    "# 1) TokenChunker\n",
    "print(\"[1/4] TokenChunker (고정 토큰)\")\n",
    "chunker_token = TokenChunker(chunk_size=64, chunk_overlap=10)\n",
    "chunks_token = chunker_token.chunk(sample_text)\n",
    "print(f\"   생성된 청크 수: {len(chunks_token)}\")\n",
    "print(f\"   첫 청크: {chunks_token[0].text[:80]}...\")\n",
    "\n",
    "# 2) SentenceChunker\n",
    "print(\"\\n[2/4] SentenceChunker (문장 단위)\")\n",
    "chunker_sent = SentenceChunker(chunk_size=128, min_sentences_per_chunk=2)\n",
    "chunks_sent = chunker_sent.chunk(sample_text)\n",
    "print(f\"   생성된 청크 수: {len(chunks_sent)}\")\n",
    "print(f\"   첫 청크: {chunks_sent[0].text[:80]}...\")\n",
    "\n",
    "# 3) RecursiveChunker\n",
    "print(\"\\n[3/4] RecursiveChunker (재귀적 분할)\")\n",
    "chunker_recur = RecursiveChunker(chunk_size=128, min_characters_per_chunk=24)\n",
    "chunks_recur = chunker_recur.chunk(sample_text)\n",
    "print(f\"   생성된 청크 수: {len(chunks_recur)}\")\n",
    "print(f\"   첫 청크: {chunks_recur[0].text[:80]}...\")\n",
    "\n",
    "# 4) SemanticChunker (임베딩 필요)\n",
    "print(\"\\n[4/4] SemanticChunker (의미 유사도 기반)\")\n",
    "chunker_sem = SemanticChunker(\n",
    "    embedding_model=chunkie_openai_embeddins,\n",
    "    # embedding_model=chunkie_tei_embeddings,\n",
    "    chunk_size=1000,\n",
    "    threshold=0.5,\n",
    ")\n",
    "chunks_sem = chunker_sem.chunk(sample_text)\n",
    "print(f\"   생성된 청크 수: {len(chunks_sem)}\")\n",
    "print(f\"   첫 청크: {chunks_sem[0].text}...\")\n",
    "\n",
    "# 5) SlumberChunker (LLM 기반 Contextual Chunking)\n",
    "print(\"\\n[5/5] SlumberChunker (LLM 기반 Contextual Chunking)\")\n",
    "chunker_slumber = SlumberChunker(\n",
    "    genie=OpenAIGenie(\n",
    "        model=\"openai/gpt-4.1\",\n",
    "        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    ),\n",
    "    chunk_size=1200,\n",
    "    candidate_size=200,\n",
    ")\n",
    "chunks_slumber = chunker_slumber.chunk(sample_text)\n",
    "print(f\"   생성된 청크 수: {len(chunks_slumber)}\")\n",
    "print(f\"   첫 청크: {chunks_slumber[0].text[:80]}...\")\n",
    "\n",
    "# 청크 수 비교\n",
    "print(\"청크 수 비교:\")\n",
    "print(f\"  TokenChunker:     {len(chunks_token)}개\")\n",
    "print(f\"  SentenceChunker:  {len(chunks_sent)}개\")\n",
    "print(f\"  RecursiveChunker: {len(chunks_recur)}개\")\n",
    "print(f\"  SemanticChunker:  {len(chunks_sem)}개\")\n",
    "print(f\"  SlumberChunker:    {len(chunks_slumber)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b2f03",
   "metadata": {},
   "source": [
    "### Experimental: CodeChunker\n",
    "\n",
    "코드 전용 청킹 전략입니다. 프로그래밍 언어의 구조를 인식하여 분할합니다.\n",
    "\n",
    "**참고 문서:**\n",
    "- [Code Chunker](https://docs.chonkie.ai/oss/experimental/code-chunker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecec604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "코드 샘플 길이: 494 문자\n",
      "\n",
      "[1/1] CodeChunker (Python)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:27:50.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized CodeChunker\u001b[0m\n",
      "\u001b[32m2025-11-09 17:27:51.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.code\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m328\u001b[0m - \u001b[34m\u001b[1mStarting code chunking for text of length 494\u001b[0m\n",
      "\u001b[32m2025-11-09 17:27:51.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.code\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m339\u001b[0m - \u001b[34m\u001b[1mUsing configured language: python\u001b[0m\n",
      "\u001b[32m2025-11-09 17:27:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.code\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mCreated 3 code chunks from parsed syntax tree\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   생성된 청크 수: 3\n",
      "\n",
      "   --- 청크 1 ---\n",
      "   \n",
      "# utils.py\n",
      "def add(a, b):\n",
      "    \"\"\"두 수를 더합니다.\"\"\"\n",
      "    return a + b\n",
      "\n",
      "def subtract(a, b):\n",
      "    \"\"\"두 수를 뺍니다.\"\"\"\n",
      "    return a - b\n",
      "\n",
      "...\n",
      "\n",
      "   --- 청크 2 ---\n",
      "   class Calculator:\n",
      "    \"\"\"계산기 클래스\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.result = 0\n",
      "\n",
      "    def multiply(self, a, b):\n",
      "        \"\"\"두 수를 곱합니다.\"\"\"\n",
      "        s...\n",
      "\n",
      "   --- 청크 3 ---\n",
      "   def divide(self, a, b):\n",
      "        \"\"\"두 수를 나눕니다.\"\"\"\n",
      "        if b == 0:\n",
      "            raise ValueError(\"0으로 나눌 수 없습니다\")\n",
      "        self.result = a / b\n",
      "        ...\n",
      "\n",
      "CodeChunker는 함수/클래스 단위로 분할하여 코드 구조를 보존합니다.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CodeChunker - Experimental\n",
    "# CodeChunker는 함수/클래스 단위로 분할하여 코드 구조를 보존합니다.\n",
    "# ============================================================================\n",
    "try:\n",
    "    from chonkie.chunker import CodeChunker\n",
    "\n",
    "    # Python 코드 샘플\n",
    "    code_text = '''\n",
    "# utils.py\n",
    "def add(a, b):\n",
    "    \"\"\"두 수를 더합니다.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    \"\"\"두 수를 뺍니다.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "class Calculator:\n",
    "    \"\"\"계산기 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "\n",
    "    def multiply(self, a, b):\n",
    "        \"\"\"두 수를 곱합니다.\"\"\"\n",
    "        self.result = a * b\n",
    "        return self.result\n",
    "\n",
    "    def divide(self, a, b):\n",
    "        \"\"\"두 수를 나눕니다.\"\"\"\n",
    "        if b == 0:\n",
    "            raise ValueError(\"0으로 나눌 수 없습니다\")\n",
    "        self.result = a / b\n",
    "        return self.result\n",
    "'''\n",
    "\n",
    "    print(f\"\\n코드 샘플 길이: {len(code_text)} 문자\\n\")\n",
    "\n",
    "    # CodeChunker로 분할\n",
    "    print(\"[1/1] CodeChunker (Python)\")\n",
    "    chunker_code = CodeChunker(language=\"python\", chunk_size=200)\n",
    "    chunks_code = chunker_code.chunk(code_text)\n",
    "\n",
    "    print(f\"   생성된 청크 수: {len(chunks_code)}\")\n",
    "    for i, chunk in enumerate(chunks_code):\n",
    "        print(f\"\\n   --- 청크 {i + 1} ---\")\n",
    "        print(f\"   {chunk.text}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  pip install 'chonkie[code]' 또는 'chonkie[all]'로 설치하세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bd7b3",
   "metadata": {},
   "source": [
    "## 4. Refinery (문맥 보강)\n",
    "\n",
    "청크에 `추가 문맥 정보`를 부착하여 검색 품질을 향상시킬 수 있습니다.\n",
    "\n",
    "**종류:**\n",
    "- **OverlapRefinery**: 인접 청크의 내용을 오버랩으로 추가\n",
    "- **EmbeddingsRefinery**: 임베딩 벡터를 청크에 부착\n",
    "\n",
    "**참고 문서:**\n",
    "- [Refinery 개요](https://docs.chonkie.ai/oss/refinery/overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f2cfc",
   "metadata": {},
   "source": [
    "### Refinery 상세 설명\n",
    "\n",
    "#### EmbeddingsRefinery\n",
    "\n",
    "**클래스 개요**\n",
    "\n",
    "`EmbeddingsRefinery`는 청크의 텍스트를 임베딩 모델을 사용하여 임베딩하고, 청크에 임베딩 벡터를 부착합니다. 이는 벡터 데이터베이스에 upsert하는 등의 다운스트림 작업에 사용됩니다.\n",
    "\n",
    "**초기화 (`__init__`)**\n",
    "\n",
    "```python\n",
    "def __init__(\n",
    "    self,\n",
    "    embedding_model: Union[str, BaseEmbeddings, AutoEmbeddings] = \"minishlab/potion-retrieval-32M\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "**파라미터:**\n",
    "- `embedding_model`: 사용할 임베딩 모델\n",
    "  - 타입: `Union[str, BaseEmbeddings, AutoEmbeddings]`\n",
    "  - 기본값: `\"minishlab/potion-retrieval-32M\"`\n",
    "  - 설명: 문자열로 모델 이름을 전달하거나, `BaseEmbeddings` 인스턴스를 직접 전달할 수 있습니다. 문자열인 경우 `AutoEmbeddings`를 통해 자동으로 로드됩니다.\n",
    "\n",
    "**리파인 메서드 (`refine`)**\n",
    "\n",
    "```python\n",
    "def refine(self, chunks: List[Chunk]) -> List[Chunk]:\n",
    "    \"\"\"Refine the chunks.\"\"\"\n",
    "    logger.debug(f\"Starting embedding refinery for {len(chunks)} chunks\")\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "    embeds = self.embedding_model.embed_batch(texts)\n",
    "    for chunk, embed in zip(chunks, embeds):\n",
    "        chunk.embedding = embed\n",
    "    logger.info(f\"Embedding refinement complete: added embeddings to {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "```\n",
    "\n",
    "**동작 방식:**\n",
    "1. 입력된 청크 리스트에서 텍스트를 추출합니다.\n",
    "2. `embed_batch()` 메서드를 사용하여 배치로 임베딩을 생성합니다 (성능 최적화).\n",
    "3. 각 청크에 해당하는 임베딩 벡터를 `chunk.embedding` 속성에 부착합니다.\n",
    "4. 임베딩이 부착된 청크 리스트를 반환합니다.\n",
    "\n",
    "**사용 예시:**\n",
    "\n",
    "```python\n",
    "from chonkie.refinery import EmbeddingsRefinery\n",
    "from chonkie.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# EmbeddingsRefinery 초기화\n",
    "refinery = EmbeddingsRefinery(embedding_model=embeddings)\n",
    "\n",
    "# 청크에 임베딩 부착\n",
    "refined_chunks = refinery.refine(chunks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### OverlapRefinery\n",
    "\n",
    "**클래스 개요**\n",
    "\n",
    "`OverlapRefinery`는 청크에 오버랩을 추가하여 문맥을 보강합니다. LRU 캐싱(maxsize=8192)을 사용하여 토크나이제이션 작업의 성능을 향상시킵니다. 캐시는 `cache_info()`로 모니터링하고 `clear_cache()`로 초기화할 수 있습니다.\n",
    "\n",
    "**초기화 (`__init__`)**\n",
    "\n",
    "```python\n",
    "def __init__(\n",
    "    self,\n",
    "    tokenizer: Union[str, TokenizerProtocol] = \"character\",\n",
    "    context_size: Union[int, float] = 0.25,\n",
    "    mode: Literal[\"token\", \"recursive\"] = \"token\",\n",
    "    method: Literal[\"suffix\", \"prefix\"] = \"suffix\",\n",
    "    rules: RecursiveRules = RecursiveRules(),\n",
    "    merge: bool = True,\n",
    "    inplace: bool = True,\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "**파라미터:**\n",
    "\n",
    "| 파라미터 | 타입 | 기본값 | 설명 |\n",
    "|---------|------|-------|------|\n",
    "| `tokenizer` | `Union[str, TokenizerProtocol]` | `\"character\"` | 사용할 토크나이저. 제공되지 않으면 문자 단위 오버랩을 기본으로 사용합니다. |\n",
    "| `context_size` | `Union[int, float]` | `0.25` | 청크에 추가할 컨텍스트 크기. 정수인 경우 토큰 수, float인 경우 최대 청크 크기의 비율 (0-1 사이). |\n",
    "| `mode` | `Literal[\"token\", \"recursive\"]` | `\"token\"` | 오버랩 모드. `\"token\"`은 토큰 기반, `\"recursive\"`는 재귀적 분할 기반. |\n",
    "| `method` | `Literal[\"suffix\", \"prefix\"]` | `\"suffix\"` | 컨텍스트 추가 방법. `\"suffix\"`는 다음 청크의 시작 부분을 현재 청크 끝에 추가, `\"prefix\"`는 이전 청크의 끝 부분을 현재 청크 시작에 추가. |\n",
    "| `rules` | `RecursiveRules` | `RecursiveRules()` | 재귀적 오버랩에 사용할 규칙. 기본값은 빈 규칙입니다. |\n",
    "| `merge` | `bool` | `True` | 컨텍스트를 청크 텍스트에 병합할지 여부. `True`이면 텍스트에 직접 추가, `False`이면 `context` 속성으로만 저장. |\n",
    "| `inplace` | `bool` | `True` | 청크를 제자리에서 수정할지 여부. `False`이면 청크의 복사본을 생성합니다. |\n",
    "\n",
    "**리파인 메서드 (`refine`)**\n",
    "\n",
    "```python\n",
    "def refine(self, chunks: List[Chunk]) -> List[Chunk]:\n",
    "    \"\"\"Refine the chunks based on the overlap.\"\"\"\n",
    "    logger.debug(f\"Starting overlap refinery for {len(chunks)} chunks with method={self.method}, mode={self.mode}\")\n",
    "    \n",
    "    # 빈 청크 리스트 체크\n",
    "    if not chunks:\n",
    "        return chunks\n",
    "    \n",
    "    # 모든 청크가 같은 타입인지 확인\n",
    "    if len(set(type(chunk) for chunk in chunks)) > 1:\n",
    "        raise ValueError(\"All chunks must be of the same type.\")\n",
    "    \n",
    "    # inplace가 False이면 복사본 생성\n",
    "    if not self.inplace:\n",
    "        chunks = [chunk.copy() for chunk in chunks]\n",
    "    \n",
    "    # 효과적인 컨텍스트 크기 계산\n",
    "    effective_context_size = self._get_overlap_context_size(chunks)\n",
    "    \n",
    "    # method에 따라 prefix 또는 suffix 오버랩 적용\n",
    "    if self.method == \"prefix\":\n",
    "        refined_chunks = self._refine_prefix(chunks, effective_context_size)\n",
    "    elif self.method == \"suffix\":\n",
    "        refined_chunks = self._refine_suffix(chunks, effective_context_size)\n",
    "    \n",
    "    logger.info(f\"Overlap refinement complete: added context to {len(refined_chunks)} chunks\")\n",
    "    return refined_chunks\n",
    "```\n",
    "\n",
    "**동작 방식:**\n",
    "\n",
    "1. **Suffix 모드** (`method=\"suffix\"`):\n",
    "   - 각 청크의 끝에 다음 청크의 시작 부분을 추가합니다.\n",
    "   - 예: 청크 1의 끝에 청크 2의 시작 부분을 추가하여 문맥을 보강합니다.\n",
    "\n",
    "2. **Prefix 모드** (`method=\"prefix\"`):\n",
    "   - 각 청크의 시작에 이전 청크의 끝 부분을 추가합니다.\n",
    "   - 예: 청크 2의 시작에 청크 1의 끝 부분을 추가하여 문맥을 보강합니다.\n",
    "\n",
    "3. **Token 모드** (`mode=\"token\"`):\n",
    "   - 토크나이저를 사용하여 정확한 토큰 수만큼 컨텍스트를 추출합니다.\n",
    "\n",
    "4. **Recursive 모드** (`mode=\"recursive\"`):\n",
    "   - 재귀적 규칙을 사용하여 의미 있는 단위로 컨텍스트를 추출합니다.\n",
    "\n",
    "**성능 최적화:**\n",
    "\n",
    "- **LRU 캐싱**: 토크나이제이션 작업을 캐싱하여 반복 작업의 성능을 향상시킵니다 (maxsize=8192).\n",
    "- **캐시 모니터링**: `cache_info()` 메서드로 캐시 통계를 확인할 수 있습니다.\n",
    "- **캐시 초기화**: `clear_cache()` 메서드로 메모리를 해제할 수 있습니다.\n",
    "\n",
    "**사용 예시:**\n",
    "\n",
    "```python\n",
    "from chonkie.refinery import OverlapRefinery\n",
    "\n",
    "# 기본 설정 (문자 단위, 25% 오버랩, suffix 모드)\n",
    "refinery = OverlapRefinery()\n",
    "\n",
    "# 토큰 기반 오버랩 (50 토큰)\n",
    "refinery = OverlapRefinery(\n",
    "    tokenizer=\"tiktoken\",\n",
    "    context_size=50,\n",
    "    mode=\"token\",\n",
    "    method=\"suffix\"\n",
    ")\n",
    "\n",
    "# 재귀적 오버랩 (20% 비율)\n",
    "refinery = OverlapRefinery(\n",
    "    context_size=0.2,\n",
    "    mode=\"recursive\",\n",
    "    method=\"prefix\",\n",
    "    merge=True,\n",
    "    inplace=False  # 원본 청크 보존\n",
    ")\n",
    "\n",
    "# 청크에 오버랩 적용\n",
    "refined_chunks = refinery.refine(chunks)\n",
    "```\n",
    "\n",
    "**주의사항:**\n",
    "\n",
    "- `context_size`가 float인 경우, 최대 청크 크기의 비율로 계산됩니다.\n",
    "- `merge=True`일 때, 컨텍스트가 청크 텍스트에 직접 추가되므로 원본 위치 정보(`start_index`, `end_index`)는 변경되지 않습니다.\n",
    "- `inplace=False`로 설정하면 원본 청크를 보존하고 복사본을 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:36:23.339\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m456\u001b[0m - \u001b[34m\u001b[1mStarting overlap refinery for 4 chunks with method=suffix, mode=token\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:23.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1mOverlap refinement complete: added context to 4 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "원본 청크 정보\n",
      "================================================================================\n",
      "원본 청크 수: 4\n",
      "원본 첫 청크 길이: 151 문자\n",
      "원본 첫 청크 텍스트 (처음 100자): Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunke...\n",
      "\n",
      "================================================================================\n",
      "[1/3] OverlapRefinery - 다양한 옵션 비교\n",
      "================================================================================\n",
      "\n",
      "[1-1] 기본 설정 (문자 단위, context_size=0.25, suffix 모드)\n",
      "   리파인 후 첫 청크 길이: 188 문자\n",
      "   길이 증가: 0 문자\n",
      "   첫 청크 텍스트 (처음 150자): Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니...\n",
      "\n",
      "[1-2] 토큰 기반 오버랩 (context_size=50 토큰, suffix 모드)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:36:23.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m456\u001b[0m - \u001b[34m\u001b[1mStarting overlap refinery for 4 chunks with method=suffix, mode=token\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:23.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1mOverlap refinement complete: added context to 4 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:23.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m456\u001b[0m - \u001b[34m\u001b[1mStarting overlap refinery for 4 chunks with method=prefix, mode=token\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:23.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1mOverlap refinement complete: added context to 4 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:23.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mStarting embedding refinery for 4 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⚠️  토크나이저 사용 불가: Tokenizer not found in transformers, tokenizers, or tiktoken\n",
      "   문자 단위로 대체 실행...\n",
      "   리파인 후 첫 청크 길이: 238 문자\n",
      "\n",
      "[1-3] Prefix 모드 (이전 청크의 끝 부분을 현재 청크 시작에 추가)\n",
      "   리파인 후 두 번째 청크 길이: 207 문자\n",
      "   두 번째 청크 텍스트 (처음 150자): 빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "세 번째로 Rec...\n",
      "\n",
      "[1-4] LRU 캐시 정보 확인\n",
      "   토큰 캐시 정보: {'hits': 0, 'misses': 3, 'maxsize': 8192, 'currsize': 3}\n",
      "   카운트 캐시 정보: {'hits': 0, 'misses': 3, 'maxsize': 8192, 'currsize': 3}\n",
      "\n",
      "================================================================================\n",
      "[2/3] EmbeddingsRefinery - 임베딩 벡터 부착\n",
      "================================================================================\n",
      "\n",
      "[2-1] EmbeddingsRefinery 기본 사용\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:36:24.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mEmbedding refinement complete: added embeddings to 4 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:24.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m456\u001b[0m - \u001b[34m\u001b[1mStarting overlap refinery for 4 chunks with method=suffix, mode=token\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:24.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1mOverlap refinement complete: added context to 4 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 17:36:24.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mStarting embedding refinery for 4 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   리파인 후 청크 수: 4\n",
      "   첫 청크 임베딩 차원: 1536\n",
      "   임베딩 벡터 (처음 5개): [ 0.02146324  0.0103791  -0.01436661  0.02064262  0.01710586]\n",
      "   임베딩 벡터 타입: <class 'numpy.ndarray'>\n",
      "   임베딩이 성공적으로 부착되었습니다!\n",
      "\n",
      "================================================================================\n",
      "[3/3] 통합 파이프라인 - OverlapRefinery → EmbeddingsRefinery\n",
      "================================================================================\n",
      "\n",
      "[Step 1] OverlapRefinery로 문맥 보강\n",
      "   원본 첫 청크 길이: 188 문자\n",
      "   오버랩 후 첫 청크 길이: 206 문자\n",
      "   길이 증가: 18 문자\n",
      "\n",
      "[Step 2] EmbeddingsRefinery로 임베딩 부착\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:36:24.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mEmbedding refinement complete: added embeddings to 4 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   최종 청크 수: 4\n",
      "   첫 청크 임베딩 차원: 1536\n",
      "   첫 청크 텍스트 길이: 206 문자\n",
      "\n",
      "  통합 파이프라인 완료!\n",
      "   - 오버랩으로 문맥이 보강되었습니다.\n",
      "   - 임베딩 벡터가 부착되었습니다.\n",
      "   - 벡터 데이터베이스에 upsert할 준비가 되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Refinery 활용 - OverlapRefinery, EmbeddingsRefinery\n",
    "# ============================================================================\n",
    "from chonkie.refinery import EmbeddingsRefinery, OverlapRefinery\n",
    "\n",
    "# 기본 청크 사용 (이전에 생성한 chunks_sent)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"원본 청크 정보\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"원본 청크 수: {len(chunks_sent)}\")\n",
    "print(f\"원본 첫 청크 길이: {len(chunks_sent[0].text)} 문자\")\n",
    "print(f\"원본 첫 청크 텍스트 (처음 100자): {chunks_sent[0].text[:100]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1) OverlapRefinery - 다양한 옵션 비교\n",
    "# ============================================================================\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"[1/3] OverlapRefinery - 다양한 옵션 비교\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# 1-1) 기본 설정 (문자 단위, 25% 오버랩, suffix 모드)\n",
    "print(\"\\n[1-1] 기본 설정 (문자 단위, context_size=0.25, suffix 모드)\")\n",
    "refinery_basic = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.25,  # 최대 청크 크기의 25%\n",
    "    mode=\"token\",\n",
    "    method=\"suffix\",\n",
    "    merge=True,\n",
    "    inplace=True,\n",
    ")\n",
    "refined_basic = refinery_basic.refine(\n",
    "    chunks_sent.copy() if hasattr(chunks_sent, \"copy\") else chunks_sent[:]\n",
    ")\n",
    "print(f\"   리파인 후 첫 청크 길이: {len(refined_basic[0].text)} 문자\")\n",
    "print(f\"   길이 증가: {len(refined_basic[0].text) - len(chunks_sent[0].text)} 문자\")\n",
    "print(f\"   첫 청크 텍스트 (처음 150자): {refined_basic[0].text[:150]}...\")\n",
    "\n",
    "# 1-2) 토큰 기반 오버랩 (고정 토큰 수)\n",
    "print(\"\\n[1-2] 토큰 기반 오버랩 (context_size=50 토큰, suffix 모드)\")\n",
    "try:\n",
    "    refinery_token = OverlapRefinery(\n",
    "        tokenizer=\"tiktoken\",  # 또는 다른 토크나이저\n",
    "        context_size=50,  # 고정 토큰 수\n",
    "        mode=\"token\",\n",
    "        method=\"suffix\",\n",
    "        merge=True,\n",
    "        inplace=False,  # 원본 보존\n",
    "    )\n",
    "    refined_token = refinery_token.refine(\n",
    "        chunks_sent.copy() if hasattr(chunks_sent, \"copy\") else chunks_sent[:]\n",
    "    )\n",
    "    print(f\"   리파인 후 첫 청크 길이: {len(refined_token[0].text)} 문자\")\n",
    "    print(f\"   첫 청크 텍스트 (처음 150자): {refined_token[0].text[:150]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   토크나이저 사용 불가: {e}\")\n",
    "    print(\"   문자 단위로 대체 실행...\")\n",
    "    refinery_token = OverlapRefinery(\n",
    "        tokenizer=\"character\",\n",
    "        context_size=50,  # 문자 수로 대체\n",
    "        mode=\"token\",\n",
    "        method=\"suffix\",\n",
    "        merge=True,\n",
    "        inplace=False,\n",
    "    )\n",
    "    refined_token = refinery_token.refine(\n",
    "        chunks_sent.copy() if hasattr(chunks_sent, \"copy\") else chunks_sent[:]\n",
    "    )\n",
    "    print(f\"   리파인 후 첫 청크 길이: {len(refined_token[0].text)} 문자\")\n",
    "\n",
    "# 1-3) Prefix 모드 (이전 청크의 끝 부분을 현재 청크 시작에 추가)\n",
    "print(\"\\n[1-3] Prefix 모드 (이전 청크의 끝 부분을 현재 청크 시작에 추가)\")\n",
    "refinery_prefix = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.2,  # 20% 비율\n",
    "    mode=\"token\",\n",
    "    method=\"prefix\",  # prefix 모드\n",
    "    merge=True,\n",
    "    inplace=False,\n",
    ")\n",
    "refined_prefix = refinery_prefix.refine(\n",
    "    chunks_sent.copy() if hasattr(chunks_sent, \"copy\") else chunks_sent[:]\n",
    ")\n",
    "print(\n",
    "    f\"   리파인 후 두 번째 청크 길이: {len(refined_prefix[1].text) if len(refined_prefix) > 1 else 0} 문자\"\n",
    ")\n",
    "if len(refined_prefix) > 1:\n",
    "    print(f\"   두 번째 청크 텍스트 (처음 150자): {refined_prefix[1].text[:150]}...\")\n",
    "\n",
    "# 1-4) 캐시 정보 확인 (LRU 캐싱 최적화)\n",
    "print(\"\\n[1-4] LRU 캐시 정보 확인\")\n",
    "cache_info = refinery_basic.cache_info()\n",
    "print(f\"   토큰 캐시 정보: {cache_info.get('tokens_cache', {})}\")\n",
    "print(f\"   카운트 캐시 정보: {cache_info.get('count_cache', {})}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2) EmbeddingsRefinery - 임베딩 벡터 부착\n",
    "# ============================================================================\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"[2/3] EmbeddingsRefinery - 임베딩 벡터 부착\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# 기본 설정으로 임베딩 부착\n",
    "print(\"\\n[2-1] EmbeddingsRefinery 기본 사용\")\n",
    "refinery_emb = EmbeddingsRefinery(embedding_model=chunkie_openai_embeddins)\n",
    "refined_emb = refinery_emb.refine(refined_basic)\n",
    "\n",
    "print(f\"   리파인 후 청크 수: {len(refined_emb)}\")\n",
    "\n",
    "# 임베딩 확인\n",
    "if hasattr(refined_emb[0], \"embedding\"):\n",
    "    emb_vec = refined_emb[0].embedding\n",
    "    print(f\"   첫 청크 임베딩 차원: {len(emb_vec)}\")\n",
    "    print(f\"   임베딩 벡터 (처음 5개): {emb_vec[:5]}\")\n",
    "    print(f\"   임베딩 벡터 타입: {type(emb_vec)}\")\n",
    "    print(\"   임베딩이 성공적으로 부착되었습니다!\")\n",
    "elif hasattr(refined_emb[0], \"metadata\") and \"embedding\" in refined_emb[0].metadata:\n",
    "    emb_vec = refined_emb[0].metadata.get(\"embedding\")\n",
    "    print(f\"   첫 청크 임베딩 차원: {len(emb_vec)}\")\n",
    "    print(f\"   임베딩 벡터 (처음 5개): {emb_vec[:5]}\")\n",
    "    print(\"   임베딩이 메타데이터에 부착되었습니다!\")\n",
    "else:\n",
    "    print(\"    임베딩 속성을 찾을 수 없습니다.\")\n",
    "    print(f\"   Chunk 속성: {[attr for attr in dir(refined_emb[0]) if not attr.startswith('_')]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3) 통합 파이프라인 - OverlapRefinery + EmbeddingsRefinery\n",
    "# ============================================================================\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"[3/3] 통합 파이프라인 - OverlapRefinery → EmbeddingsRefinery\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# 원본 청크 복사 (inplace=False로 원본 보존)\n",
    "original_chunks = chunks_sent.copy() if hasattr(chunks_sent, \"copy\") else chunks_sent[:]\n",
    "\n",
    "# Step 1: OverlapRefinery로 문맥 보강\n",
    "print(\"\\n[Step 1] OverlapRefinery로 문맥 보강\")\n",
    "refinery_overlap_final = OverlapRefinery(\n",
    "    tokenizer=\"character\",\n",
    "    context_size=0.1,  # 10% 오버랩\n",
    "    mode=\"token\",\n",
    "    method=\"suffix\",\n",
    "    merge=True,\n",
    "    inplace=False,\n",
    ")\n",
    "chunks_with_overlap = refinery_overlap_final.refine(original_chunks)\n",
    "print(f\"   원본 첫 청크 길이: {len(original_chunks[0].text)} 문자\")\n",
    "print(f\"   오버랩 후 첫 청크 길이: {len(chunks_with_overlap[0].text)} 문자\")\n",
    "print(f\"   길이 증가: {len(chunks_with_overlap[0].text) - len(original_chunks[0].text)} 문자\")\n",
    "\n",
    "# Step 2: EmbeddingsRefinery로 임베딩 부착\n",
    "print(\"\\n[Step 2] EmbeddingsRefinery로 임베딩 부착\")\n",
    "refinery_emb_final = EmbeddingsRefinery(embedding_model=chunkie_openai_embeddins)\n",
    "chunks_final = refinery_emb_final.refine(chunks_with_overlap)\n",
    "print(f\"   최종 청크 수: {len(chunks_final)}\")\n",
    "\n",
    "# 최종 결과 확인\n",
    "if hasattr(chunks_final[0], \"embedding\") and chunks_final[0].embedding is not None:\n",
    "    print(f\"   첫 청크 임베딩 차원: {len(chunks_final[0].embedding)}\")\n",
    "    print(f\"   첫 청크 텍스트 길이: {len(chunks_final[0].text)} 문자\")\n",
    "    print(\"\\n  통합 파이프라인 완료!\")\n",
    "    print(\"   - 오버랩으로 문맥이 보강되었습니다.\")\n",
    "    print(\"   - 임베딩 벡터가 부착되었습니다.\")\n",
    "    print(\"   - 벡터 데이터베이스에 upsert할 준비가 되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded19ec6",
   "metadata": {},
   "source": [
    "## 5. Chunkie 를 통한 Qdrant Handshake (벡터DB 연동)\n",
    "\n",
    "Chonkie는 다양한 벡터 데이터베이스와의 통합을 지원합니다.   \n",
    "여기서는 앞에서부터 계속 활용하던 Qdrant를 사용합니다.  \n",
    "\n",
    "**Qdrant 서버 시작 (Docker):**\n",
    "```bash\n",
    "docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest\n",
    "```\n",
    "\n",
    "**기능:**\n",
    "- 컬렉션 자동 생성\n",
    "- 청크 upsert (색인)\n",
    "- 벡터 검색\n",
    "\n",
    "**참고 문서:**\n",
    "- [Qdrant Handshake](https://docs.chonkie.ai/oss/handshakes/qdrant-handshake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43106a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:44:32.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m191\u001b[0m - \u001b[34m\u001b[1mWriting 4 chunks to Qdrant collection: chonkie_demo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Qdrant 서버 확인\n",
      "   Qdrant URL: http://localhost:6333\n",
      "\n",
      "[2/4] QdrantHandshake 초기화\n",
      "   컬렉션: chonkie_demo\n",
      "   벡터 차원: 1536\n",
      "\n",
      "[3/4] 청크 색인 (upsert)\n",
      "   Indexing 4 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:44:33.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSuccessfully wrote 4 chunks to Qdrant collection: chonkie_demo\u001b[0m\n",
      "\u001b[32m2025-11-09 17:44:33.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mSearching Qdrant collection: chonkie_demo with limit=3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦛 Chonkie wrote 4 chunks to Qdrant collection: chonkie_demo\n",
      "    Indexing 완료!\n",
      "\n",
      "[4/4] 벡터 검색\n",
      "   쿼리: '청킹 전략의 장단점'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:44:34.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mSearch complete: found 3 matching chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   검색 결과 (상위 3개):\n",
      "\n",
      "   [1] Score: 0.4111\n",
      "       Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째\n",
      "\n",
      "   [2] Score: 0.2166\n",
      "       가장 정교하지만 임베딩 모델이 필요합니다.\n",
      "\n",
      "마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.\n",
      "\n",
      "   [3] Score: 0.2012\n",
      "       빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "세 번째로 RecursiveChunker는 계층적으로\n",
      "세 번째로 RecursiveChunker는 계층적으로 분할합니다.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Qdrant Handshake 데모 - 색인 및 검색\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# Qdrant 접속 정보\n",
    "QDRANT_PORT = 6333\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", f\"http://localhost:{QDRANT_PORT}\")\n",
    "\n",
    "print(\"\\n[1/4] Qdrant 서버 확인\")\n",
    "print(f\"   Qdrant URL: {QDRANT_URL}\")\n",
    "\n",
    "from chonkie.handshakes import QdrantHandshake\n",
    "\n",
    "# QdrantHandshake 초기화\n",
    "print(\"\\n[2/4] QdrantHandshake 초기화\")\n",
    "\n",
    "# QdrantHandshake 초기화 (embedding_model을 통해 벡터 차원 자동 감지)\n",
    "handshake = QdrantHandshake(\n",
    "    url=QDRANT_URL,\n",
    "    collection_name=\"chonkie_demo\",\n",
    "    embedding_model=chunkie_openai_embeddins,\n",
    ")\n",
    "print(\"   컬렉션: chonkie_demo\")\n",
    "print(f\"   벡터 차원: {handshake.dimension}\")\n",
    "\n",
    "# 청크 upsert (색인)\n",
    "print(\"\\n[3/4] 청크 색인 (upsert)\")\n",
    "\n",
    "# 리파인된 청크 사용 (임베딩 부착된 청크)\n",
    "# Chonkie Handshake는 임베딩이 부착된 청크를 기대함\n",
    "print(f\"   Indexing {len(refined_emb)} chunks\")\n",
    "\n",
    "try:\n",
    "    handshake.write(refined_emb)\n",
    "    print(\"    Indexing 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"   Indexing 중 오류: {e}\")\n",
    "\n",
    "# 검색\n",
    "print(\"\\n[4/4] 벡터 검색\")\n",
    "query = \"청킹 전략의 장단점\"\n",
    "print(f\"   쿼리: '{query}'\")\n",
    "\n",
    "try:\n",
    "    hits = handshake.search(query=query, limit=3)\n",
    "\n",
    "    print(f\"   검색 결과 (상위 {len(hits)}개):\")\n",
    "    for i, hit in enumerate(hits):\n",
    "        text = hit.get(\"text\", \"\")\n",
    "        score = hit.get(\"score\", 0.0)\n",
    "        print(f\"\\n   [{i + 1}] Score: {score:.4f}\")\n",
    "        print(f\"       {text}\")\n",
    "except Exception as e:\n",
    "    print(f\"    검색 중 오류: {e}\")\n",
    "\n",
    "    # 직접 검색 (query_points 사용 - search는 deprecated)\n",
    "    from qdrant_client import QdrantClient\n",
    "\n",
    "    client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "    query_emb = chunkie_openai_embeddins.embed(query)\n",
    "    results = client.query_points(\n",
    "        collection_name=\"chonkie_demo\",\n",
    "        query=query_emb,\n",
    "        limit=3,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    # query_points 결과를 dict 형태로 변환\n",
    "    results = [{\"id\": point.id, \"score\": point.score, **point.payload} for point in results.points]\n",
    "\n",
    "    print(f\"   검색 결과 (상위 {len(results)}개):\")\n",
    "    for i, result in enumerate(results):\n",
    "        # query_points 결과는 dict 형태\n",
    "        text = result.get(\"text\", \"\")[:100]\n",
    "        score = result.get(\"score\", 0.0)\n",
    "        print(f\"\\n   [{i + 1}] Score: {score:.4f}\")\n",
    "        print(f\"       {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05792c5b",
   "metadata": {},
   "source": [
    "## 6. Porters (입출력)\n",
    "\n",
    "청크 데이터를 파일로 저장하고 로드합니다.\n",
    "\n",
    "**종류:**\n",
    "- **JSONPorter**: JSON 포맷으로 저장/로드\n",
    "- **DatasetsPorter**: Hugging Face Datasets 포맷\n",
    "\n",
    "**참고 문서:**\n",
    "- [JSONPorter](https://docs.chonkie.ai/oss/porters/json-porter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 17:48:19.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.porters.json\u001b[0m:\u001b[36m_export_json\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mExporting 4 chunks to JSON file: chonkie_chunks.json\u001b[0m\n",
      "\u001b[32m2025-11-09 17:48:19.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.porters.json\u001b[0m:\u001b[36m_export_json\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mSuccessfully exported 4 chunks to JSON: chonkie_chunks.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] 청크 저장 (export)\n",
      "   저장할 청크 수: 4\n",
      "   출력 파일: chonkie_chunks.json\n",
      "    저장 완료!\n",
      "   파일 크기: 214,126 bytes (209.11 KB)\n",
      "\n",
      "[2/3] 파일 내용 미리보기\n",
      "   [\n",
      "    {\n",
      "        \"id\": \"chnk_bf80c894bec34f98b36f3cf50775206c\",\n",
      "        \"text\": \"Chonkie\\ub294 \\ub2e4\\uc591\\ud55c \\uccad\\ud0b9 \\uc804\\ub7b5\\uc744 \\uc81c\\uacf5\\ud569\\ub2c8\\ub2e4. \\ubb38\\uc7a5 \\ub2e8\\uc704, \\ud1a0\\ud070 \\ub2e8\\uc704,\\n\\uc7ac\\uadc0\\uc801 \\uad6c\\uc870 \\uc778\\uc2dd, \\uc758\\ubbf8 \\uae30\\ubc18 \\ubd84\\ud560 \\ub4f1 \\uc5ec\\ub7ec \\ubc29\\ubc95\\uc744 \\ube44\\uad50\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\uccab \\ubc88\\uc9f8\\ub85c TokenChunker\\ub294 \\uace0\\uc815\\ub41c \\ud1a0\\ud070 \\ud06c\\uae30...\n",
      "\n",
      "[3/3] 청크 로드 (load)\n",
      "    로드 완료!\n",
      "   로드된 청크 수: 4\n",
      "   첫 청크 텍스트 (처음 80자): Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다...\n",
      "\n",
      " JSONPorter로 청크를 파일로 저장하고 재사용할 수 있습니다.\n",
      "   - 실험 결과 저장\n",
      "   - 청크 버전 관리\n",
      "   - 팀 간 청크 공유\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from chonkie.porters import JSONPorter\n",
    "from chonkie.types import Chunk\n",
    "\n",
    "# JSONPorter 초기화\n",
    "output_path = \"chonkie_chunks.json\"\n",
    "porter = JSONPorter(lines=False)  # JSON 형식으로 저장\n",
    "\n",
    "print(\"\\n[1/3] 청크 저장 (export)\")\n",
    "print(f\"   저장할 청크 수: {len(refined_emb)}\")\n",
    "print(f\"   출력 파일: {output_path}\")\n",
    "\n",
    "try:\n",
    "    porter.export(refined_emb, file=output_path)\n",
    "\n",
    "    # 파일 크기 확인\n",
    "    file_size = os.path.getsize(output_path)\n",
    "    print(\"    저장 완료!\")\n",
    "    print(f\"   파일 크기: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "\n",
    "    # 저장된 내용 미리보기\n",
    "    print(\"\\n[2/3] 파일 내용 미리보기\")\n",
    "    with open(output_path, encoding=\"utf-8\") as f:\n",
    "        content = f.read(500)  # 처음 500자\n",
    "        print(f\"   {content}...\")\n",
    "\n",
    "    # 청크 로드\n",
    "    print(\"\\n[3/3] 청크 로드 (load)\")\n",
    "    with open(output_path, encoding=\"utf-8\") as f:\n",
    "        chunks_data = json.load(f)\n",
    "        reloaded_chunks = [Chunk.from_dict(chunk_dict) for chunk_dict in chunks_data]\n",
    "\n",
    "    print(\"    로드 완료!\")\n",
    "    print(f\"   로드된 청크 수: {len(reloaded_chunks)}\")\n",
    "\n",
    "    if reloaded_chunks:\n",
    "        first_chunk = reloaded_chunks[0]\n",
    "        if hasattr(first_chunk, \"text\"):\n",
    "            print(f\"   첫 청크 텍스트 (처음 80자): {first_chunk.text[:80]}...\")\n",
    "        else:\n",
    "            print(f\"   첫 청크: {str(first_chunk)[:80]}...\")\n",
    "\n",
    "    print(\"\\n JSONPorter로 청크를 파일로 저장하고 재사용할 수 있습니다.\")\n",
    "    print(\"   - 실험 결과 저장\")\n",
    "    print(\"   - 청크 버전 관리\")\n",
    "    print(\"   - 팀 간 청크 공유\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  오류: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    print(f\"   상세: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911f23a",
   "metadata": {},
   "source": [
    "## 7. Chunkie Utils (유틸리티)\n",
    "\n",
    "청크 시각화 및 헬퍼 함수를 제공합니다.\n",
    "\n",
    "**기능:**\n",
    "- **Visualizer**: 청크 시각화 (텍스트, 그래프)\n",
    "- **Hubbie**: 편의 함수 모음\n",
    "\n",
    "**참고 문서:**\n",
    "- [Visualizer](https://docs.chonkie.ai/oss/utils/visualizer)\n",
    "- [Hubbie](https://docs.chonkie.ai/oss/utils/hubbie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6796ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/2] Visualizer - 청크 시각화\n",
      "================================================================================\n",
      "\n",
      "[1-1] 터미널 시각화 (Rich 라이브러리 사용)\n",
      "--------------------------------------------------------------------------------\n",
      "   기본 테마 'pastel'로 처음 5개 청크를 색상으로 시각화합니다.\n",
      "   각 청크는 서로 다른 색상으로 표시됩니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,</span>\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.</span>\n",
       "\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.</span>\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.</span>\n",
       "\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">두 번째로 SentenceChunker는 문장 단위로 분할합니다.</span>\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffadad\">자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.</span>\n",
       "\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffd6a5\">세 번째로 RecursiveChunker는 계층적으로 분할합니다.</span>\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffd6a5\">문단, 문장, 단어 순서로 구조를 보존합니다.</span>\n",
       "\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffd6a5\">네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.</span>\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #ffd6a5\">가장 정교하지만 임베딩 모델이 필요합니다.</span>\n",
       "\n",
       "<span style=\"color: #333333; text-decoration-color: #333333; background-color: #fdffb6\">마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;51;51;51;48;2;255;173;173mChonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\u001b[0m\n",
       "\u001b[38;2;51;51;51;48;2;255;173;173m재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\u001b[0m\n",
       "\n",
       "\u001b[38;2;51;51;51;48;2;255;173;173m첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\u001b[0m\n",
       "\u001b[38;2;51;51;51;48;2;255;173;173m빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\u001b[0m\n",
       "\n",
       "\u001b[38;2;51;51;51;48;2;255;173;173m두 번째로 SentenceChunker는 문장 단위로 분할합니다.\u001b[0m\n",
       "\u001b[38;2;51;51;51;48;2;255;173;173m자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\u001b[0m\n",
       "\n",
       "\u001b[38;2;51;51;51;48;2;255;214;165m세 번째로 RecursiveChunker는 계층적으로 분할합니다.\u001b[0m\n",
       "\u001b[38;2;51;51;51;48;2;255;214;165m문단, 문장, 단어 순서로 구조를 보존합니다.\u001b[0m\n",
       "\n",
       "\u001b[38;2;51;51;51;48;2;255;214;165m네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\u001b[0m\n",
       "\u001b[38;2;51;51;51;48;2;255;214;165m가장 정교하지만 임베딩 모델이 필요합니다.\u001b[0m\n",
       "\n",
       "\u001b[38;2;51;51;51;48;2;253;255;182m마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   터미널에서 각 청크가 색상으로 구분되어 표시되었습니다!\n",
      "\n",
      "[1-2] HTML 파일로 저장\n",
      "--------------------------------------------------------------------------------\n",
      "   처음 10개 청크를 HTML 파일로 저장합니다.\n",
      "   저장 경로: /Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization.html\n",
      "\n",
      "HTML visualization saved to: file:///Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization.html\n",
      "\n",
      "   HTML 파일이 저장되었습니다!\n",
      "   브라우저에서 열기: file:///Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization.html\n",
      "\n",
      "[1-3] 다양한 테마 비교\n",
      "--------------------------------------------------------------------------------\n",
      "   여러 테마로 동일한 청크를 시각화하여 비교합니다.\n",
      "\n",
      "   테마: pastel (파스텔 톤의 부드러운 색상)\n",
      "HTML visualization saved to: file:///Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization_pastel.html\n",
      "      저장됨: chunks_visualization_pastel.html\n",
      "   테마: tiktokenizer (Tiktokenizer 스타일의 색상)\n",
      "HTML visualization saved to: file:///Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization_tiktokenizer.html\n",
      "      저장됨: chunks_visualization_tiktokenizer.html\n",
      "   테마: ocean_breeze (바다를 연상시키는 시원한 색상)\n",
      "HTML visualization saved to: file:///Users/jhj/Desktop/sds_class/Day2/notebooks/outputs/chunks_visualization_ocean_breeze.html\n",
      "      저장됨: chunks_visualization_ocean_breeze.html\n",
      "\n",
      "   각 테마별 HTML 파일이 저장되었습니다!\n",
      "   브라우저에서 열어서 테마별 차이를 비교해보세요.\n",
      "\n",
      "================================================================================\n",
      "[2/2] Hubbie - Recipe 관리\n",
      "================================================================================\n",
      "\n",
      "   Hubbie는 HuggingFace Hub에서 Chonkie Recipe를 가져오는 도구입니다.\n",
      "   Recipe는 청킹 설정을 재사용 가능한 형태로 저장한 것입니다.\n",
      "\n",
      "[2-1] Hubbie 인스턴스 생성\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883ce02f2c884e5fb339d2c5886e96e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1.schema.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Hubbie 인스턴스가 생성되었습니다.\n",
      "\n",
      "[2-2] Recipe Schema 조회\n",
      "--------------------------------------------------------------------------------\n",
      "   Recipe의 구조를 정의하는 스키마를 조회합니다.\n",
      "\n",
      "   스키마 버전: v1\n",
      "   스키마 키: ['$schema', 'title', 'type', 'required', 'properties']...\n",
      "\n",
      "   Recipe Schema를 성공적으로 조회했습니다!\n",
      "   Recipe는 이 스키마에 따라 구조화되어 있습니다.\n",
      "\n",
      "[2-3] Recipe 조회 (선택적)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d9b9e42c324cfd8c2ae814bd4fb8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "default_en.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Recipe를 성공적으로 조회했습니다!\n",
      "   Recipe 키: ['name', 'schema', 'description', 'language', 'metadata']...\n",
      "\n",
      "   Recipe는 청킹 설정을 포함하고 있습니다:\n",
      "\n",
      "[2-4] Pipeline Recipe 조회 (선택적)\n",
      "--------------------------------------------------------------------------------\n",
      "   Hub에서 Pipeline Recipe를 조회합니다.\n",
      "   ⚠️  네트워크 연결이 필요합니다.\n",
      "\n",
      "    Pipeline Recipe 조회 실패 (네트워크 연결 필요 또는 Recipe가 존재하지 않음)\n",
      "   오류: Could not download pipeline recipe 'default'. Ensure name is correct or provide a valid path. Error: 404 Client Error. (Request ID: Root=1-6910568f-0a9dca7950765a622350bbae;5cb54b07-0432-4c1c-b50a-844c1021d726)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/datasets/chonkie-ai/recipes/resolve/main/pipelines/default.json.\n",
      "\n",
      "   Pipeline Recipe 조회는 선택적 기능입니다.\n",
      "\n",
      "================================================================================\n",
      "💡 청크 통계\n",
      "================================================================================\n",
      "   총 청크 수: 3\n",
      "   평균 길이: 137.0 문자\n",
      "   최소 길이: 55 문자\n",
      "   최대 길이: 224 문자\n",
      "\n",
      "   토큰 통계:\n",
      "   평균 토큰 수: 97.0\n",
      "   최소 토큰 수: 25\n",
      "   최대 토큰 수: 176\n",
      "\n",
      "================================================================================\n",
      "Utils 데모 완료!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Utils 데모 - Visualizer, Hubbie\n",
    "# ============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 1) Visualizer - 청크 시각화\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from chonkie.utils import Visualizer\n",
    "\n",
    "    # 1-1) 터미널 시각화 (print 메서드)\n",
    "    print(\"\\n[1-1] 터미널 시각화 (Rich 라이브러리 사용)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   기본 테마 'pastel'로 처음 5개 청크를 색상으로 시각화합니다.\")\n",
    "    print(\"   각 청크는 서로 다른 색상으로 표시됩니다.\\n\")\n",
    "\n",
    "    visualizer = Visualizer(theme=\"pastel\")\n",
    "    visualizer.print(chunks_sem[:5])\n",
    "\n",
    "    print(\"\\n   터미널에서 각 청크가 색상으로 구분되어 표시되었습니다!\")\n",
    "\n",
    "    # 1-2) HTML 파일로 저장 (save 메서드)\n",
    "    print(\"\\n[1-2] HTML 파일로 저장\")\n",
    "    print(\"-\" * 80)\n",
    "    html_path = output_dir / \"chunks_visualization.html\"\n",
    "    print(\"   처음 10개 청크를 HTML 파일로 저장합니다.\")\n",
    "    print(f\"   저장 경로: {html_path.absolute()}\\n\")\n",
    "\n",
    "    visualizer.save(str(html_path), chunks_sem[:10], title=\"Chonkie 청크 시각화\")\n",
    "\n",
    "    print(\"\\n   HTML 파일이 저장되었습니다!\")\n",
    "    print(f\"   브라우저에서 열기: file://{html_path.absolute()}\")\n",
    "\n",
    "    # 1-3) 다양한 테마 시연\n",
    "    print(\"\\n[1-3] 다양한 테마 비교\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   여러 테마로 동일한 청크를 시각화하여 비교합니다.\\n\")\n",
    "\n",
    "    themes = [\"pastel\", \"tiktokenizer\", \"ocean_breeze\"]\n",
    "    theme_descriptions = {\n",
    "        \"pastel\": \"파스텔 톤의 부드러운 색상\",\n",
    "        \"tiktokenizer\": \"Tiktokenizer 스타일의 색상\",\n",
    "        \"ocean_breeze\": \"바다를 연상시키는 시원한 색상\",\n",
    "    }\n",
    "\n",
    "    for theme in themes:\n",
    "        print(f\"   테마: {theme} ({theme_descriptions[theme]})\")\n",
    "        viz = Visualizer(theme=theme)\n",
    "        # HTML로 저장하여 테마별 비교 가능하도록\n",
    "        theme_html_path = output_dir / f\"chunks_visualization_{theme}.html\"\n",
    "        viz.save(str(theme_html_path), chunks_sem[:5], title=f\"Chonkie 청크 시각화 - {theme} 테마\")\n",
    "        print(f\"      저장됨: {theme_html_path.name}\")\n",
    "\n",
    "    print(\"\\n   각 테마별 HTML 파일이 저장되었습니다!\")\n",
    "    print(\"   브라우저에서 열어서 테마별 차이를 비교해보세요.\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"   Visualizer를 import할 수 없습니다.\")\n",
    "    print(f\"   오류 상세: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"    오류: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    print(f\"   상세: {traceback.format_exc()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2) Hubbie - Recipe 관리\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[2/2] Hubbie - Recipe 관리\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from chonkie.utils import Hubbie\n",
    "\n",
    "    print(\"\\n   Hubbie는 HuggingFace Hub에서 Chonkie Recipe를 가져오는 도구입니다.\")\n",
    "    print(\"   Recipe는 청킹 설정을 재사용 가능한 형태로 저장한 것입니다.\\n\")\n",
    "\n",
    "    # 2-1) Hubbie 인스턴스 생성\n",
    "    print(\"[2-1] Hubbie 인스턴스 생성\")\n",
    "    print(\"-\" * 80)\n",
    "    hubbie = Hubbie()\n",
    "    print(\"   Hubbie 인스턴스가 생성되었습니다.\")\n",
    "\n",
    "    # 2-2) Recipe Schema 조회\n",
    "    print(\"\\n[2-2] Recipe Schema 조회\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   Recipe의 구조를 정의하는 스키마를 조회합니다.\\n\")\n",
    "\n",
    "    schema = hubbie.get_recipe_schema()\n",
    "    print(f\"   스키마 버전: {hubbie.SCHEMA_VERSION}\")\n",
    "    print(f\"   스키마 키: {list(schema.keys())[:5]}...\")  # 처음 5개 키만 표시\n",
    "    print(\"\\n   Recipe Schema를 성공적으로 조회했습니다!\")\n",
    "    print(\"   Recipe는 이 스키마에 따라 구조화되어 있습니다.\")\n",
    "\n",
    "    # 2-3) Recipe 조회 (선택적, 네트워크 연결 필요)\n",
    "    print(\"\\n[2-3] Recipe 조회 (선택적)\")\n",
    "\n",
    "    try:\n",
    "        recipe = hubbie.get_recipe(name=\"default\", lang=\"en\")\n",
    "        print(\"    Recipe를 성공적으로 조회했습니다!\")\n",
    "        print(f\"   Recipe 키: {list(recipe.keys())[:5]}...\")  # 처음 5개 키만 표시\n",
    "        print(\"\\n   Recipe는 청킹 설정을 포함하고 있습니다:\")\n",
    "        if \"chunker\" in recipe:\n",
    "            print(f\"      - Chunker: {recipe.get('chunker', {}).get('type', 'N/A')}\")\n",
    "        if \"refinery\" in recipe:\n",
    "            print(f\"      - Refinery: {len(recipe.get('refinery', []))}개\")\n",
    "    except Exception as recipe_error:\n",
    "        print(\"    Recipe 조회 실패 (네트워크 연결 필요 또는 Recipe가 존재하지 않음)\")\n",
    "        print(f\"   오류: {recipe_error}\")\n",
    "        print(\"\\n    Recipe 조회는 선택적 기능입니다.\")\n",
    "        print(\"   네트워크 연결이 없어도 Schema 조회는 가능합니다.\")\n",
    "\n",
    "    # 2-4) Pipeline Recipe 조회 (선택적)\n",
    "    print(\"\\n[2-4] Pipeline Recipe 조회 (선택적)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"   Hub에서 Pipeline Recipe를 조회합니다.\")\n",
    "    print(\"   ⚠️  네트워크 연결이 필요합니다.\\n\")\n",
    "    print(\"   💡 Pipeline Recipe는 전체 청킹 파이프라인 설정을 포함합니다.\")\n",
    "    print(\"   참고: 실제로 존재하는 recipe 이름을 사용해야 합니다.\\n\")\n",
    "\n",
    "    try:\n",
    "        # 일반적인 pipeline recipe 이름 시도\n",
    "        pipeline_recipe = hubbie.get_pipeline_recipe(name=\"default\")\n",
    "        print(\"    Pipeline Recipe를 성공적으로 조회했습니다!\")\n",
    "        print(f\"   Pipeline 단계 수: {len(pipeline_recipe.get('steps', []))}\")\n",
    "    except Exception as pipeline_error:\n",
    "        error_str = str(pipeline_error)\n",
    "\n",
    "        print(f\"   ⚠️  Pipeline Recipe 조회 실패\")\n",
    "\n",
    "        # 404 오류인 경우 더 명확한 안내\n",
    "        if \"404\" in error_str or \"Not Found\" in error_str:\n",
    "            print(f\"   원인: 'default'라는 이름의 Pipeline Recipe가 Hub에 존재하지 않습니다.\")\n",
    "            print(f\"\\n   💡 해결 방법:\")\n",
    "            print(f\"   1. HuggingFace Hub에서 사용 가능한 Pipeline Recipe 확인:\")\n",
    "            print(f\"      https://huggingface.co/datasets/chonkie-ai/recipes/tree/main/pipelines\")\n",
    "            print(f\"   2. 실제로 존재하는 recipe 이름을 사용하세요.\")\n",
    "            print(f\"      예: hubbie.get_pipeline_recipe(name='실제_recipe_이름')\")\n",
    "            print(f\"   3. 또는 로컬 파일 경로를 직접 지정할 수 있습니다.\")\n",
    "            print(\n",
    "                f\"      예: hubbie.get_pipeline_recipe(name='recipe', path='./local_recipe.json')\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   오류: {pipeline_error}\")\n",
    "\n",
    "        print(f\"\\n   💡 Pipeline Recipe 조회는 선택적 기능입니다.\")\n",
    "        print(f\"   일반 Recipe 조회와 Schema 조회는 정상적으로 작동합니다.\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"    Hubbie를 import할 수 없습니다.\")\n",
    "    print(\"   의존성 설치: pip install 'chonkie[hub]'\")\n",
    "    print(f\"   오류 상세: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  오류: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    print(f\"   상세: {traceback.format_exc()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3) 청크 통계\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"💡 청크 통계\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"   총 청크 수: {len(chunks_sem)}\")\n",
    "\n",
    "chunk_lengths = [len(c.text if hasattr(c, \"text\") else str(c)) for c in chunks_sem]\n",
    "if chunk_lengths:\n",
    "    avg_len = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    min_len = min(chunk_lengths)\n",
    "    max_len = max(chunk_lengths)\n",
    "\n",
    "    print(f\"   평균 길이: {avg_len:.1f} 문자\")\n",
    "    print(f\"   최소 길이: {min_len} 문자\")\n",
    "    print(f\"   최대 길이: {max_len} 문자\")\n",
    "\n",
    "    # 토큰 수 통계 (가능한 경우)\n",
    "    if hasattr(chunks_sem[0], \"token_count\") and chunks_sem[0].token_count:\n",
    "        token_counts = [\n",
    "            c.token_count for c in chunks_sem if hasattr(c, \"token_count\") and c.token_count\n",
    "        ]\n",
    "        if token_counts:\n",
    "            avg_tokens = sum(token_counts) / len(token_counts)\n",
    "            min_tokens = min(token_counts)\n",
    "            max_tokens = max(token_counts)\n",
    "            print(\"\\n   토큰 통계:\")\n",
    "            print(f\"   평균 토큰 수: {avg_tokens:.1f}\")\n",
    "            print(f\"   최소 토큰 수: {min_tokens}\")\n",
    "            print(f\"   최대 토큰 수: {max_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Utils 데모 완료!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a3261",
   "metadata": {},
   "source": [
    "## 8. End-to-End 파이프라인  \n",
    "### (청킹 → 리파인 → 임베딩 → Indexing → 검색)\n",
    "\n",
    "Chunkie 를 활용한 Document Chunking - Indexing 과정 전체 흐름을 하나의 파이프라인으로 통합합니다.\n",
    "\n",
    "**파이프라인 단계:**\n",
    "1. 텍스트 입력\n",
    "2. Chunker로 분할\n",
    "3. OverlapRefinery로 문맥 보강\n",
    "4. EmbeddingsRefinery로 임베딩 추가\n",
    "5. Qdrant Handshake로 Indexing\n",
    "6. 벡터 검색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efa288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 18:01:27.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mInitialized RecursiveChunker\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:27.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.chunker.recursive\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mStarting recursive chunking for text of length 2055\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:27.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.chunker.recursive\u001b[0m:\u001b[36mchunk\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mCreated 9 chunks using recursive chunking\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:27.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m456\u001b[0m - \u001b[34m\u001b[1mStarting overlap refinery for 9 chunks with method=suffix, mode=recursive\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:27.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.overlap\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1mOverlap refinement complete: added context to 9 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:27.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mStarting embedding refinery for 9 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "입력 텍스트 길이: 2055 문자\n",
      "\n",
      "[1/5] 청킹 (RecursiveChunker)\n",
      "   생성된 청크 수: 9\n",
      "\n",
      "[2/5] 오버랩 리파인 (OverlapRefinery)\n",
      "   리파인 후 청크 수: 9\n",
      "\n",
      "[3/5] 임베딩 보강 (EmbeddingsRefinery)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 18:01:28.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.refinery.embedding\u001b[0m:\u001b[36mrefine\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mEmbedding refinement complete: added embeddings to 9 chunks\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:28.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m191\u001b[0m - \u001b[34m\u001b[1mWriting 9 chunks to Qdrant collection: chonkie_e2e_pipeline\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   임베딩 완료\n",
      "\n",
      "[4/5] Qdrant 색인 (QdrantHandshake)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 18:01:33.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mSuccessfully wrote 9 chunks to Qdrant collection: chonkie_e2e_pipeline\u001b[0m\n",
      "\u001b[32m2025-11-09 18:01:33.225\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mSearching Qdrant collection: chonkie_e2e_pipeline with limit=5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦛 Chonkie wrote 9 chunks to Qdrant collection: chonkie_e2e_pipeline\n",
      "    Indexing 완료\n",
      "\n",
      "[5/5] 벡터 검색\n",
      "   쿼리: '의미 기반 청킹의 장점은?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-09 18:01:33.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mchonkie.handshakes.qdrant\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mSearch complete: found 5 matching chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   검색 결과 (상위 5개):\n",
      "\n",
      "   [1] Score: 0.3074\n",
      "       Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "세 번째로 RecursiveChunker는 계층적으로 분할합니다.\n",
      "...\n",
      "\n",
      "   [2] Score: 0.2487\n",
      "       \n",
      "마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "...\n",
      "\n",
      "   [3] Score: 0.2487\n",
      "       \n",
      "마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "...\n",
      "\n",
      "   [4] Score: 0.2337\n",
      "       \n",
      "네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\n",
      "가장 정교하지만 임베딩 모델이 필요합니다.\n",
      "\n",
      "마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\n",
      "재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "...\n",
      "\n",
      "   [5] Score: 0.1811\n",
      "       재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\n",
      "\n",
      "첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\n",
      "빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\n",
      "\n",
      "두 번째로 SentenceChunker는 문장 단위로 분할합니다.\n",
      "자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\n",
      "\n",
      "세 번째로 RecursiveChunker는 계층적으로 분할합니다.\n",
      "문단, 문장, 단어 순서로 구조를 보존합니다.\n",
      "\n",
      "네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '101f2f4b-a129-502a-bc68-5f405af31a37',\n",
       "  'score': 0.3074306,\n",
       "  'text': 'Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\\n재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\\n\\n첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\\n빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\\n\\n두 번째로 SentenceChunker는 문장 단위로 분할합니다.\\n자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\\n\\n세 번째로 RecursiveChunker는 계층적으로 분할합니다.\\n',\n",
       "  'start_index': 0,\n",
       "  'end_index': 224,\n",
       "  'token_count': 262},\n",
       " {'id': '99cf4d5e-2a13-5e83-8f73-193d25ed3979',\n",
       "  'score': 0.24867697,\n",
       "  'text': '\\n마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\\n재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\\n\\n첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\\n빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\\n\\n두 번째로 SentenceChunker는 문장 단위로 분할합니다.\\n자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\\n\\n',\n",
       "  'start_index': 1589,\n",
       "  'end_index': 1835,\n",
       "  'token_count': 280},\n",
       " {'id': '0ae45e21-4143-5265-9923-2ac4b8760089',\n",
       "  'score': 0.24867697,\n",
       "  'text': '\\n마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\\n재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\\n\\n첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\\n빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\\n\\n두 번째로 SentenceChunker는 문장 단위로 분할합니다.\\n자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\\n\\n',\n",
       "  'start_index': 1178,\n",
       "  'end_index': 1424,\n",
       "  'token_count': 280},\n",
       " {'id': 'bda83565-4bf0-5c32-bef4-8a499ab9a009',\n",
       "  'score': 0.23369828,\n",
       "  'text': '\\n네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\\n가장 정교하지만 임베딩 모델이 필요합니다.\\n\\n마지막으로는 SlumberChunker는 LLM 기반 Contextual Chunking 입니다.Chonkie는 다양한 청킹 전략을 제공합니다. 문장 단위, 토큰 단위,\\n재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\\n\\n첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\\n빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\\n\\n',\n",
       "  'start_index': 699,\n",
       "  'end_index': 943,\n",
       "  'token_count': 277},\n",
       " {'id': '7d1155ec-821d-52f3-8574-7d6f67a24b95',\n",
       "  'score': 0.18106145,\n",
       "  'text': '재귀적 구조 인식, 의미 기반 분할 등 여러 방법을 비교할 수 있습니다.\\n\\n첫 번째로 TokenChunker는 고정된 토큰 크기로 분할합니다.\\n빠르고 예측 가능하지만 의미 경계를 무시할 수 있습니다.\\n\\n두 번째로 SentenceChunker는 문장 단위로 분할합니다.\\n자연스러운 경계를 유지하지만 크기가 불균등할 수 있습니다.\\n\\n세 번째로 RecursiveChunker는 계층적으로 분할합니다.\\n문단, 문장, 단어 순서로 구조를 보존합니다.\\n\\n네 번째로는 SemanticChunker는 의미 유사도 기반으로 분할합니다.\\n',\n",
       "  'start_index': 452,\n",
       "  'end_index': 699,\n",
       "  'token_count': 291}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from chonkie.chunker import RecursiveChunker\n",
    "from chonkie.handshakes import QdrantHandshake\n",
    "from chonkie.refinery import EmbeddingsRefinery, OverlapRefinery\n",
    "\n",
    "\n",
    "def end_to_end_pipeline(\n",
    "    text: str,\n",
    "    query: str,\n",
    "    embeddings_model,\n",
    "    qdrant_url: str = \"http://127.0.0.1:6333\",\n",
    "):\n",
    "    \"\"\"\n",
    "    E2E RAG 파이프라인\n",
    "\n",
    "    Args:\n",
    "        text: 입력 텍스트\n",
    "        query: 검색 쿼리\n",
    "        embeddings_model: 임베딩 모델 (OpenAIEmbeddings 등)\n",
    "        qdrant_url: Qdrant 서버 URL\n",
    "\n",
    "    Returns:\n",
    "        검색 결과 리스트\n",
    "    \"\"\"\n",
    "    print(f\"\\n입력 텍스트 길이: {len(text)} 문자\")\n",
    "\n",
    "    # 1) 청킹\n",
    "    print(\"\\n[1/5] 청킹 (RecursiveChunker)\")\n",
    "    chunker = RecursiveChunker(chunk_size=256, min_characters_per_chunk=24)\n",
    "    chunks = chunker.chunk(text)\n",
    "    print(f\"   생성된 청크 수: {len(chunks)}\")\n",
    "\n",
    "    # 2) 오버랩 리파인\n",
    "    print(\"\\n[2/5] 오버랩 리파인 (OverlapRefinery)\")\n",
    "    overlap_refinery = OverlapRefinery(mode=\"recursive\")\n",
    "    chunks_refined = overlap_refinery.refine(chunks)\n",
    "    print(f\"   리파인 후 청크 수: {len(chunks_refined)}\")\n",
    "\n",
    "    # 3) 임베딩 보강\n",
    "    print(\"\\n[3/5] 임베딩 보강 (EmbeddingsRefinery)\")\n",
    "    embeddings_refinery = EmbeddingsRefinery(embedding_model=embeddings_model)\n",
    "    chunks_with_embeddings = embeddings_refinery.refine(chunks_refined)\n",
    "    print(\"   임베딩 완료\")\n",
    "\n",
    "    # 4) Qdrant 색인\n",
    "    print(\"\\n[4/5] Qdrant 색인 (QdrantHandshake)\")\n",
    "\n",
    "    try:\n",
    "        handshake = QdrantHandshake(\n",
    "            url=qdrant_url,\n",
    "            collection_name=\"chonkie_e2e_pipeline\",\n",
    "            embedding_model=embeddings_model,\n",
    "        )\n",
    "\n",
    "        # Indexing (Handshake API 사용 시도)\n",
    "        try:\n",
    "            handshake.write(chunks_with_embeddings)\n",
    "            print(\"    Indexing 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Handshake API 오류, Indexing: {e}\")\n",
    "\n",
    "        # 5) 검색\n",
    "        print(\"\\n[5/5] 벡터 검색\")\n",
    "        print(f\"   쿼리: '{query}'\")\n",
    "\n",
    "        try:\n",
    "            results = handshake.search(query=query, limit=5)\n",
    "        except Exception:\n",
    "            print(\"   검색 결과가 없습니다.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"   검색 결과 (상위 {len(results)}개):\")\n",
    "        for i, hit in enumerate(results):\n",
    "            text = hit.get(\"text\", \"\")\n",
    "            score = hit.get(\"score\", 0.0)\n",
    "\n",
    "            print(f\"\\n   [{i + 1}] Score: {score:.4f}\")\n",
    "            print(f\"       {text}...\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Qdrant 연결 실패: {e}\")\n",
    "        print(\"   Qdrant 서버를 시작하세요:\")\n",
    "        print(\"   docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 긴 샘플 텍스트\n",
    "pipeline_text = sample_text * 5  # 충분한 텍스트 확보\n",
    "\n",
    "# 쿼리\n",
    "pipeline_query = \"의미 기반 청킹의 장점은?\"\n",
    "\n",
    "# 실행\n",
    "results = end_to_end_pipeline(\n",
    "    text=pipeline_text,\n",
    "    query=pipeline_query,\n",
    "    embeddings_model=chunkie_openai_embeddins,\n",
    "    qdrant_url=QDRANT_URL,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef08a2c",
   "metadata": {},
   "source": [
    "## 학습 내용 요약\n",
    "\n",
    "1. **설치 및 환경 설정**\n",
    "   - Chonkie, OpenAI, Qdrant 클라이언트 설치\n",
    "   - 환경 변수 설정 (OPENAI_API_KEY 등)\n",
    "\n",
    "2. **Embeddings**\n",
    "   - OpenAIEmbeddings: OpenAI 임베딩 모델 사용\n",
    "   - TEI CustomEmbeddings: 온프레미스 임베딩 서버 연동\n",
    "\n",
    "3. **Chunkers (청킹 전략)**\n",
    "   - TokenChunker: 고정 토큰 크기\n",
    "   - SentenceChunker: 문장 단위\n",
    "   - RecursiveChunker: 계층적 분할\n",
    "   - SemanticChunker: 의미 유사도 기반\n",
    "   - CodeChunker (Experimental): 코드 전용\n",
    "\n",
    "4. **Refinery (문맥 보강)**\n",
    "   - OverlapRefinery: 인접 청크 오버랩\n",
    "   - EmbeddingsRefinery: 임베딩 벡터 부착\n",
    "\n",
    "5. **Handshakes (벡터DB 연동)**\n",
    "   - QdrantHandshake: Qdrant 색인 및 검색\n",
    "\n",
    "6. **Porters (입출력)**\n",
    "   - JSONPorter: JSON 저장/로드\n",
    "\n",
    "7. **Utils (유틸리티)**\n",
    "   - Visualizer: 청크 시각화\n",
    "   - Hubbie: 편의 함수\n",
    "\n",
    "8. **E2E 파이프라인**\n",
    "   - 청킹 → 리파인 → 임베딩 → 색인 → 검색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2830bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 직접 E2E 파이프라인을 구현해볼까요?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
