{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62132f34",
   "metadata": {},
   "source": [
    "# 임베딩 모델 성능 비교\n",
    "\n",
    "### 비교 기준\n",
    "\n",
    "1. **임베딩 차원**: 벡터 크기 (차원이 높을수록 표현력 ↑, 메모리 ↑, 연산 속도 ⬇)\n",
    "2. **비용**: API 비용 vs 로컬 서빙 비용\n",
    "3. **메모리**: GPU 메모리 사용량 (로컬 모델)\n",
    "4. **품질**: 의미적 유사도 정확도 (주관적, 벤치마크 기준)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d36aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Text Embedding Inference(TEI) 활용을 위한 공통 코드\n",
    "# Dense Embedding, Reranker\n",
    "import json\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import httpx\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_core.documents import BaseDocumentCompressor, Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "class HuggingfaceEmbedding(Embeddings):\n",
    "    def __init__(self, url: str, model_kwargs: dict | None = None) -> None:\n",
    "        self.url = url\n",
    "        self.model_kwargs = model_kwargs\n",
    "\n",
    "    def health_check(self) -> bool:\n",
    "        try:\n",
    "            httpx.get(f\"{self.url}openapi.json\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Call out to HuggingFaceHub's embedding endpoint for embedding search docs.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        # replace newlines, which can negatively affect performance.\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "        # NOTE: [API 문서](https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/embed)\n",
    "        responses = httpx.post(url=f\"{self.url}embed\", json={\"inputs\": texts, **_model_kwargs})\n",
    "        responses.raise_for_status()\n",
    "        return responses.json()\n",
    "\n",
    "    async def aembed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Async Call to HuggingFaceHub's embedding endpoint for embedding search docs.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        # replace newlines, which can negatively affect performance.\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "        httpx_client = httpx.AsyncClient()\n",
    "        responses = await httpx_client.post(\n",
    "            url=f\"{self.url}embed\", json={\"inputs\": texts, \"parameters\": _model_kwargs}\n",
    "        )\n",
    "        return json.loads(responses.decode())\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Call out to HuggingFaceHub's embedding endpoint for embedding query text.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        response = self.embed_documents([text])[0]\n",
    "        return response\n",
    "\n",
    "    async def aembed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Async Call to HuggingFaceHub's embedding endpoint for embedding query text.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        response = (await self.aembed_documents([text]))[0]\n",
    "        return response\n",
    "\n",
    "\n",
    "# 위 HuggingfaceEmbedding 과 똑같이 TEI 서버를 호출하지만 Endpoint 가 다릅니다.\n",
    "# https://huggingface.co/naver/xprovence-reranker-bgem3-v1\n",
    "class HuggingfaceRerank(BaseDocumentCompressor):\n",
    "    \"\"\"Document compressor using a custom rerank service.\"\"\"\n",
    "\n",
    "    url: str = \"\"\n",
    "    \"\"\"URL of the custom rerank service.\"\"\"\n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "    batch_size: int = 10\n",
    "    \"\"\"Batch size to use for reranking.\"\"\"\n",
    "\n",
    "    def health_check(self) -> bool:\n",
    "        try:\n",
    "            httpx.get(f\"{self.url}openapi.json\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Rerank 모델 서버 연결 실패: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def rerank(self, query: str, texts: list[str]) -> list[dict]:\n",
    "        response = httpx.post(\n",
    "            f\"{self.url}rerank\", json={\"query\": query, \"texts\": texts, \"truncate\": True}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Failed to rerank documents, detail: {response}\")\n",
    "        return response.json()\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks: Callbacks | None = None,\n",
    "    ) -> Sequence[Document]:\n",
    "        if not documents:\n",
    "            print(\"No documents to compress\")\n",
    "            return []\n",
    "\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        batches = [texts[i : i + self.batch_size] for i in range(0, len(texts), self.batch_size)]\n",
    "        all_results = []\n",
    "\n",
    "        for batch in batches:\n",
    "            results = self.rerank(query=query, texts=batch)\n",
    "            all_results.extend(results)\n",
    "\n",
    "        # Sort results based on scores and select top_n\n",
    "        all_results = sorted(all_results, key=lambda x: x[\"score\"], reverse=True)[: self.top_n]\n",
    "\n",
    "        final_results = []\n",
    "        for result in all_results:\n",
    "            index = int(result[\"index\"])\n",
    "            metadata = documents[index].metadata.copy()\n",
    "            metadata[\"relevance_score\"] = result[\"score\"]\n",
    "            final_results.append(\n",
    "                Document(page_content=documents[index].page_content, metadata=metadata)\n",
    "            )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c354167",
   "metadata": {},
   "source": [
    "#### KURE-v1 Embedding via TEI(Based on BGE-M3)\n",
    "\n",
    "1. TEI 서버 와 모델을 RunPod 에 셋팅합니다. \n",
    "- 직접 Docker Image 를 RunPod Template 으로 셋팅해야합니다.\n",
    "  > 특히 HTTP Service PORT 셋팅에 주의하도록 해야합니다 + Environment Variables 에 필요한 값(MODEL_ID, HOST, PORT 등) 들을 셋팅합니다. \n",
    "- 실행 후에 바로 HTTP Service 에 접근하면 에러가 나는게 당연합니다. Model Download 및 Loading 까지는 시간이 소요됩니다.  \n",
    "   Logs 로 살펴보겠습니다.\n",
    "\n",
    "\n",
    "2. 해당 주소로 정상적인 접근이 되는지 확인합니다. (https://주소:8000/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5fd70f",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '404 Not Found' for url 'https://9v0dqiwjovw8zw-8000.proxy.runpod.net/embed'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m test_text = \u001b[33m\"\u001b[39m\u001b[33m이것은 테스트 문장입니다.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 테스트 임베딩\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m test_embedding_bge = \u001b[43mbge_embedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m임베딩 차원 확인: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_embedding_bge)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m sample_embeddings_bge = bge_embedder.embed_documents(\n\u001b[32m     12\u001b[39m     [\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m샘플 텍스트 테스트 중입니다.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m어떤 긴 글을 복사해서 임베딩 테스트를 해볼까요?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     ]\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mHuggingfaceEmbedding.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to HuggingFaceHub's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m        Embeddings for the text.\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mHuggingfaceEmbedding.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# NOTE: [API 문서](https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/embed)\u001b[39;00m\n\u001b[32m     37\u001b[39m responses = httpx.post(url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33membed\u001b[39m\u001b[33m\"\u001b[39m, json={\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: texts, **_model_kwargs})\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m responses.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sds_class/.venv/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    827\u001b[39m error_type = error_types.get(status_class, \u001b[33m\"\u001b[39m\u001b[33mInvalid status code\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://9v0dqiwjovw8zw-8000.proxy.runpod.net/embed'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404"
     ]
    }
   ],
   "source": [
    "# TODO: TEI 서버 셋팅하고 그 주소에 맞게 셋팅이 필요합니다.\n",
    "URL = \"https://9v0dqiwjovw8zw-8000.proxy.runpod.net/\"\n",
    "bge_embedder = HuggingfaceEmbedding(url=URL)\n",
    "\n",
    "test_text = \"이것은 테스트 문장입니다.\"\n",
    "\n",
    "# 테스트 임베딩\n",
    "test_embedding_bge = bge_embedder.embed_query(test_text)\n",
    "print(f\"\\n임베딩 차원 확인: {len(test_embedding_bge)}\")\n",
    "\n",
    "sample_embeddings_bge = bge_embedder.embed_documents(\n",
    "    [\n",
    "        \"샘플 텍스트 테스트 중입니다.\",\n",
    "        \"어떤 긴 글을 복사해서 임베딩 테스트를 해볼까요?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(sample_embeddings_bge)\n",
    "print(f\"{len(sample_embeddings_bge)}개 청크 임베딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab19bf4",
   "metadata": {},
   "source": [
    "#### 다양한 Dense Embedding Model: `Qwen3-Embedding-4B` via TEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b62819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qwen3 임베딩 초기화 실패: Client error '404 Not Found' for url 'https://9v0dqiwjovw8zw-8000.proxy.runpod.net/embed'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Qwen3-Embedding 모델 시리즈 활용하기\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: Qwen3-Embedding-0.6B 모델은 임베딩 차원을 1024 까지 지원합니다\n",
    "# Qwen3-Embedding-4B 모델은 임베딩 차원을 2560 까지 지원합니다\n",
    "# Qwen3-Embedding-8B 모델은 임베딩 차원을 4096 까지 지원합니다\n",
    "\n",
    "# NOTE: MODEL_ID = Qwen/Qwen3-Embedding-4B\n",
    "# NOTE: MAX_BATCH_TOKENS=40960\n",
    "# TODO: TEI 서버(RUNPOD) 주소 - 실습 환경에 맞게 교체가 필요합니다.\n",
    "RUNPOD_URL = \"https://9v0dqiwjovw8zw-8000.proxy.runpod.net/\"\n",
    "\n",
    "try:\n",
    "    qwen_embedder = HuggingfaceEmbedding(url=RUNPOD_URL)\n",
    "\n",
    "    # 테스트 임베딩\n",
    "    test_embedding_qwen = qwen_embedder.embed_query(\"임베딩 차원 테스트\")\n",
    "    print(test_embedding_qwen)\n",
    "    print(f\"   임베딩 차원: {len(test_embedding_qwen)}\")\n",
    "\n",
    "    sample_embeddings_qwen = qwen_embedder.embed_documents([\"메롱\", \"메롱2\"])\n",
    "\n",
    "    print(f\"   {len(sample_embeddings_qwen)}개 청크 임베딩 완료\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nQwen3 임베딩 초기화 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646981f",
   "metadata": {},
   "source": [
    "#### Qwen3-Reranker-0.6B via TEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Qwen3-Reranker 모델 시리즈 활용하기\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: Qwen3-Reranker-0.6B 모델은 Context Window 32K 까지 지원합니다.\n",
    "\n",
    "# NOTE: MODEL_ID = Qwen/Qwen3-Reranker-0.6B\n",
    "# NOTE: MAX_BATCH_TOKENS=32768\n",
    "# TODO: TEI 서버(RUNPOD) 주소 - 실습 환경에 맞게 교체가 필요합니다.\n",
    "RUNPOD_URL = \"https://9v0dqiwjovw8zw-8000.proxy.runpod.net/\"\n",
    "\n",
    "try:\n",
    "    qwen_embedder = HuggingfaceEmbedding(url=RUNPOD_URL)\n",
    "\n",
    "    # 테스트 임베딩\n",
    "    test_embedding_qwen = qwen_embedder.embed_query(\"임베딩 차원 테스트\")\n",
    "    print(test_embedding_qwen)\n",
    "    print(f\"   임베딩 차원: {len(test_embedding_qwen)}\")\n",
    "\n",
    "    start = time.time()\n",
    "    sample_embeddings_qwen = qwen_embedder.embed_documents([\"메롱\", \"메롱2\"])\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"   {len(sample_embeddings_qwen)}개 청크 임베딩 완료\")\n",
    "    print(f\"   소요 시간: {elapsed:.2f}초\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nQwen3 임베딩 초기화 실패: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
