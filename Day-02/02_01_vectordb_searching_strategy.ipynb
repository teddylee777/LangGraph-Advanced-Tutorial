{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorDB Searching Strategy\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "1. Qdrant Named Vectors를 활용한 Dense/Sparse 동시 저장 전략을 이해합니다\n",
    "2. Prefetch + RRF Fusion을 사용한 하이브리드 검색을 구현할 수 있습니다\n",
    "3. Dense 임베딩 모델 선택 (Cloud vs TEI)을 적용할 수 있습니다\n",
    "4. INT8 양자화 및 HNSW 최적화로 프로덕션 성능을 개선할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28293b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 환경 확인 및 패키지 설치\n",
    "# ============================================================================\n",
    "import sys\n",
    "\n",
    "# Python 버전 확인\n",
    "python_version = sys.version_info\n",
    "print(f\"Python 버전: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "if python_version < (3, 10):\n",
    "    raise RuntimeError(\"❌ Python 3.10 이상 필요합니다.\")\n",
    "\n",
    "# LangChain 1.0+ 설치\n",
    "print(\"\\nLangChain 1.0+ 설치 중...\")\n",
    "%pip install -qU langchain langgraph langchain-community\n",
    "\n",
    "print(\"LangChain 통합 패키지 설치 중...\")\n",
    "%pip install -qU langchain-openai langchain-qdrant\n",
    "\n",
    "print(\"검색 및 유틸리티 설치 중...\")\n",
    "%pip install -qU python-dotenv rank-bm25 kiwipiepy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505402ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트 경로 설정 완료:\n",
      "  - Day2 Root: /Users/jhj/Desktop/sds_class/Day2/notebooks\n",
      "  - Project Root: /Users/jhj/Desktop/sds_class/Day2\n",
      "\n",
      ".env 파일 로드: /Users/jhj/Desktop/sds_class/Day2/.env\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    FieldCondition,\n",
    "    Filter,\n",
    "    MatchValue,\n",
    "    PointStruct,\n",
    "    QueryRequest,\n",
    "    SearchRequest,\n",
    "    SparseIndexParams,\n",
    "    SparseVector,\n",
    "    SparseVectorParams,\n",
    "    VectorParams,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"pydantic\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. 프로젝트 경로 설정\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DAY2_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebook_outputs\" else NOTEBOOK_DIR\n",
    "PROJECT_ROOT = DAY2_ROOT.parent\n",
    "\n",
    "for path in [DAY2_ROOT]:\n",
    "    if path.exists() and str(path) not in sys.path:\n",
    "        sys.path.insert(0, str(path))\n",
    "\n",
    "print(\"프로젝트 경로 설정 완료:\")\n",
    "print(f\"  - Day2 Root: {DAY2_ROOT}\")\n",
    "print(f\"  - Project Root: {PROJECT_ROOT}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. 환경 변수 로드\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "env_paths = [\n",
    "    DAY2_ROOT / \".env\",\n",
    "    PROJECT_ROOT / \".env\",\n",
    "]\n",
    "\n",
    "env_loaded = False\n",
    "for env_path in env_paths:\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"\\n.env 파일 로드: {env_path}\")\n",
    "        env_loaded = True\n",
    "        break\n",
    "\n",
    "if not env_loaded:\n",
    "    print(\"\\n.env 파일을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# OpenAI / OpenRouter 모델 초기화 헬퍼\n",
    "# ----------------------------------------------------------------------------\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _resolve_api_context() -> tuple[str, str]:\n",
    "    \"\"\"선택된 API 키와 베이스 URL 정보를 반환합니다.\"\"\"\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY가 필요합니다.\")\n",
    "\n",
    "    base_url = os.getenv(\"OPENROUTER_API_BASE\") or \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "    return (api_key, base_url)\n",
    "\n",
    "\n",
    "def create_openrouter_llm(\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.3,\n",
    "    max_tokens: int | None = None,\n",
    "    **kwargs: object,\n",
    ") -> ChatOpenAI:\n",
    "    \"\"\"OpenAI 호환 LLM 생성 헬퍼.\n",
    "\n",
    "    Args:\n",
    "        model: 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/gpt-4o, anthropic/claude-3-sonnet, google/gemini-pro)\n",
    "        temperature: 생성 온도 (0.0-2.0)\n",
    "        max_tokens: 최대 생성 토큰 수\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: 설정된 LLM 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    openai_kwargs: dict = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_retries\": 3,\n",
    "        \"timeout\": 60,\n",
    "        **kwargs,\n",
    "    }\n",
    "    if max_tokens is not None:\n",
    "        openai_kwargs[\"max_tokens\"] = max_tokens\n",
    "    if base_url:\n",
    "        openai_kwargs[\"base_url\"] = base_url\n",
    "    return ChatOpenAI(**openai_kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model(\n",
    "    model: str = \"openai/text-embedding-3-small\",\n",
    "    **kwargs,\n",
    ") -> OpenAIEmbeddings:\n",
    "    \"\"\"OpenAI 호환 임베딩 모델 생성.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/text-embedding-3-small, openai/text-embedding-3-large)\n",
    "        **kwargs: 추가 파라미터 (encoding_format 등은 model_kwargs로 전달됨)\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: 설정된 임베딩 모델 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    # 전달받은 kwargs에서 model_kwargs로 전달할 파라미터 분리\n",
    "    # encoding_format, extra_headers 등은 model_kwargs로 전달\n",
    "    model_kwargs: dict = {}\n",
    "    embedding_kwargs: dict = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"show_progress_bar\": True,\n",
    "        \"skip_empty\": True,\n",
    "    }\n",
    "\n",
    "    # 전달받은 kwargs 처리\n",
    "    for key, value in kwargs.items():\n",
    "        # OpenRouter API 특정 파라미터는 model_kwargs로 전달\n",
    "        if key in (\"encoding_format\"):\n",
    "            model_kwargs[key] = value\n",
    "        else:\n",
    "            # 나머지는 OpenAIEmbeddings에 직접 전달\n",
    "            embedding_kwargs[key] = value\n",
    "\n",
    "    if base_url:\n",
    "        embedding_kwargs[\"base_url\"] = base_url\n",
    "\n",
    "    # model_kwargs가 있으면 전달\n",
    "    if model_kwargs:\n",
    "        embedding_kwargs[\"model_kwargs\"] = model_kwargs\n",
    "\n",
    "    return OpenAIEmbeddings(**embedding_kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model_direct(\n",
    "    model: str = \"qwen/qwen3-embedding-0.6b\",\n",
    "    encoding_format: Literal[\"float\", \"base64\"] = \"float\",\n",
    "    input_text: str | list[str] = \"\",\n",
    "    **kwargs,\n",
    ") -> list[float] | list[list[float]]:\n",
    "    \"\"\"OpenAI SDK를 직접 사용하여 임베딩 생성 (encoding_format 지원).\n",
    "\n",
    "    LangChain의 OpenAIEmbeddings가 encoding_format을 지원하지 않을 때 사용.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름\n",
    "        encoding_format: 인코딩 형식 (\"float\")\n",
    "        input_text: 임베딩할 텍스트 (문자열 또는 문자열 리스트)\n",
    "        **kwargs: 추가 파라미터\n",
    "\n",
    "    Returns:\n",
    "        임베딩 벡터 리스트 (단일 텍스트) 또는 리스트의 리스트 (여러 텍스트)\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    # input_text가 비어있으면 kwargs에서 가져오기\n",
    "    if not input_text:\n",
    "        input_text = kwargs.get(\"input\", \"\")\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text,\n",
    "        encoding_format=encoding_format,\n",
    "    )\n",
    "\n",
    "    # 단일 텍스트인 경우 첫 번째 임베딩 반환\n",
    "    if isinstance(input_text, str):\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        # 여러 텍스트인 경우 모든 임베딩 반환\n",
    "        return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def get_available_model_types() -> dict[str, list[str]]:\n",
    "    \"\"\"OpenRouter에서 사용 가능한 모델 유형을 반환합니다.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: 모델 유형별 모델 목록\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"chat\": [\n",
    "            \"openai/gpt-4.1\",\n",
    "            \"openai/gpt-4.1-mini\",\n",
    "            \"openai/gpt-5\",\n",
    "            \"openai/gpt-5-mini\",\n",
    "            \"anthropic/claude-sonnet-4.5\",\n",
    "            \"anthropic/claude-haiku-4.5\",\n",
    "            \"google/gemini-2.5-flash-preview-09-2025\",\n",
    "            \"google/gemini-pro-2.5\",\n",
    "            \"x-ai/grok-4-fast\",\n",
    "            \"moonshotai/kimi-k2-thinking\",\n",
    "            \"liquid/lfm-2.2-6b\",\n",
    "            \"z-ai/glm-4.6\",\n",
    "        ],\n",
    "        \"embedding\": [\n",
    "            \"openai/text-embedding-3-small\",\n",
    "            \"openai/text-embedding-3-large\",\n",
    "            \"google/gemini-embedding-001\",\n",
    "            \"qwen/qwen3-embedding-0.6b\",\n",
    "            \"qwen/qwen3-embedding-4b\",\n",
    "            \"qwen/qwen3-embedding-8b\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "embeddings = create_embedding_model()\n",
    "llm = create_openrouter_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curriculum-mapping",
   "metadata": {},
   "source": [
    "## Document Pre-processing - Data Flow 정리\n",
    "\n",
    "```\n",
    "Notebook 01: 문서 변환 & 청킹\n",
    "  ├─ PDF → 텍스트 변환\n",
    "  │   ├─ 고급 옵션: OCR 엔진 선택 (RapidOCR)\n",
    "  │   ├─ 원격 VLM 연동\n",
    "  │   └─ 멀티모달 내보내기\n",
    "  ├─ 메타데이터 저장: ../outputs/converted/{basename}_meta.json\n",
    "  ├─ 텍스트(마크다운) 저장: ../outputs/converted/{basename}.md\n",
    "  └─ 청킹 전략 적용 (RecursiveCharacterTextSplitter, SemanticChunker 등)\n",
    "\n",
    "---\n",
    "\n",
    "Notebook 02 (현재): 벡터 DB 인덱싱 & 검색\n",
    "  ├─ 변환된 텍스트(마크다운) 로드\n",
    "  ├─ 임베딩 & Qdrant 인덱싱\n",
    "  └─ 하이브리드 검색 (Dense + Sparse, Kiwi 형태소 분석기 등)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gy84mtuu4yp",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# RAG 시스템을 위한 VectorDB 검색 전략\n",
    " - Dense \n",
    " - Sparse \n",
    " - **Hybrid(RRF)**\n",
    "\n",
    "## 검색 전략 비교\n",
    "\n",
    "| 전략 | 방법 | 장점 | 단점 | 사용 사례 |\n",
    "|------|------|------|------|----------|\n",
    "| **Dense Vector** | 임베딩 유사도 | 의미 파악, 동의어 처리 | 정확한 키워드 약함 | 자연어 질문 |\n",
    "| **Sparse BM25** | 키워드 빈도(TF-IDF 기반 BM25) + Kiwi | 고유명사, 숫자 정확 | 의미 유사도 무시 | 정확한 용어 검색 |\n",
    "| **Hybrid(RRF)** | Dense + Sparse 결합 | 두 장점 결합 | 복잡도 증가 | **프로덕션 RAG** |\n",
    "\n",
    "\n",
    "- 각각 따로 해보는건 너무 쉬운 Naive RAG 이고, 입문 레벨에서 충분히 해보셨을테니 저희는 바로 Hybrid(RRF) 를 직접 구현해보는 단계로 넘어갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c5151",
   "metadata": {},
   "source": [
    "## Hybrid Search: RRF (Reciprocal Rank Fusion)\n",
    "\n",
    "### RRF란?\n",
    "\n",
    "**Reciprocal Rank Fusion**은 여러 검색 결과를 '순위'를 활용해 결합하는 알고리즘입니다:\n",
    "\n",
    "#### 동작 원리\n",
    "\n",
    "```\n",
    "Dense 검색 결과: [doc_A(rank=1), doc_B(rank=2), doc_C(rank=3)]\n",
    "Sparse 검색 결과: [doc_B(rank=1), doc_D(rank=2), doc_A(rank=3)]\n",
    "```\n",
    "\n",
    "```\n",
    "RRF 점수 계산:\n",
    "doc_A: 1/(60+1) + 1/(60+3) = 0.0164 + 0.0159 = 0.0323\n",
    "doc_B: 1/(60+2) + 1/(60+1) = 0.0161 + 0.0164 = 0.0325 ← 최고점\n",
    "doc_C: 1/(60+3) + 0 = 0.0159\n",
    "doc_D: 0 + 1/(60+2) = 0.0161\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "최종 순위: [doc_B, doc_A, doc_D, doc_C]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**공식**: `score(d) = Σ (weight / (k + rank(d)))`\n",
    "- `k`: RRF 상수 (기본 60)\n",
    "- `weight`: 각 검색기의 가중치\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RRF 알고리즘 구현\n",
    "# 항상 기본 내용을 알고 있어야 충분히 응용하고 변형할 수 있습니다.\n",
    "# ============================================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    dense_results: list[Any],\n",
    "    sparse_results: list[Any],\n",
    "    k: int = 60,\n",
    "    dense_weight: float = 0.5,\n",
    "    sparse_weight: float = 0.5,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion 알고리즘\n",
    "\n",
    "    Args:\n",
    "        dense_results: Dense 검색 결과 (Qdrant 결과)\n",
    "        sparse_results: Sparse 검색 결과 (BM25 결과)\n",
    "        k: RRF 상수 (기본 60)\n",
    "        dense_weight: Dense 검색 가중치\n",
    "        sparse_weight: Sparse 검색 가중치\n",
    "\n",
    "    Returns:\n",
    "        융합된 검색 결과 리스트\n",
    "    \"\"\"\n",
    "    # 문서별 RRF 점수 계산\n",
    "    rrf_scores = defaultdict(float)\n",
    "    doc_data = {}  # 문서 ID → 문서 데이터\n",
    "\n",
    "    # Dense 결과 처리\n",
    "    for rank, result in enumerate(dense_results, start=1):\n",
    "        doc_id = result.id\n",
    "        rrf_scores[doc_id] += dense_weight / (k + rank)\n",
    "        doc_data[doc_id] = {\n",
    "            \"text\": result.payload[\"text\"],\n",
    "            \"metadata\": result.payload.get(\"metadata\", {}),\n",
    "            \"dense_score\": result.score,\n",
    "            \"dense_rank\": rank,\n",
    "        }\n",
    "\n",
    "    # Sparse 결과 처리\n",
    "    for rank, result in enumerate(sparse_results, start=1):\n",
    "        # BM25 결과는 인덱스로 식별 (chunks_data의 인덱스)\n",
    "        doc_id = rank - 1  # 임시: 인덱스를 doc_id로 사용\n",
    "        rrf_scores[doc_id] += sparse_weight / (k + rank)\n",
    "\n",
    "        if doc_id not in doc_data:\n",
    "            doc_data[doc_id] = {\n",
    "                \"text\": result[\"document\"],\n",
    "                \"metadata\": result.get(\"metadata\", {}),\n",
    "                \"sparse_score\": result[\"score\"],\n",
    "                \"sparse_rank\": rank,\n",
    "            }\n",
    "        else:\n",
    "            doc_data[doc_id][\"sparse_score\"] = result[\"score\"]\n",
    "            doc_data[doc_id][\"sparse_rank\"] = rank\n",
    "\n",
    "    # 점수 기준 정렬\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 결과 생성\n",
    "    results = []\n",
    "    for doc_id, rrf_score in sorted_docs:\n",
    "        data = doc_data[doc_id]\n",
    "        results.append(\n",
    "            {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"text\": data[\"text\"],\n",
    "                \"metadata\": data[\"metadata\"],\n",
    "                \"rrf_score\": rrf_score,\n",
    "                \"dense_score\": data.get(\"dense_score\", 0),\n",
    "                \"sparse_score\": data.get(\"sparse_score\", 0),\n",
    "                \"dense_rank\": data.get(\"dense_rank\", None),\n",
    "                \"sparse_rank\": data.get(\"sparse_rank\", None),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
