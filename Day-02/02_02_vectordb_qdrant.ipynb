{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d781a246",
   "metadata": {},
   "source": [
    "## VectorSearch with QdrantVectorStore – 검색 패턴\n",
    "\n",
    "### 학습 목표\n",
    "\n",
    "1. **기본 kNN 검색**: 가장 유사한 k개의 문서 검색\n",
    "2. **Threshold 검색**: 임계값 이상의 유사도를 가진 문서만 검색\n",
    "3. **Filter 검색**: 메타데이터 조건을 만족하는 문서 내에서 검색\n",
    "4. **Hybrid 검색**: Dense + Sparse 벡터를 결합한 RRF 기반 검색\n",
    "\n",
    "### 사전 준비\n",
    "\n",
    "이 섹션을 실행하기 전에 앞선 섹션에서 다음이 완료되어야 합니다:\n",
    "- Qdrant 컬렉션 생성 및 데이터 인덱싱\n",
    "- Dense/Sparse Named Vectors 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b173c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    FieldCondition,\n",
    "    Filter,\n",
    "    MatchValue,\n",
    "    PointStruct,\n",
    "    QueryRequest,\n",
    "    SearchRequest,\n",
    "    SparseIndexParams,\n",
    "    SparseVector,\n",
    "    SparseVectorParams,\n",
    "    VectorParams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707c5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# OpenAI / OpenRouter 모델 초기화 헬퍼\n",
    "# ----------------------------------------------------------------------------\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _resolve_api_context() -> tuple[str, str]:\n",
    "    \"\"\"선택된 API 키와 베이스 URL 정보를 반환합니다.\"\"\"\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY가 필요합니다.\")\n",
    "\n",
    "    base_url = os.getenv(\"OPENROUTER_API_BASE\") or \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "    return (api_key, base_url)\n",
    "\n",
    "\n",
    "def create_openrouter_llm(\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.3,\n",
    "    max_tokens: int | None = None,\n",
    "    **kwargs: object,\n",
    ") -> ChatOpenAI:\n",
    "    \"\"\"OpenAI 호환 LLM 생성 헬퍼.\n",
    "\n",
    "    Args:\n",
    "        model: 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/gpt-4o, anthropic/claude-3-sonnet, google/gemini-pro)\n",
    "        temperature: 생성 온도 (0.0-2.0)\n",
    "        max_tokens: 최대 생성 토큰 수\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: 설정된 LLM 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    kwargs: dict[str, object] = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_retries\": 3,\n",
    "        \"timeout\": 60,\n",
    "    }\n",
    "    if max_tokens is not None:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "    if base_url:\n",
    "        kwargs[\"base_url\"] = base_url\n",
    "    return ChatOpenAI(**kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model(\n",
    "    model: str = \"openai/text-embedding-3-small\",\n",
    "    **kwargs: object,\n",
    ") -> OpenAIEmbeddings:\n",
    "    \"\"\"OpenAI 호환 임베딩 모델 생성.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름. OpenRouter에서는 provider/model 형식 사용 가능\n",
    "               (예: openai/text-embedding-3-small, openai/text-embedding-3-large)\n",
    "        **kwargs: 추가 파라미터 (encoding_format 등은 model_kwargs로 전달됨)\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: 설정된 임베딩 모델 인스턴스\n",
    "    \"\"\"\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    # 전달받은 kwargs에서 model_kwargs로 전달할 파라미터 분리\n",
    "    # encoding_format, extra_headers 등은 model_kwargs로 전달\n",
    "    model_kwargs: dict[str, object] = {}\n",
    "    embedding_kwargs: dict[str, object] = {\n",
    "        \"model\": model,\n",
    "        \"api_key\": api_key,\n",
    "        \"show_progress_bar\": True,\n",
    "        \"skip_empty\": True,\n",
    "    }\n",
    "\n",
    "    # 전달받은 kwargs 처리\n",
    "    for key, value in kwargs.items():\n",
    "        # OpenRouter API 특정 파라미터는 model_kwargs로 전달\n",
    "        if key in (\"encoding_format\"):\n",
    "            model_kwargs[key] = value\n",
    "        else:\n",
    "            # 나머지는 OpenAIEmbeddings에 직접 전달\n",
    "            embedding_kwargs[key] = value\n",
    "\n",
    "    if base_url:\n",
    "        embedding_kwargs[\"base_url\"] = base_url\n",
    "\n",
    "    # model_kwargs가 있으면 전달\n",
    "    if model_kwargs:\n",
    "        embedding_kwargs[\"model_kwargs\"] = model_kwargs\n",
    "\n",
    "    return OpenAIEmbeddings(**embedding_kwargs)\n",
    "\n",
    "\n",
    "def create_embedding_model_direct(\n",
    "    model: str = \"qwen/qwen3-embedding-0.6b\",\n",
    "    encoding_format: str = \"float\",\n",
    "    input_text: str | list[str] = \"\",\n",
    "    **kwargs: object,\n",
    ") -> list[float] | list[list[float]]:\n",
    "    \"\"\"OpenAI SDK를 직접 사용하여 임베딩 생성 (encoding_format 지원).\n",
    "\n",
    "    LangChain의 OpenAIEmbeddings가 encoding_format을 지원하지 않을 때 사용.\n",
    "\n",
    "    Args:\n",
    "        model: 임베딩 모델 이름\n",
    "        encoding_format: 인코딩 형식 (\"float\")\n",
    "        input_text: 임베딩할 텍스트 (문자열 또는 문자열 리스트)\n",
    "        **kwargs: 추가 파라미터\n",
    "\n",
    "    Returns:\n",
    "        임베딩 벡터 리스트 (단일 텍스트) 또는 리스트의 리스트 (여러 텍스트)\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    api_key, base_url = _resolve_api_context()\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    # input_text가 비어있으면 kwargs에서 가져오기\n",
    "    if not input_text:\n",
    "        input_text = kwargs.get(\"input\", \"\")\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text,\n",
    "        encoding_format=encoding_format,\n",
    "    )\n",
    "\n",
    "    # 단일 텍스트인 경우 첫 번째 임베딩 반환\n",
    "    if isinstance(input_text, str):\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        # 여러 텍스트인 경우 모든 임베딩 반환\n",
    "        return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def get_available_model_types() -> dict[str, list[str]]:\n",
    "    \"\"\"OpenRouter에서 사용 가능한 모델 유형을 반환합니다.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: 모델 유형별 모델 목록\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"chat\": [\n",
    "            \"openai/gpt-4.1\",\n",
    "            \"openai/gpt-4.1-mini\",\n",
    "            \"openai/gpt-5\",\n",
    "            \"openai/gpt-5-mini\",\n",
    "            \"anthropic/claude-sonnet-4.5\",\n",
    "            \"anthropic/claude-haiku-4.5\",\n",
    "            \"google/gemini-2.5-flash-preview-09-2025\",\n",
    "            \"google/gemini-pro-2.5\",\n",
    "            \"x-ai/grok-4-fast\",\n",
    "            \"moonshotai/kimi-k2-thinking\",\n",
    "            \"liquid/lfm-2.2-6b\",\n",
    "            \"z-ai/glm-4.6\",\n",
    "        ],\n",
    "        \"embedding\": [\n",
    "            \"openai/text-embedding-3-small\",\n",
    "            \"openai/text-embedding-3-large\",\n",
    "            \"google/gemini-embedding-001\",\n",
    "            \"qwen/qwen3-embedding-0.6b\",\n",
    "            \"qwen/qwen3-embedding-4b\",\n",
    "            \"qwen/qwen3-embedding-8b\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "embeddings = create_embedding_model()\n",
    "llm = create_openrouter_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e8da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant URL: http://localhost:6333\n",
      "\n",
      "기존 Qdrant Collections 목록:\n",
      "  - day2_kiwi_bm25_real_test: 8 points\n",
      "  - day2_product: 0 points\n",
      "  - day2_productionize_sample_collection: 0 points\n",
      "  - day2_int8: 0 points\n",
      "  - hybrid_search: 0 points\n",
      "  - ranking_test: 5 points\n",
      "  - multi_vector_demo: 3 points\n",
      "  - day2_kiwi_bm25_hybrid: 0 points\n",
      "  - chonkie_e2e_pipeline: 9 points\n",
      "  - law_docs_v1: 3 points\n",
      "  - mem0migrations: 1 points\n",
      "  - docs_te3l: 0 points\n",
      "  - chonkie_demo: 4 points\n",
      "  - mem0_user: 3 points\n"
     ]
    }
   ],
   "source": [
    "# Qdrant 클라이언트 생성 및 연결 확인\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "print(f\"Qdrant URL: {qdrant_url}\")\n",
    "\n",
    "# Qdrant 클라이언트 생성\n",
    "try:\n",
    "    client = QdrantClient(url=qdrant_url)\n",
    "\n",
    "    # 연결 확인\n",
    "    collections = client.get_collections()\n",
    "    if collections.collections:\n",
    "        print(\"\\n기존 Qdrant Collections 목록:\")\n",
    "        for col in collections.collections:\n",
    "            info = client.get_collection(col.name)\n",
    "            print(f\"  - {col.name}: {info.points_count} points\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Qdrant 연결 실패: {e}\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9927996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. 기본 kNN 검색\n",
      "================================================================================\n",
      "검색 실패: Unexpected Response: 404 (Not Found)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Not found: Collection `demo_collection` doesn\\'t exist!\"},\"time\":0.000017125}'\n",
      "이 셀을 실행하려면 먼저 Qdrant 컬렉션을 생성하고 데이터를 인덱싱해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. 기본 kNN 검색 (k-Nearest Neighbors)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def qdrant_knn_search(\n",
    "    query: str,\n",
    "    collection_name: str = \"demo_collection\",\n",
    "    k: int = 5,\n",
    "    vector_name: str = \"dense\",\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    기본 kNN 검색: 쿼리와 가장 유사한 상위 k개의 문서를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query: 검색 쿼리\n",
    "        collection_name: Qdrant 컬렉션 이름\n",
    "        k: 반환할 문서 개수\n",
    "        vector_name: 사용할 Named Vector 이름\n",
    "\n",
    "    Returns:\n",
    "        (Document, score) 튜플의 리스트\n",
    "    \"\"\"\n",
    "    # Qdrant 클라이언트 연결\n",
    "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"))\n",
    "\n",
    "    # Embeddings 초기화\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # VectorStore 생성\n",
    "    vectorstore = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "        vector_name=vector_name,\n",
    "        retrieval_mode=RetrievalMode.DENSE,\n",
    "    )\n",
    "\n",
    "    # 유사도 검색 (기본 kNN)\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 테스트 예시\n",
    "print(\"=\" * 80)\n",
    "print(\"1. 기본 kNN 검색\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 예제 쿼리\n",
    "example_query = \"연차휴가는 어떻게 계산되나요?\"\n",
    "\n",
    "try:\n",
    "    results = qdrant_knn_search(\n",
    "        query=example_query,\n",
    "        k=3,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n쿼리: {example_query}\")\n",
    "    print(f\"검색 결과 (상위 {len(results)}개):\\n\")\n",
    "\n",
    "    for idx, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"[{idx}] 유사도 점수: {score:.4f}\")\n",
    "        print(f\"    내용: {doc.page_content[:100]}...\")\n",
    "        print(f\"    메타데이터: {doc.metadata}\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"검색 실패: {e}\")\n",
    "    print(\"이 셀을 실행하려면 먼저 Qdrant 컬렉션을 생성하고 데이터를 인덱싱해야 합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb97420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "2. Threshold 검색\n",
      "================================================================================\n",
      "검색 실패: Unexpected Response: 404 (Not Found)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Not found: Collection `demo_collection` doesn\\'t exist!\"},\"time\":0.000018417}'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. Threshold 검색 (Score Threshold)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def qdrant_threshold_search(\n",
    "    query: str,\n",
    "    collection_name: str = \"demo_collection\",\n",
    "    k: int = 10,\n",
    "    score_threshold: float = 0.7,\n",
    "    vector_name: str = \"dense\",\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Threshold 검색: 임계값 이상의 유사도를 가진 문서만 반환합니다.\n",
    "\n",
    "    낮은 품질의 검색 결과를 필터링하여 정확도를 높입니다.\n",
    "\n",
    "    Args:\n",
    "        query: 검색 쿼리\n",
    "        collection_name: Qdrant 컬렉션 이름\n",
    "        k: 최대 반환 문서 개수\n",
    "        score_threshold: 최소 유사도 점수 (0.0 ~ 1.0)\n",
    "        vector_name: 사용할 Named Vector 이름\n",
    "\n",
    "    Returns:\n",
    "        (Document, score) 튜플의 리스트 (score >= threshold인 것만)\n",
    "    \"\"\"\n",
    "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"))\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\"),\n",
    "    )\n",
    "\n",
    "    vectorstore = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "        vector_name=vector_name,\n",
    "        retrieval_mode=RetrievalMode.DENSE,\n",
    "    )\n",
    "\n",
    "    # Retriever로 변환하여 threshold 설정\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": k,\n",
    "            \"score_threshold\": score_threshold,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 검색 실행\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    # 점수와 함께 반환 (메타데이터에서 추출)\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        score = doc.metadata.get(\"_score\", 1.0)\n",
    "        results.append((doc, score))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 테스트 예시\n",
    "print(\"=\" * 80)\n",
    "print(\"2. Threshold 검색\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_query = \"퇴직금 계산 방법\"\n",
    "\n",
    "try:\n",
    "    # 높은 임계값으로 검색\n",
    "    high_threshold_results = qdrant_threshold_search(\n",
    "        query=example_query,\n",
    "        k=10,\n",
    "        score_threshold=0.8,  # 높은 임계값\n",
    "    )\n",
    "\n",
    "    # 낮은 임계값으로 검색\n",
    "    low_threshold_results = qdrant_threshold_search(\n",
    "        query=example_query,\n",
    "        k=10,\n",
    "        score_threshold=0.5,  # 낮은 임계값\n",
    "    )\n",
    "\n",
    "    print(f\"\\n쿼리: {example_query}\\n\")\n",
    "\n",
    "    print(f\"높은 임계값 (≥0.8): {len(high_threshold_results)}개 문서\")\n",
    "    for idx, (doc, score) in enumerate(high_threshold_results[:3], 1):\n",
    "        print(f\"  [{idx}] 점수: {score:.4f} | {doc.page_content[:80]}...\")\n",
    "\n",
    "    print(f\"\\n낮은 임계값 (≥0.5): {len(low_threshold_results)}개 문서\")\n",
    "    for idx, (doc, score) in enumerate(low_threshold_results[:3], 1):\n",
    "        print(f\"  [{idx}] 점수: {score:.4f} | {doc.page_content[:80]}...\")\n",
    "\n",
    "    print(\"\\n임계값이 높을수록 정확도↑, 재현율↓\")\n",
    "    print(\" 임계값이 낮을수록 재현율↑, 정확도↓\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"검색 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca443f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "3. Filter 검색\n",
      "================================================================================\n",
      "⚠️  검색 실패: Unexpected Response: 404 (Not Found)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Not found: Collection `demo_collection` doesn\\'t exist!\"},\"time\":0.000080083}'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. Filter 검색 (Metadata Filtering)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def qdrant_filter_search(\n",
    "    query: str,\n",
    "    filter_conditions: dict,\n",
    "    collection_name: str = \"demo_collection\",\n",
    "    k: int = 5,\n",
    "    vector_name: str = \"dense\",\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Filter 검색: 메타데이터 조건을 만족하는 문서 내에서만 검색합니다.\n",
    "\n",
    "    예: 특정 문서 타입, 날짜 범위, 태그 등으로 사전 필터링\n",
    "\n",
    "    Args:\n",
    "        query: 검색 쿼리\n",
    "        filter_conditions: 필터 조건 딕셔너리\n",
    "            예: {\"source\": \"근로기준법\", \"pages_gte\": 10}\n",
    "        collection_name: Qdrant 컬렉션 이름\n",
    "        k: 반환할 문서 개수\n",
    "        vector_name: 사용할 Named Vector 이름\n",
    "\n",
    "    Returns:\n",
    "        (Document, score) 튜플의 리스트\n",
    "    \"\"\"\n",
    "    from qdrant_client.models import FieldCondition, Filter, MatchValue, Range\n",
    "\n",
    "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"))\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    # 필터 조건 빌드\n",
    "    must_conditions = []\n",
    "\n",
    "    for key, value in filter_conditions.items():\n",
    "        if key.endswith(\"_gte\"):\n",
    "            # Range: greater than or equal\n",
    "            field = key[:-4]\n",
    "            must_conditions.append(FieldCondition(key=field, range=Range(gte=value)))\n",
    "        elif key.endswith(\"_lte\"):\n",
    "            # Range: less than or equal\n",
    "            field = key[:-4]\n",
    "            must_conditions.append(FieldCondition(key=field, range=Range(lte=value)))\n",
    "        elif key.endswith(\"_gt\"):\n",
    "            # Range: greater than\n",
    "            field = key[:-3]\n",
    "            must_conditions.append(FieldCondition(key=field, range=Range(gt=value)))\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            # Multiple match values (OR)\n",
    "            from qdrant_client.models import MatchAny\n",
    "\n",
    "            must_conditions.append(FieldCondition(key=key, match=MatchAny(any=list(value))))\n",
    "        else:\n",
    "            # Exact match\n",
    "            must_conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "\n",
    "    qdrant_filter = Filter(must=must_conditions) if must_conditions else None\n",
    "\n",
    "    # 검색 실행\n",
    "    search_result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        using=vector_name,\n",
    "        query_filter=qdrant_filter,\n",
    "        limit=k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # Document 객체로 변환\n",
    "    results = []\n",
    "    for point in search_result.points:\n",
    "        doc = Document(\n",
    "            page_content=point.payload.get(\"page_content\", \"\"),\n",
    "            metadata=point.payload.get(\"metadata\", {}),\n",
    "        )\n",
    "        results.append((doc, point.score))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 테스트 예시\n",
    "print(\"=\" * 80)\n",
    "print(\"3. Filter 검색\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_query = \"근로시간 제한\"\n",
    "\n",
    "try:\n",
    "    # 예시 1: 특정 소스 문서만 검색\n",
    "    filter1 = {\"source\": \"근로기준법\"}\n",
    "    results1 = qdrant_filter_search(\n",
    "        query=example_query,\n",
    "        filter_conditions=filter1,\n",
    "        k=3,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n쿼리: {example_query}\")\n",
    "    print(f\"필터: {filter1}\")\n",
    "    print(f\"결과: {len(results1)}개\\n\")\n",
    "\n",
    "    for idx, (doc, score) in enumerate(results1, 1):\n",
    "        print(f\"[{idx}] 점수: {score:.4f}\")\n",
    "        print(f\"    내용: {doc.page_content[:80]}...\")\n",
    "        print(f\"    소스: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print()\n",
    "\n",
    "    # 예시 2: 페이지 범위 필터\n",
    "    filter2 = {\"pages_gte\": 5}\n",
    "    results2 = qdrant_filter_search(\n",
    "        query=example_query,\n",
    "        filter_conditions=filter2,\n",
    "        k=3,\n",
    "    )\n",
    "\n",
    "    print(\"\\n필터: 5페이지 이상 문서\")\n",
    "    print(f\"결과: {len(results2)}개\\n\")\n",
    "\n",
    "    # 예시 3: 복합 필터 (AND 조건)\n",
    "    filter3 = {\"source\": \"근로기준법\", \"ocr_used\": True}\n",
    "    results3 = qdrant_filter_search(\n",
    "        query=example_query,\n",
    "        filter_conditions=filter3,\n",
    "        k=3,\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  검색 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7c2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. Hybrid 검색 (RRF Fusion)\n",
      "================================================================================\n",
      " 검색 실패: name 'QdrantClient' is not defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. Hybrid 검색 (Dense + Sparse RRF Fusion)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    results_list: list[list[tuple[Document, float]]],\n",
    "    k: int = 60,\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion (RRF): 여러 검색 결과를 `순위 기반으로 결합`합니다.\n",
    "\n",
    "    RRF Score = Σ (1 / (k + rank_i))\n",
    "\n",
    "    Args:\n",
    "        results_list: 각 검색기의 결과 리스트\n",
    "        k: RRF 상수 (기본값 60, 일반적으로 60이 최적)\n",
    "\n",
    "    Returns:\n",
    "        융합된 결과 (Document, fused_score) 리스트\n",
    "    \"\"\"\n",
    "    # 문서별 점수 누적\n",
    "    doc_scores = {}\n",
    "    doc_objects = {}\n",
    "\n",
    "    for results in results_list:\n",
    "        for rank, (doc, _score) in enumerate(results, start=1):\n",
    "            # 문서 식별자 생성 (content 기반 해시)\n",
    "            doc_id = hash(doc.page_content)\n",
    "\n",
    "            # RRF 점수 계산: 1 / (k + rank)\n",
    "            rrf_score = 1.0 / (k + rank)\n",
    "\n",
    "            if doc_id not in doc_scores:\n",
    "                doc_scores[doc_id] = 0.0\n",
    "                doc_objects[doc_id] = doc\n",
    "\n",
    "            doc_scores[doc_id] += rrf_score\n",
    "\n",
    "    # 점수 기준 정렬\n",
    "    sorted_docs = sorted(\n",
    "        doc_scores.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    # 결과 리스트 생성\n",
    "    fused_results = [(doc_objects[doc_id], score) for doc_id, score in sorted_docs]\n",
    "\n",
    "    return fused_results\n",
    "\n",
    "\n",
    "def qdrant_hybrid_search(\n",
    "    query: str,\n",
    "    collection_name: str = \"demo_collection\",\n",
    "    k: int = 10,\n",
    "    dense_vector_name: str = \"dense\",\n",
    "    sparse_vector_name: str = \"sparse\",\n",
    "    rrf_k: int = 60,\n",
    ") -> list[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Hybrid 검색: Dense와 Sparse 벡터 검색 결과를 RRF로 결합합니다.\n",
    "\n",
    "    Args:\n",
    "        query: 검색 쿼리\n",
    "        collection_name: Qdrant 컬렉션 이름\n",
    "        k: 각 검색에서 가져올 문서 개수\n",
    "        dense_vector_name: Dense Named Vector 이름\n",
    "        sparse_vector_name: Sparse Named Vector 이름\n",
    "        rrf_k: RRF 상수\n",
    "\n",
    "    Returns:\n",
    "        RRF로 융합된 결과 리스트\n",
    "    \"\"\"\n",
    "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"))\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # 1. Dense 검색\n",
    "    vectorstore_dense = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "        vector_name=dense_vector_name,\n",
    "        retrieval_mode=RetrievalMode.DENSE,\n",
    "    )\n",
    "    dense_results = vectorstore_dense.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # 2. Sparse 검색 (Kiwi 토크나이징)\n",
    "    try:\n",
    "        from kiwipiepy import Kiwi\n",
    "\n",
    "        kiwi = Kiwi()\n",
    "\n",
    "        # 쿼리 토크나이징\n",
    "        tokens = [token.form for token in kiwi.tokenize(query)]\n",
    "\n",
    "        # Sparse 벡터 생성 (간단한 TF 기반)\n",
    "        from collections import Counter\n",
    "\n",
    "        token_counts = Counter(tokens)\n",
    "\n",
    "        # Sparse 검색 수행 (실제 구현 시 SPLADE 등 사용)\n",
    "        # 여기서는 Dense만 사용 (Sparse는 Named Vector 설정 필요)\n",
    "        sparse_results = []\n",
    "\n",
    "        print(\" Sparse 검색을 위해서는 Named Vector 'sparse' 설정이 필요합니다.\")\n",
    "        print(f\"   토크나이징 결과: {tokens[:10]}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"⚠️  kiwipiepy가 설치되지 않았습니다. Dense 검색만 수행합니다.\")\n",
    "        sparse_results = []\n",
    "\n",
    "    # 3. RRF Fusion\n",
    "    if sparse_results:\n",
    "        fused_results = reciprocal_rank_fusion(\n",
    "            [dense_results, sparse_results],\n",
    "            k=rrf_k,\n",
    "        )\n",
    "    else:\n",
    "        # Sparse 결과가 없으면 Dense만 반환\n",
    "        fused_results = dense_results\n",
    "\n",
    "    return fused_results\n",
    "\n",
    "\n",
    "# 테스트 예시\n",
    "print(\"=\" * 80)\n",
    "print(\"4. Hybrid 검색 (RRF Fusion)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_query = \"야간근로 수당 지급 기준\"\n",
    "\n",
    "try:\n",
    "    results = qdrant_hybrid_search(\n",
    "        query=example_query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n쿼리: {example_query}\")\n",
    "    print(f\"결과 (RRF 융합): {len(results)}개\\n\")\n",
    "\n",
    "    for idx, (doc, score) in enumerate(results[:5], 1):\n",
    "        print(f\"[{idx}] RRF 점수: {score:.6f}\")\n",
    "        print(f\"    내용: {doc.page_content[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RRF (Reciprocal Rank Fusion) 개념\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\"\"\n",
    "RRF는 여러 검색 결과를 순위 기반으로 결합하는 알고리즘입니다.\n",
    "\n",
    "핵심 아이디어:\n",
    "- 각 검색기의 순위(rank)를 역수로 변환하여 점수화\n",
    "- 공식: score = 1 / (k + rank)\n",
    "- k는 일반적으로 60 (경험적 최적값)\n",
    "\n",
    "장점:\n",
    "- 검색기별 점수 스케일이 달라도 정규화 불필요\n",
    "- 상위 순위 문서에 더 높은 가중치\n",
    "- 구현이 간단하고 효과적\n",
    "\n",
    "예시:\n",
    "Dense 검색: [Doc A(rank=1), Doc B(rank=2), Doc C(rank=3)]\n",
    "Sparse 검색: [Doc B(rank=1), Doc C(rank=2), Doc D(rank=3)]\n",
    "\n",
    "RRF 점수 (k=60):\n",
    "- Doc A: 1/(60+1) = 0.0164\n",
    "- Doc B: 1/(60+1) + 1/(60+1) = 0.0328  ← 두 검색 모두 상위\n",
    "- Doc C: 1/(60+3) + 1/(60+2) = 0.0317\n",
    "- Doc D: 1/(60+3) = 0.0159\n",
    "\n",
    "최종 순위: B > C > A > D\n",
    "\"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" 검색 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e09fdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Qdrant Named Vectors 실습: Dense + Sparse 하이브리드 인덱싱\n",
    "\n",
    "## [Qdrant Named Vectors란](https://qdrant.tech/documentation/concepts/vectors/#named-vectors)\n",
    "\n",
    "**단일 포인트에 여러 벡터를 저장**할 수 있는 기능입니다.\n",
    "서로 다른 크기와 유형의 여러 벡터를 동일한 데이터 포인트에 저장할 수 있습니다.  \n",
    "이는 서로 다른 특징이나 모달리티(예: 이미지, 텍스트 또는 비디오)를 표현하기 위해 여러 임베딩으로 데이터를 정의해야 할 때 유용할 수 있습니다.  \n",
    "컬렉션 내에 별도의 명명된 벡터 공간을 생성해야 합니다.  \n",
    "이러한 벡터 공간은 컬렉션 생성 시 정의할 수 있으며 독립적으로 관리할 수 있습니다.\n",
    "\n",
    "\n",
    "### 사용 사례\n",
    "- **Dense + Sparse**: 의미 검색 + 키워드 검색 동시 지원\n",
    "- **Multi-Vector (ColBERT 등, Matryoseka 기법 응용)**: 여러 토큰 벡터 저장\n",
    "- **Multi-Modal**: 텍스트 벡터 + 이미지 벡터 동시 저장\n",
    "\n",
    "### 장점\n",
    "1. 단일 컬렉션에서 여러 검색 전략 사용\n",
    "2. Prefetch API로 결과 결합 (RRF Fusion)\n",
    "3. 메타데이터 중복 없이 효율적 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1b871d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m COLLECTION_NAME = \u001b[33m\"\u001b[39m\u001b[33mhybrid_search\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 기존 컬렉션 삭제\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclient\u001b[49m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m client.collection_exists(COLLECTION_NAME):\n\u001b[32m     18\u001b[39m         client.delete_collection(COLLECTION_NAME)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Named Vectors 컬렉션 생성 (Dense + Sparse)\n",
    "# ============================================================================\n",
    "from qdrant_client.models import (\n",
    "    HnswConfigDiff,\n",
    "    PayloadSchemaType,\n",
    "    ScalarQuantization,\n",
    "    ScalarQuantizationConfig,\n",
    "    ScalarType,\n",
    ")\n",
    "\n",
    "# 컬렉션명\n",
    "COLLECTION_NAME = \"hybrid_search\"\n",
    "\n",
    "# 기존 컬렉션 삭제\n",
    "if client:\n",
    "    if client.collection_exists(COLLECTION_NAME):\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(\"  - 기존 컬렉션 삭제 완료\")\n",
    "\n",
    "    # Named Vectors 설정\n",
    "    # 1. Dense vector (의미 검색)\n",
    "    # 2. Sparse vector (키워드 검색)\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config={\n",
    "            \"dense\": VectorParams(\n",
    "                size=1024,  # KURE-v1(BGE-M3 기반)\n",
    "                distance=Distance.COSINE,\n",
    "                on_disk=False,  # 프로덕션 최적화: 디스크 기반\n",
    "            ),\n",
    "            # NOTE: 여기에 이제 여러가지 벡터의 Naming 및 파라미터를 선언해줄 수 있습니다.\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"sparse\": SparseVectorParams(\n",
    "                index=SparseIndexParams(\n",
    "                    on_disk=False,\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "        # NOTE: [참고 문서](https://qdrant.tech/documentation/guides/optimize/)\n",
    "        quantization_config=ScalarQuantization(\n",
    "            scalar=ScalarQuantizationConfig(\n",
    "                type=ScalarType.INT8,\n",
    "                quantile=0.99,\n",
    "                always_ram=True,\n",
    "            )\n",
    "        ),\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=16,  # 정확도 vs 속도 균형\n",
    "            ef_construct=100,\n",
    "            full_scan_threshold=10000,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"컬렉션 생성 완료: {COLLECTION_NAME}\")\n",
    "\n",
    "    # Payload 인덱싱 **메타데이터 필터 성능 향상 - DB Indexing 과 동일하다고 생각하시면 됩니다.**\n",
    "    client.create_payload_index(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        field_name=\"source\",\n",
    "        field_schema=PayloadSchemaType.KEYWORD,\n",
    "    )\n",
    "\n",
    "    client.create_payload_index(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        field_name=\"chunk_id\",\n",
    "        field_schema=PayloadSchemaType.INTEGER,\n",
    "    )\n",
    "\n",
    "## Notebook 01에서 저장한 청크 데이터 로드\n",
    "# TODO: 환경에 맞춰서 저장했던 경로에 대한 수정이 필요할 수 있습니다.\n",
    "DATA_DIR = DAY2_ROOT / \"data\"\n",
    "chunks_file = DATA_DIR / \"chunks.json\"\n",
    "chunks_data = {}\n",
    "\n",
    "# 청크 데이터 로드\n",
    "if chunks_file.exists():\n",
    "    print(f\"청크 파일 발견: {chunks_file}\")\n",
    "\n",
    "    with open(chunks_file, encoding=\"utf-8\") as f:\n",
    "        chunks_data = json.load(f)\n",
    "\n",
    "    print(f\"청크 로드 완료: {len(chunks_data)}개\")\n",
    "\n",
    "    # 샘플 출력\n",
    "    if chunks_data:\n",
    "        print(\"\\n첫 번째 청크 샘플:\")\n",
    "        print(f\"  - 텍스트: {chunks_data[0].get('text', '')}...\")\n",
    "        print(f\"  - 메타데이터: {chunks_data[0].get('metadata', {})}\")\n",
    "\n",
    "print(f\"\\n현재 청크 수: {len(chunks_data)}개\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
